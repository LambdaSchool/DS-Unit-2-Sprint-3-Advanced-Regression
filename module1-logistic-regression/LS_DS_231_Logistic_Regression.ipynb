{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N7SXF6jEBd5_"
   },
   "source": [
    "# Lambda School Data Science - Logistic Regression\n",
    "\n",
    "Logistic regression is the baseline for classification models, as well as a handy way to predict probabilities (since those too live in the unit interval). While relatively simple, it is also the foundation for more sophisticated classification techniques such as neural networks (many of which can effectively be thought of as networks of logistic models)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E7-AOngjadRN"
   },
   "source": [
    "## Lecture - Where Linear goes Wrong\n",
    "### Return of the Titanic ðŸš¢\n",
    "\n",
    "You've likely already explored the rich dataset that is the Titanic - let's use regression and try to predict survival with it. The data is [available from Kaggle](https://www.kaggle.com/c/titanic/data), so we'll also play a bit with [the Kaggle API](https://github.com/Kaggle/kaggle-api)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 228
    },
    "colab_type": "code",
    "id": "MnHLWPYDcyIe",
    "outputId": "1d0b1c18-7dc4-46e0-8d52-2e3693868670"
   },
   "outputs": [],
   "source": [
    "# Already installed locally\n",
    "#!pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 298
    },
    "colab_type": "code",
    "id": "wPgce-jQc5zi",
    "outputId": "bfe4f0e8-0730-4b58-8e2a-ea4e44aebc8e"
   },
   "outputs": [],
   "source": [
    "# Note - you'll also have to sign up for Kaggle and authorize the API\n",
    "# https://github.com/Kaggle/kaggle-api#api-credentials\n",
    "\n",
    "# This essentially means uploading a kaggle.json file\n",
    "# For Colab we can have it in Google Drive\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# %env KAGGLE_CONFIG_DIR=/content/drive/My Drive/\n",
    "\n",
    "# You also have to join the Titanic competition to have access to the data\n",
    "# !kaggle competitions download -c titanic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and Describe data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "colab_type": "code",
    "id": "-PtztP8YlFym",
    "outputId": "8b18c64d-988a-44b1-e988-9de3fa61a5d7"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "colab_type": "code",
    "id": "-PtztP8YlFym",
    "outputId": "8b18c64d-988a-44b1-e988-9de3fa61a5d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(891, 12) (418, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>101</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Petranec, Miss. Matilda</td>\n",
       "      <td>female</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>349245</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>159</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Smiljanic, Mr. Mile</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>315037</td>\n",
       "      <td>8.6625</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>137</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Newsom, Miss. Helen Monypeny</td>\n",
       "      <td>female</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>11752</td>\n",
       "      <td>26.2833</td>\n",
       "      <td>D47</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>134</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Weisz, Mrs. Leopold (Mathilde Francoise Pede)</td>\n",
       "      <td>female</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>228414</td>\n",
       "      <td>26.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Samaan, Mr. Youssef</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2662</td>\n",
       "      <td>21.6792</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived  Pclass  \\\n",
       "100          101         0       3   \n",
       "158          159         0       3   \n",
       "136          137         1       1   \n",
       "133          134         1       2   \n",
       "48            49         0       3   \n",
       "\n",
       "                                              Name     Sex   Age  SibSp  \\\n",
       "100                        Petranec, Miss. Matilda  female  28.0      0   \n",
       "158                            Smiljanic, Mr. Mile    male   NaN      0   \n",
       "136                   Newsom, Miss. Helen Monypeny  female  19.0      0   \n",
       "133  Weisz, Mrs. Leopold (Mathilde Francoise Pede)  female  29.0      1   \n",
       "48                             Samaan, Mr. Youssef    male   NaN      2   \n",
       "\n",
       "     Parch  Ticket     Fare Cabin Embarked  \n",
       "100      0  349245   7.8958   NaN        S  \n",
       "158      0  315037   8.6625   NaN        S  \n",
       "136      2   11752  26.2833   D47        S  \n",
       "133      0  228414  26.0000   NaN        S  \n",
       "48       0    2662  21.6792   NaN        C  "
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'Survived' target feature is already dropped from test_df for Kaggle submission\n",
    "print(train_df.shape, test_df.shape)\n",
    "train_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "colab_type": "code",
    "id": "Ey2ZHrGW_n_t",
    "outputId": "49215e63-a75f-4de4-af3c-ebb434cea2e4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>714.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.383838</td>\n",
       "      <td>2.308642</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>0.523008</td>\n",
       "      <td>0.381594</td>\n",
       "      <td>32.204208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>257.353842</td>\n",
       "      <td>0.486592</td>\n",
       "      <td>0.836071</td>\n",
       "      <td>14.526497</td>\n",
       "      <td>1.102743</td>\n",
       "      <td>0.806057</td>\n",
       "      <td>49.693429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>223.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>20.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.910400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.454200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>668.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>512.329200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PassengerId    Survived      Pclass         Age       SibSp  \\\n",
       "count   891.000000  891.000000  891.000000  714.000000  891.000000   \n",
       "mean    446.000000    0.383838    2.308642   29.699118    0.523008   \n",
       "std     257.353842    0.486592    0.836071   14.526497    1.102743   \n",
       "min       1.000000    0.000000    1.000000    0.420000    0.000000   \n",
       "25%     223.500000    0.000000    2.000000   20.125000    0.000000   \n",
       "50%     446.000000    0.000000    3.000000   28.000000    0.000000   \n",
       "75%     668.500000    1.000000    3.000000   38.000000    1.000000   \n",
       "max     891.000000    1.000000    3.000000   80.000000    8.000000   \n",
       "\n",
       "            Parch        Fare  \n",
       "count  891.000000  891.000000  \n",
       "mean     0.381594   32.204208  \n",
       "std      0.806057   49.693429  \n",
       "min      0.000000    0.000000  \n",
       "25%      0.000000    7.910400  \n",
       "50%      0.000000   14.454200  \n",
       "75%      0.000000   31.000000  \n",
       "max      6.000000  512.329200  "
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split train_df descriptions into numeric and non-numeric\n",
    "train_df.describe(include='number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>891</td>\n",
       "      <td>891</td>\n",
       "      <td>891</td>\n",
       "      <td>204</td>\n",
       "      <td>889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>891</td>\n",
       "      <td>2</td>\n",
       "      <td>681</td>\n",
       "      <td>147</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Beane, Mr. Edward</td>\n",
       "      <td>male</td>\n",
       "      <td>347082</td>\n",
       "      <td>G6</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>577</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>644</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Name   Sex  Ticket Cabin Embarked\n",
       "count                 891   891     891   204      889\n",
       "unique                891     2     681   147        3\n",
       "top     Beane, Mr. Edward  male  347082    G6        S\n",
       "freq                    1   577       7     4      644"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.describe(exclude='number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.616162\n",
       "1    0.383838\n",
       "Name: Survived, dtype: float64"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What percentage survived?\n",
    "train_df['Survived'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "QiZn2p1K8DED",
    "outputId": "be8b1a4c-ca08-4bba-98ed-3cda0942499c"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Use features that are already of numeric type\n",
    "features = ['Pclass', 'Age', 'Fare']\n",
    "target = 'Survived'\n",
    "X_train = train_df[features]\n",
    "X_test = test_df[features]\n",
    "y_train = train_df[target]\n",
    "\n",
    "imputer = SimpleImputer()\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "\n",
    "# Reminder: model score is equivalent to the R-squared statistics.\n",
    "# R-squared: is the proportion of the variance in the dependent variable \n",
    "# that is predictable from the independent variable.\n",
    "linear_reg = LinearRegression().fit(X_train_imputed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "QiZn2p1K8DED",
    "outputId": "be8b1a4c-ca08-4bba-98ed-3cda0942499c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((891, 3), (891, 3), (418, 3), (418, 3))"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_train_imputed.shape, X_test.shape, X_test_imputed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29.69911764705882"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train['Age'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30.272590361445783"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test['Age'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>22.0</td>\n",
       "      <td>7.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>71.2833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>26.0</td>\n",
       "      <td>7.9250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>53.1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>35.0</td>\n",
       "      <td>8.0500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.4583</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pclass   Age     Fare\n",
       "0       3  22.0   7.2500\n",
       "1       1  38.0  71.2833\n",
       "2       3  26.0   7.9250\n",
       "3       1  35.0  53.1000\n",
       "4       3  35.0   8.0500\n",
       "5       3   NaN   8.4583"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.        , 22.        ,  7.25      ],\n",
       "       [ 1.        , 38.        , 71.2833    ],\n",
       "       [ 3.        , 26.        ,  7.925     ],\n",
       "       [ 1.        , 35.        , 53.1       ],\n",
       "       [ 3.        , 35.        ,  8.05      ],\n",
       "       [ 3.        , 29.69911765,  8.4583    ]])"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SimpleImputer() used default 'mean' strategy: notice last age index\n",
    "X_train_imputed[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>1</td>\n",
       "      <td>39.0</td>\n",
       "      <td>108.9000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>3</td>\n",
       "      <td>38.5</td>\n",
       "      <td>7.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.3583</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pclass   Age      Fare\n",
       "413       3   NaN    8.0500\n",
       "414       1  39.0  108.9000\n",
       "415       3  38.5    7.2500\n",
       "416       3   NaN    8.0500\n",
       "417       3   NaN   22.3583"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  3.        ,  29.69911765,   8.05      ],\n",
       "       [  1.        ,  39.        , 108.9       ],\n",
       "       [  3.        ,  38.5       ,   7.25      ],\n",
       "       [  3.        ,  29.69911765,   8.05      ],\n",
       "       [  3.        ,  29.69911765,  22.3583    ]])"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Notice that SimpleImputer is filling in NaNs on the test data\n",
    "# with the train means. This is considered the proper method because\n",
    "# the train data is generally more robust\n",
    "X_test_imputed[-5::]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 330
    },
    "colab_type": "code",
    "id": "8HsBb1hp_cev",
    "outputId": "00536e80-15a1-437f-90e2-94ae4e3ca8ab"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.18476056, 0.09216475, 0.19420436, 0.24057013, 0.2800808 ,\n",
       "       0.33664465, 0.21772081, 0.47358573, 0.30571491, 0.29634405,\n",
       "       0.53456631, 0.74603759, 0.19909257, 0.55361492, 0.48734115,\n",
       "       0.39486551, 0.28363816, 0.24001647, 0.1070494 , 0.49341948,\n",
       "       0.36888899, 0.74507271, 0.69730135, 0.07572154, 0.73816528,\n",
       "       0.27260136, 0.57473387, 0.29474482, 0.49017826, 0.20536585,\n",
       "       0.67612743, 0.30203598, 0.28471732, 0.25591458, 0.1558448 ,\n",
       "       0.13695006, 0.4321428 , 0.56185887, 0.2547322 , 0.54470183,\n",
       "       0.46931104, 0.17978265, 0.72196373, 0.45574283, 0.51322862,\n",
       "       0.84492785, 0.38101538, 0.18113163, 0.25452575, 0.78559573,\n",
       "       0.3135732 , 0.41780243, 0.30610588, 0.27665565, 0.95482663,\n",
       "       0.30620288, 0.53952021, 0.64683924, 0.60947616, 0.26195869,\n",
       "       0.28414174, 0.22530074, 0.66462079, 0.75476086, 0.77315552,\n",
       "       0.46812031, 0.4321428 , 0.26195869, 0.40002749, 0.52702595,\n",
       "       0.51249272, 0.23998209, 0.3063311 , 0.6456702 , 0.28129133,\n",
       "       0.6938844 , 0.68908148, 0.25463521, 0.35350576, 0.22530074,\n",
       "       0.29146838, 0.19596312, 0.5787423 , 0.46397572, 0.24726173,\n",
       "       0.32139719, 0.24361908, 0.28408544, 0.51675825, 0.3527421 ,\n",
       "       0.61240844, 0.30207659, 0.55645741, 0.31113884, 0.43790851,\n",
       "       0.64511155, 0.44925999, 0.56664885, 0.6393381 , 0.23215187,\n",
       "       0.31973224, 0.27670881, 0.34384842, 0.26218391, 0.20332099,\n",
       "       0.48493805, 0.12226871, 0.26203692, 0.2431699 , 0.46157426,\n",
       "       0.26954177, 0.17362342, 0.39435938, 0.7244711 , 0.6016491 ,\n",
       "       0.45661785, 0.56441067, 0.21814849, 0.27689964, 0.44190212,\n",
       "       0.74670693, 0.17699246, 0.36064029, 0.26180855, 0.80662482,\n",
       "       0.26939478, 0.56441067, 0.25176603, 0.39266963, 0.46194962,\n",
       "       0.35120628, 0.25795945, 0.53824   , 0.32609373, 0.27758467,\n",
       "       0.23949097, 0.27393347, 0.16197793, 0.56202942, 0.52355098,\n",
       "       0.49780578, 0.40751425, 0.44503446, 0.4321428 , 0.64369682,\n",
       "       0.76093097, 0.84066521, 0.3608163 , 0.50436932, 0.31368896,\n",
       "       0.35856415, 0.41247066, 0.35899935, 0.20356101, 0.60865482,\n",
       "       0.19581613, 0.91033373, 0.30612465, 0.48177106, 0.44111163,\n",
       "       0.67849672, 0.60377516, 0.46705533, 0.18102217, 0.4744132 ,\n",
       "       0.63151039, 0.25471028, 0.21418762, 0.57321343, 0.22116617,\n",
       "       0.15896735, 0.57638461, 0.55787941, 0.64439288, 0.43278714,\n",
       "       0.2842575 , 0.48912893, 0.28406667, 0.48414664, 0.26935094,\n",
       "       0.14607169, 0.38799561, 0.58890593, 0.71892584, 0.28309074,\n",
       "       0.62006632, 0.29146838, 0.43909756, 0.29099603, 0.52043718,\n",
       "       0.58022701, 0.46963736, 0.33453701, 0.62337699, 0.59077106,\n",
       "       0.49701146, 0.3584874 , 0.44925999, 0.6571826 , 0.29152153,\n",
       "       0.7575296 , 0.26314107, 0.20073059, 0.23275245, 0.51307932,\n",
       "       0.28405106, 0.1772298 , 0.28411051, 0.44700784, 0.43451775,\n",
       "       0.43194574, 0.31414878, 0.5715329 , 0.76488746, 0.51548077,\n",
       "       0.44498174, 0.36832347, 0.43814854, 0.48912893, 0.27000159,\n",
       "       0.43755213, 0.3779539 , 0.43318754, 0.17328895, 0.73867972,\n",
       "       0.21721093, 0.52499504, 0.17497806, 0.24735242, 0.66697579,\n",
       "       0.66693114, 0.22524759, 0.20311454, 0.34249629, 0.26264372,\n",
       "       0.40253763, 0.74654469, 0.43364158, 0.51901893, 0.11221681,\n",
       "       0.30679091, 0.2762803 , 0.16630644, 0.55064912, 0.31345121,\n",
       "       0.54410683, 0.51120252, 0.23996332, 0.50489399, 0.24726173,\n",
       "       0.25420985, 0.46157426, 0.63063146, 0.72520741, 0.26879106,\n",
       "       0.58410302, 0.57450447, 0.44925999, 0.49836359, 0.31753076,\n",
       "       0.58884902, 0.30474752, 0.23999455, 0.65229216, 0.41742707,\n",
       "       0.15119974, 0.48282039, 0.51856039, 0.20296755, 0.8113676 ,\n",
       "       0.32074654, 0.46157426, 0.15855761, 0.47666535, 0.43079067,\n",
       "       0.56996969, 0.46705533, 0.56585557, 0.33138065, 0.44692457,\n",
       "       0.50513485, 0.45804174, 0.22366709, 0.37825273, 0.49431036,\n",
       "       0.43079067, 0.24057013, 0.71118387, 0.30065045, 0.56699813,\n",
       "       0.44014501, 0.49723751, 0.7263871 , 0.49297407, 0.32913269,\n",
       "       0.51763717, 0.71937494, 0.27735945, 0.49723751, 0.48636274,\n",
       "       0.41950321, 0.24733995, 0.30501651, 0.51532728, 0.26197746,\n",
       "       0.23348044, 0.28403229, 0.40440357, 0.75447062, 0.5078366 ,\n",
       "       0.35227917, 0.3051823 , 0.23588585, 0.76611092, 0.26158333,\n",
       "       0.55978873, 0.27669319, 0.2104411 , 0.75654177, 0.37928562,\n",
       "       0.73622903, 0.76378454, 0.55793174, 0.50636894, 0.48177106,\n",
       "       0.64439288, 0.42099693, 0.648833  , 0.232546  , 0.64830582,\n",
       "       0.15489429])"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_reg.predict(test_df[['Pclass', 'Age', 'Fare']].dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pclass   -0.210390\n",
       "Age      -0.007358\n",
       "Fare      0.000751\n",
       "dtype: float64"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(linear_reg.coef_, X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "AFiisZU7_2Fr",
    "outputId": "97e9d9e2-6c2f-49b9-9955-e956b8d42e2a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.19207871])"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "test_case = np.array([[1, 5, 500]])  # Rich 5-year old in first class\n",
    "linear_reg.predict(test_case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How would we fit this data with Logistic Regression?\n",
    "\n",
    "[sklearn.linear_model.LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "dpUm8Dl-u2aB",
    "outputId": "44bc9b92-52ac-4e13-ab03-e87cbfd5fea7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for rich 5 yr old:  [1]\n",
      "Predicted probabilities for rich 5 yr olds (Die/Survive):  [[0.02778799 0.97221201]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg = LogisticRegression(solver='lbfgs').fit(X_train_imputed, y_train)\n",
    "print('Prediction for rich 5 yr old: ', log_reg.predict(test_case))\n",
    "print('Predicted probabilities for rich 5 yr olds (Die/Survive): ', log_reg.predict_proba(test_case))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 278
    },
    "colab_type": "code",
    "id": "IM8g42clF2-6",
    "outputId": "5f10db01-0ec6-4a62-a05f-56f0ab2c04f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method predict in module sklearn.linear_model.base:\n",
      "\n",
      "predict(X) method of sklearn.linear_model.logistic.LogisticRegression instance\n",
      "    Predict class labels for samples in X.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    X : array_like or sparse matrix, shape (n_samples, n_features)\n",
      "        Samples.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    C : array, shape [n_samples]\n",
      "        Predicted class label per sample.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(log_reg.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "cUhr2c66F_th",
    "outputId": "a9ed9706-3418-4ec8-bdb6-f0c4d6fc8c5c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,\n",
       "       1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,\n",
       "       1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,\n",
       "       1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "       0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,\n",
       "       0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,\n",
       "       0], dtype=int64)"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg.predict(test_df[['Pclass', 'Age', 'Fare']].dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "r7xWwqBrFuWL",
    "outputId": "4ea8705c-82b5-4378-cd8d-d0aca8e5d19e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold = 0.5\n",
    "manual_predictions = (log_reg.predict_proba(X_test_imputed)[:, 1] > threshold).astype(int)\n",
    "direct_predictions = log_reg.predict(X_test_imputed)\n",
    "\n",
    "all(manual_predictions == direct_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How accurate is this logistic regression model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy score:  0.7025813692480359\n"
     ]
    }
   ],
   "source": [
    "# Use different metrics for classification vs regression: \n",
    "# R-squared only makes sense for regression and\n",
    "# accuracy only makes sense for classification\n",
    "\n",
    "score = log_reg.score(X_train_imputed, y_train)\n",
    "print('Train accuracy score: ', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total predictions:  (891, 3)\n"
     ]
    }
   ],
   "source": [
    "print('Total predictions: ', X_train_imputed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "891"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = log_reg.predict(X_train_imputed)\n",
    "len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 1 0]\n",
      "[0 1 1 1 0]\n",
      "Correct predictions:  4\n",
      "Accuracy:  0.8\n"
     ]
    }
   ],
   "source": [
    "print(y_pred[:5])\n",
    "print(y_train[:5].values)\n",
    "print('Correct predictions: ', 1 + 1 + 0 + 1 + 1)\n",
    "print('Accuracy: ', 4/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_train[:5], y_pred[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Accuracy Scores:\n",
      " [0.63333333 0.62222222 0.68539326 0.71910112 0.69662921 0.69662921\n",
      " 0.76404494 0.75280899 0.73033708 0.71590909]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(log_reg, X_train_imputed, y_train, cv=10)\n",
    "print('Cross-Validation Accuracy Scores:\\n', scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6222222222222222, 0.7016408466689366, 0.7640449438202247)"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The range of accuracies for this logistic regression model\n",
    "scores = pd.Series(scores)\n",
    "scores.min(), scores.mean(), scores.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "9Bq-54noR1uE",
    "outputId": "b2650236-5573-4f84-d1a8-ce8bc6d1de17"
   },
   "source": [
    "### What's the math?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "9Bq-54noR1uE",
    "outputId": "b2650236-5573-4f84-d1a8-ce8bc6d1de17"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.9345267 , -0.03569729,  0.00422069]])"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Tj0mNL7_XWNV",
    "outputId": "effe14e5-839d-4ee0-e38d-fb80eef80ce4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.55763985])"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AroeYscqR75f"
   },
   "outputs": [],
   "source": [
    "# The logistic sigmoid \"squishing\" function, implemented to accept numpy arrays\n",
    "def sigmoid(x):\n",
    "  return 1 / (1 + np.e**(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "804BA7s0SggQ",
    "outputId": "61e0fcc7-adf9-4077-ba27-de875165cea1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.97221201]])"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(log_reg.intercept_ + np.dot(log_reg.coef_, np.transpose(test_case)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uBSGY-R-Hf_b"
   },
   "source": [
    "So, clearly a more appropriate model in this situation! For more on the math, [see this Wikipedia example](https://en.wikipedia.org/wiki/Logistic_regression#Probability_of_passing_an_exam_versus_hours_of_study).\n",
    "\n",
    "For live - let's tackle [another classification dataset on absenteeism](http://archive.ics.uci.edu/ml/datasets/Absenteeism+at+work) - it has 21 classes, but remember, scikit-learn LogisticRegression automatically handles more than two classes. How? By essentially treating each label as different (1) from some base class (0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qyDBpCM0G7Hv"
   },
   "outputs": [],
   "source": [
    "# Live - let's try absenteeism!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iblW74C8afuR"
   },
   "source": [
    "## Assignment - real-world classification\n",
    "\n",
    "We're going to check out a larger dataset - the [FMA Free Music Archive data](https://github.com/mdeff/fma). It has a selection of CSVs with metadata and calculated audio features that you can load and try to use to classify genre of tracks. To get you started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "def duplicate_feature_deleter(data: pd.DataFrame):\n",
    "    df = data.copy()\n",
    "    for column in df.columns:\n",
    "        if column.endswith('.1') or column.endswith('.2') or column.endswith('.3'):\n",
    "            if all(df[column] == df[column.strip()]):\n",
    "                df = df.drop(columns=column)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read, Clean, Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "VbUJKyI1LRwD",
    "outputId": "b96e94b6-e4b8-438b-fec8-1917563f2667"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Miniconda\\envs\\JupyterLab\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3049: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "tracks = pd.read_csv('tracks.csv', header=1)\n",
    "raw_tracks = pd.read_csv('raw_tracks.csv')\n",
    "genres = pd.read_csv('raw_genres.csv')\n",
    "echonest = pd.read_csv('raw_echonest.csv', header=2)\n",
    "\n",
    "pd.set_option('display.max_columns', None)  # Unlimited columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "colab_type": "code",
    "id": "_qzn-IjIM1Pw",
    "outputId": "8b4fd4af-feff-49f0-d2ec-da238c58b716"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>track_id</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>tempo</th>\n",
       "      <th>valence</th>\n",
       "      <th>album_date</th>\n",
       "      <th>album_name</th>\n",
       "      <th>artist_latitude</th>\n",
       "      <th>artist_location</th>\n",
       "      <th>artist_longitude</th>\n",
       "      <th>artist_name</th>\n",
       "      <th>release</th>\n",
       "      <th>artist_discovery_rank</th>\n",
       "      <th>artist_familiarity_rank</th>\n",
       "      <th>artist_hotttnesss_rank</th>\n",
       "      <th>song_currency_rank</th>\n",
       "      <th>song_hotttnesss_rank</th>\n",
       "      <th>artist_discovery</th>\n",
       "      <th>artist_familiarity</th>\n",
       "      <th>artist_hotttnesss</th>\n",
       "      <th>song_currency</th>\n",
       "      <th>song_hotttnesss</th>\n",
       "      <th>000</th>\n",
       "      <th>001</th>\n",
       "      <th>002</th>\n",
       "      <th>003</th>\n",
       "      <th>004</th>\n",
       "      <th>005</th>\n",
       "      <th>006</th>\n",
       "      <th>007</th>\n",
       "      <th>008</th>\n",
       "      <th>009</th>\n",
       "      <th>010</th>\n",
       "      <th>011</th>\n",
       "      <th>012</th>\n",
       "      <th>013</th>\n",
       "      <th>014</th>\n",
       "      <th>015</th>\n",
       "      <th>016</th>\n",
       "      <th>017</th>\n",
       "      <th>018</th>\n",
       "      <th>019</th>\n",
       "      <th>020</th>\n",
       "      <th>021</th>\n",
       "      <th>022</th>\n",
       "      <th>023</th>\n",
       "      <th>024</th>\n",
       "      <th>025</th>\n",
       "      <th>026</th>\n",
       "      <th>027</th>\n",
       "      <th>028</th>\n",
       "      <th>029</th>\n",
       "      <th>030</th>\n",
       "      <th>031</th>\n",
       "      <th>032</th>\n",
       "      <th>033</th>\n",
       "      <th>034</th>\n",
       "      <th>035</th>\n",
       "      <th>036</th>\n",
       "      <th>037</th>\n",
       "      <th>038</th>\n",
       "      <th>039</th>\n",
       "      <th>040</th>\n",
       "      <th>041</th>\n",
       "      <th>042</th>\n",
       "      <th>043</th>\n",
       "      <th>044</th>\n",
       "      <th>045</th>\n",
       "      <th>046</th>\n",
       "      <th>047</th>\n",
       "      <th>048</th>\n",
       "      <th>049</th>\n",
       "      <th>050</th>\n",
       "      <th>051</th>\n",
       "      <th>052</th>\n",
       "      <th>053</th>\n",
       "      <th>054</th>\n",
       "      <th>055</th>\n",
       "      <th>056</th>\n",
       "      <th>057</th>\n",
       "      <th>058</th>\n",
       "      <th>059</th>\n",
       "      <th>060</th>\n",
       "      <th>061</th>\n",
       "      <th>062</th>\n",
       "      <th>063</th>\n",
       "      <th>064</th>\n",
       "      <th>065</th>\n",
       "      <th>066</th>\n",
       "      <th>067</th>\n",
       "      <th>068</th>\n",
       "      <th>069</th>\n",
       "      <th>070</th>\n",
       "      <th>071</th>\n",
       "      <th>072</th>\n",
       "      <th>073</th>\n",
       "      <th>074</th>\n",
       "      <th>075</th>\n",
       "      <th>076</th>\n",
       "      <th>077</th>\n",
       "      <th>078</th>\n",
       "      <th>079</th>\n",
       "      <th>080</th>\n",
       "      <th>081</th>\n",
       "      <th>082</th>\n",
       "      <th>083</th>\n",
       "      <th>084</th>\n",
       "      <th>085</th>\n",
       "      <th>086</th>\n",
       "      <th>087</th>\n",
       "      <th>088</th>\n",
       "      <th>089</th>\n",
       "      <th>090</th>\n",
       "      <th>091</th>\n",
       "      <th>092</th>\n",
       "      <th>093</th>\n",
       "      <th>094</th>\n",
       "      <th>095</th>\n",
       "      <th>096</th>\n",
       "      <th>097</th>\n",
       "      <th>098</th>\n",
       "      <th>099</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>105</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "      <th>108</th>\n",
       "      <th>109</th>\n",
       "      <th>110</th>\n",
       "      <th>111</th>\n",
       "      <th>112</th>\n",
       "      <th>113</th>\n",
       "      <th>114</th>\n",
       "      <th>115</th>\n",
       "      <th>116</th>\n",
       "      <th>117</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "      <th>128</th>\n",
       "      <th>129</th>\n",
       "      <th>130</th>\n",
       "      <th>131</th>\n",
       "      <th>132</th>\n",
       "      <th>133</th>\n",
       "      <th>134</th>\n",
       "      <th>135</th>\n",
       "      <th>136</th>\n",
       "      <th>137</th>\n",
       "      <th>138</th>\n",
       "      <th>139</th>\n",
       "      <th>140</th>\n",
       "      <th>141</th>\n",
       "      <th>142</th>\n",
       "      <th>143</th>\n",
       "      <th>144</th>\n",
       "      <th>145</th>\n",
       "      <th>146</th>\n",
       "      <th>147</th>\n",
       "      <th>148</th>\n",
       "      <th>149</th>\n",
       "      <th>150</th>\n",
       "      <th>151</th>\n",
       "      <th>152</th>\n",
       "      <th>153</th>\n",
       "      <th>154</th>\n",
       "      <th>155</th>\n",
       "      <th>156</th>\n",
       "      <th>157</th>\n",
       "      <th>158</th>\n",
       "      <th>159</th>\n",
       "      <th>160</th>\n",
       "      <th>161</th>\n",
       "      <th>162</th>\n",
       "      <th>163</th>\n",
       "      <th>164</th>\n",
       "      <th>165</th>\n",
       "      <th>166</th>\n",
       "      <th>167</th>\n",
       "      <th>168</th>\n",
       "      <th>169</th>\n",
       "      <th>170</th>\n",
       "      <th>171</th>\n",
       "      <th>172</th>\n",
       "      <th>173</th>\n",
       "      <th>174</th>\n",
       "      <th>175</th>\n",
       "      <th>176</th>\n",
       "      <th>177</th>\n",
       "      <th>178</th>\n",
       "      <th>179</th>\n",
       "      <th>180</th>\n",
       "      <th>181</th>\n",
       "      <th>182</th>\n",
       "      <th>183</th>\n",
       "      <th>184</th>\n",
       "      <th>185</th>\n",
       "      <th>186</th>\n",
       "      <th>187</th>\n",
       "      <th>188</th>\n",
       "      <th>189</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "      <th>200</th>\n",
       "      <th>201</th>\n",
       "      <th>202</th>\n",
       "      <th>203</th>\n",
       "      <th>204</th>\n",
       "      <th>205</th>\n",
       "      <th>206</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "      <th>214</th>\n",
       "      <th>215</th>\n",
       "      <th>216</th>\n",
       "      <th>217</th>\n",
       "      <th>218</th>\n",
       "      <th>219</th>\n",
       "      <th>220</th>\n",
       "      <th>221</th>\n",
       "      <th>222</th>\n",
       "      <th>223</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.416675</td>\n",
       "      <td>0.675894</td>\n",
       "      <td>0.634476</td>\n",
       "      <td>0.010628</td>\n",
       "      <td>0.177647</td>\n",
       "      <td>0.159310</td>\n",
       "      <td>165.922</td>\n",
       "      <td>0.576661</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.6783</td>\n",
       "      <td>Georgia, US</td>\n",
       "      <td>-83.2230</td>\n",
       "      <td>AWOL</td>\n",
       "      <td>AWOL - A Way Of Life</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.388990</td>\n",
       "      <td>0.386740</td>\n",
       "      <td>0.406370</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.877233</td>\n",
       "      <td>0.588911</td>\n",
       "      <td>0.354243</td>\n",
       "      <td>0.295090</td>\n",
       "      <td>0.298413</td>\n",
       "      <td>0.309430</td>\n",
       "      <td>0.304496</td>\n",
       "      <td>0.334579</td>\n",
       "      <td>0.249495</td>\n",
       "      <td>0.259656</td>\n",
       "      <td>0.318376</td>\n",
       "      <td>0.371974</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.5710</td>\n",
       "      <td>0.278</td>\n",
       "      <td>0.2100</td>\n",
       "      <td>0.2150</td>\n",
       "      <td>0.2285</td>\n",
       "      <td>0.2375</td>\n",
       "      <td>0.279</td>\n",
       "      <td>0.1685</td>\n",
       "      <td>0.1685</td>\n",
       "      <td>0.279</td>\n",
       "      <td>0.3325</td>\n",
       "      <td>0.049848</td>\n",
       "      <td>0.104212</td>\n",
       "      <td>0.060230</td>\n",
       "      <td>0.052290</td>\n",
       "      <td>0.047403</td>\n",
       "      <td>0.052815</td>\n",
       "      <td>0.052733</td>\n",
       "      <td>0.062216</td>\n",
       "      <td>0.051613</td>\n",
       "      <td>0.057399</td>\n",
       "      <td>0.053199</td>\n",
       "      <td>0.062583</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.013</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.964</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.983</td>\n",
       "      <td>0.979</td>\n",
       "      <td>0.979</td>\n",
       "      <td>0.990</td>\n",
       "      <td>0.985</td>\n",
       "      <td>0.959</td>\n",
       "      <td>0.990</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.979</td>\n",
       "      <td>0.987</td>\n",
       "      <td>-1.899342</td>\n",
       "      <td>-0.032654</td>\n",
       "      <td>0.878469</td>\n",
       "      <td>1.147538</td>\n",
       "      <td>0.950856</td>\n",
       "      <td>0.948257</td>\n",
       "      <td>1.157887</td>\n",
       "      <td>1.147911</td>\n",
       "      <td>1.646318</td>\n",
       "      <td>1.530193</td>\n",
       "      <td>1.197568</td>\n",
       "      <td>0.745673</td>\n",
       "      <td>2.510038</td>\n",
       "      <td>-1.500183</td>\n",
       "      <td>0.030540</td>\n",
       "      <td>0.694242</td>\n",
       "      <td>0.170432</td>\n",
       "      <td>0.064695</td>\n",
       "      <td>0.874727</td>\n",
       "      <td>0.722576</td>\n",
       "      <td>2.251320</td>\n",
       "      <td>1.708159</td>\n",
       "      <td>1.054857</td>\n",
       "      <td>0.020675</td>\n",
       "      <td>42.949131</td>\n",
       "      <td>44.387436</td>\n",
       "      <td>32.409389</td>\n",
       "      <td>15.668667</td>\n",
       "      <td>10.114028</td>\n",
       "      <td>-4.069252</td>\n",
       "      <td>2.042353</td>\n",
       "      <td>2.188321</td>\n",
       "      <td>-3.805923</td>\n",
       "      <td>-0.494699</td>\n",
       "      <td>6.024670</td>\n",
       "      <td>10.692599</td>\n",
       "      <td>44.442501</td>\n",
       "      <td>42.388500</td>\n",
       "      <td>31.684999</td>\n",
       "      <td>9.987500</td>\n",
       "      <td>9.568501</td>\n",
       "      <td>-7.1485</td>\n",
       "      <td>3.8315</td>\n",
       "      <td>1.8505</td>\n",
       "      <td>-2.6875</td>\n",
       "      <td>-0.800</td>\n",
       "      <td>5.4615</td>\n",
       "      <td>10.2565</td>\n",
       "      <td>39.494820</td>\n",
       "      <td>1966.979126</td>\n",
       "      <td>1825.123047</td>\n",
       "      <td>1903.756714</td>\n",
       "      <td>828.810059</td>\n",
       "      <td>911.155823</td>\n",
       "      <td>581.015320</td>\n",
       "      <td>722.001404</td>\n",
       "      <td>404.682556</td>\n",
       "      <td>315.528473</td>\n",
       "      <td>376.632416</td>\n",
       "      <td>229.282547</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-110.367996</td>\n",
       "      <td>-100.605003</td>\n",
       "      <td>-112.581001</td>\n",
       "      <td>-75.882004</td>\n",
       "      <td>-89.160004</td>\n",
       "      <td>-80.737999</td>\n",
       "      <td>-91.498001</td>\n",
       "      <td>-66.649002</td>\n",
       "      <td>-61.845001</td>\n",
       "      <td>-66.081001</td>\n",
       "      <td>-58.043999</td>\n",
       "      <td>52.006001</td>\n",
       "      <td>216.237000</td>\n",
       "      <td>208.423004</td>\n",
       "      <td>145.194000</td>\n",
       "      <td>97.482002</td>\n",
       "      <td>98.723000</td>\n",
       "      <td>68.091003</td>\n",
       "      <td>101.588997</td>\n",
       "      <td>69.505997</td>\n",
       "      <td>58.227001</td>\n",
       "      <td>69.262001</td>\n",
       "      <td>58.175999</td>\n",
       "      <td>52.006001</td>\n",
       "      <td>326.604980</td>\n",
       "      <td>309.028015</td>\n",
       "      <td>257.774994</td>\n",
       "      <td>173.364014</td>\n",
       "      <td>187.882996</td>\n",
       "      <td>148.829010</td>\n",
       "      <td>193.087006</td>\n",
       "      <td>136.154999</td>\n",
       "      <td>120.072006</td>\n",
       "      <td>135.343002</td>\n",
       "      <td>116.220001</td>\n",
       "      <td>-2.952152</td>\n",
       "      <td>0.060379</td>\n",
       "      <td>0.525976</td>\n",
       "      <td>0.365915</td>\n",
       "      <td>0.018182</td>\n",
       "      <td>0.454431</td>\n",
       "      <td>-0.330007</td>\n",
       "      <td>0.149395</td>\n",
       "      <td>-0.214859</td>\n",
       "      <td>0.030427</td>\n",
       "      <td>-0.153877</td>\n",
       "      <td>-0.150132</td>\n",
       "      <td>13.206213</td>\n",
       "      <td>1.009934</td>\n",
       "      <td>1.577194</td>\n",
       "      <td>0.337023</td>\n",
       "      <td>0.097149</td>\n",
       "      <td>0.401260</td>\n",
       "      <td>0.006324</td>\n",
       "      <td>0.643486</td>\n",
       "      <td>0.012059</td>\n",
       "      <td>0.237947</td>\n",
       "      <td>0.655938</td>\n",
       "      <td>1.213864</td>\n",
       "      <td>-12.486146</td>\n",
       "      <td>-11.2695</td>\n",
       "      <td>46.031261</td>\n",
       "      <td>-60.000000</td>\n",
       "      <td>-3.933</td>\n",
       "      <td>56.067001</td>\n",
       "      <td>-2.587475</td>\n",
       "      <td>11.802585</td>\n",
       "      <td>0.047970</td>\n",
       "      <td>0.038275</td>\n",
       "      <td>0.000988</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.20730</td>\n",
       "      <td>0.20730</td>\n",
       "      <td>1.603659</td>\n",
       "      <td>2.984276</td>\n",
       "      <td>-21.812077</td>\n",
       "      <td>-20.312000</td>\n",
       "      <td>49.157482</td>\n",
       "      <td>-60.0</td>\n",
       "      <td>-9.691</td>\n",
       "      <td>50.308998</td>\n",
       "      <td>-1.992303</td>\n",
       "      <td>6.805694</td>\n",
       "      <td>0.233070</td>\n",
       "      <td>0.192880</td>\n",
       "      <td>0.027455</td>\n",
       "      <td>0.06408</td>\n",
       "      <td>3.67696</td>\n",
       "      <td>3.61288</td>\n",
       "      <td>13.316690</td>\n",
       "      <td>262.929749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.374408</td>\n",
       "      <td>0.528643</td>\n",
       "      <td>0.817461</td>\n",
       "      <td>0.001851</td>\n",
       "      <td>0.105880</td>\n",
       "      <td>0.461818</td>\n",
       "      <td>126.957</td>\n",
       "      <td>0.269240</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.6783</td>\n",
       "      <td>Georgia, US</td>\n",
       "      <td>-83.2230</td>\n",
       "      <td>AWOL</td>\n",
       "      <td>AWOL - A Way Of Life</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.388990</td>\n",
       "      <td>0.386740</td>\n",
       "      <td>0.406370</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.534429</td>\n",
       "      <td>0.537414</td>\n",
       "      <td>0.443299</td>\n",
       "      <td>0.390879</td>\n",
       "      <td>0.344573</td>\n",
       "      <td>0.366448</td>\n",
       "      <td>0.419455</td>\n",
       "      <td>0.747766</td>\n",
       "      <td>0.460901</td>\n",
       "      <td>0.392379</td>\n",
       "      <td>0.474559</td>\n",
       "      <td>0.406729</td>\n",
       "      <td>0.506</td>\n",
       "      <td>0.5145</td>\n",
       "      <td>0.387</td>\n",
       "      <td>0.3235</td>\n",
       "      <td>0.2805</td>\n",
       "      <td>0.3135</td>\n",
       "      <td>0.3455</td>\n",
       "      <td>0.898</td>\n",
       "      <td>0.4365</td>\n",
       "      <td>0.3385</td>\n",
       "      <td>0.398</td>\n",
       "      <td>0.3480</td>\n",
       "      <td>0.079207</td>\n",
       "      <td>0.083319</td>\n",
       "      <td>0.073595</td>\n",
       "      <td>0.071024</td>\n",
       "      <td>0.056679</td>\n",
       "      <td>0.066113</td>\n",
       "      <td>0.073889</td>\n",
       "      <td>0.088100</td>\n",
       "      <td>0.071305</td>\n",
       "      <td>0.059275</td>\n",
       "      <td>0.088222</td>\n",
       "      <td>0.067298</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.028</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.960</td>\n",
       "      <td>0.960</td>\n",
       "      <td>0.971</td>\n",
       "      <td>0.979</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.980</td>\n",
       "      <td>0.980</td>\n",
       "      <td>0.947</td>\n",
       "      <td>0.978</td>\n",
       "      <td>0.968</td>\n",
       "      <td>0.966</td>\n",
       "      <td>0.972</td>\n",
       "      <td>0.197378</td>\n",
       "      <td>0.182772</td>\n",
       "      <td>0.608394</td>\n",
       "      <td>0.785822</td>\n",
       "      <td>0.773479</td>\n",
       "      <td>0.650546</td>\n",
       "      <td>0.574605</td>\n",
       "      <td>-0.767804</td>\n",
       "      <td>0.326266</td>\n",
       "      <td>0.808435</td>\n",
       "      <td>0.506893</td>\n",
       "      <td>0.682139</td>\n",
       "      <td>-1.157548</td>\n",
       "      <td>-1.170147</td>\n",
       "      <td>-0.683188</td>\n",
       "      <td>-0.366574</td>\n",
       "      <td>-0.245357</td>\n",
       "      <td>-0.547616</td>\n",
       "      <td>-0.809711</td>\n",
       "      <td>-0.894495</td>\n",
       "      <td>-0.808738</td>\n",
       "      <td>-0.130317</td>\n",
       "      <td>-1.056485</td>\n",
       "      <td>-0.510763</td>\n",
       "      <td>44.097282</td>\n",
       "      <td>48.790241</td>\n",
       "      <td>10.584317</td>\n",
       "      <td>-2.565454</td>\n",
       "      <td>-1.292831</td>\n",
       "      <td>-4.490149</td>\n",
       "      <td>-0.633741</td>\n",
       "      <td>2.112556</td>\n",
       "      <td>-7.288055</td>\n",
       "      <td>-5.132599</td>\n",
       "      <td>-2.390409</td>\n",
       "      <td>11.447978</td>\n",
       "      <td>45.414501</td>\n",
       "      <td>51.979000</td>\n",
       "      <td>6.598000</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>-0.012000</td>\n",
       "      <td>-8.0495</td>\n",
       "      <td>-0.6830</td>\n",
       "      <td>1.0310</td>\n",
       "      <td>-7.0775</td>\n",
       "      <td>-4.321</td>\n",
       "      <td>-0.9275</td>\n",
       "      <td>9.0895</td>\n",
       "      <td>22.519407</td>\n",
       "      <td>1694.821167</td>\n",
       "      <td>1256.443848</td>\n",
       "      <td>1251.588257</td>\n",
       "      <td>907.545166</td>\n",
       "      <td>588.602051</td>\n",
       "      <td>619.971680</td>\n",
       "      <td>679.588013</td>\n",
       "      <td>301.723358</td>\n",
       "      <td>401.670532</td>\n",
       "      <td>294.221619</td>\n",
       "      <td>411.299500</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-82.774002</td>\n",
       "      <td>-137.591003</td>\n",
       "      <td>-131.729004</td>\n",
       "      <td>-106.242996</td>\n",
       "      <td>-73.445000</td>\n",
       "      <td>-83.468002</td>\n",
       "      <td>-81.322998</td>\n",
       "      <td>-71.073997</td>\n",
       "      <td>-108.565002</td>\n",
       "      <td>-71.498001</td>\n",
       "      <td>-51.351002</td>\n",
       "      <td>51.366001</td>\n",
       "      <td>190.334000</td>\n",
       "      <td>114.844002</td>\n",
       "      <td>359.941986</td>\n",
       "      <td>101.862000</td>\n",
       "      <td>111.082001</td>\n",
       "      <td>78.648003</td>\n",
       "      <td>103.000999</td>\n",
       "      <td>46.223000</td>\n",
       "      <td>53.993999</td>\n",
       "      <td>61.181000</td>\n",
       "      <td>90.429001</td>\n",
       "      <td>51.366001</td>\n",
       "      <td>273.108002</td>\n",
       "      <td>252.434998</td>\n",
       "      <td>491.670990</td>\n",
       "      <td>208.104996</td>\n",
       "      <td>184.527008</td>\n",
       "      <td>162.115997</td>\n",
       "      <td>184.324005</td>\n",
       "      <td>117.296997</td>\n",
       "      <td>162.559006</td>\n",
       "      <td>132.679001</td>\n",
       "      <td>141.779999</td>\n",
       "      <td>-1.827564</td>\n",
       "      <td>-0.083561</td>\n",
       "      <td>0.162382</td>\n",
       "      <td>0.829534</td>\n",
       "      <td>-0.164874</td>\n",
       "      <td>0.897740</td>\n",
       "      <td>-0.058807</td>\n",
       "      <td>0.365381</td>\n",
       "      <td>-0.131389</td>\n",
       "      <td>-0.245579</td>\n",
       "      <td>-0.335280</td>\n",
       "      <td>0.613120</td>\n",
       "      <td>8.424043</td>\n",
       "      <td>0.230834</td>\n",
       "      <td>0.614212</td>\n",
       "      <td>11.627348</td>\n",
       "      <td>1.015813</td>\n",
       "      <td>1.627731</td>\n",
       "      <td>0.032318</td>\n",
       "      <td>0.819126</td>\n",
       "      <td>-0.030998</td>\n",
       "      <td>0.734610</td>\n",
       "      <td>0.458883</td>\n",
       "      <td>0.999964</td>\n",
       "      <td>-12.502044</td>\n",
       "      <td>-11.4205</td>\n",
       "      <td>26.468552</td>\n",
       "      <td>-60.000000</td>\n",
       "      <td>-5.789</td>\n",
       "      <td>54.210999</td>\n",
       "      <td>-1.755855</td>\n",
       "      <td>7.895351</td>\n",
       "      <td>0.057707</td>\n",
       "      <td>0.045360</td>\n",
       "      <td>0.001397</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.33950</td>\n",
       "      <td>0.33950</td>\n",
       "      <td>2.271021</td>\n",
       "      <td>9.186051</td>\n",
       "      <td>-20.185032</td>\n",
       "      <td>-19.868000</td>\n",
       "      <td>24.002327</td>\n",
       "      <td>-60.0</td>\n",
       "      <td>-9.679</td>\n",
       "      <td>50.320999</td>\n",
       "      <td>-1.582331</td>\n",
       "      <td>8.889308</td>\n",
       "      <td>0.258464</td>\n",
       "      <td>0.220905</td>\n",
       "      <td>0.081368</td>\n",
       "      <td>0.06413</td>\n",
       "      <td>6.08277</td>\n",
       "      <td>6.01864</td>\n",
       "      <td>16.673548</td>\n",
       "      <td>325.581085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0.043567</td>\n",
       "      <td>0.745566</td>\n",
       "      <td>0.701470</td>\n",
       "      <td>0.000697</td>\n",
       "      <td>0.373143</td>\n",
       "      <td>0.124595</td>\n",
       "      <td>100.260</td>\n",
       "      <td>0.621661</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.6783</td>\n",
       "      <td>Georgia, US</td>\n",
       "      <td>-83.2230</td>\n",
       "      <td>AWOL</td>\n",
       "      <td>AWOL - A Way Of Life</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.388990</td>\n",
       "      <td>0.386740</td>\n",
       "      <td>0.406370</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.548093</td>\n",
       "      <td>0.720192</td>\n",
       "      <td>0.389257</td>\n",
       "      <td>0.344934</td>\n",
       "      <td>0.361300</td>\n",
       "      <td>0.402543</td>\n",
       "      <td>0.434044</td>\n",
       "      <td>0.388137</td>\n",
       "      <td>0.512487</td>\n",
       "      <td>0.525755</td>\n",
       "      <td>0.425371</td>\n",
       "      <td>0.446896</td>\n",
       "      <td>0.511</td>\n",
       "      <td>0.7720</td>\n",
       "      <td>0.361</td>\n",
       "      <td>0.2880</td>\n",
       "      <td>0.3310</td>\n",
       "      <td>0.3720</td>\n",
       "      <td>0.3590</td>\n",
       "      <td>0.279</td>\n",
       "      <td>0.4430</td>\n",
       "      <td>0.4840</td>\n",
       "      <td>0.368</td>\n",
       "      <td>0.3970</td>\n",
       "      <td>0.081051</td>\n",
       "      <td>0.078300</td>\n",
       "      <td>0.048697</td>\n",
       "      <td>0.056922</td>\n",
       "      <td>0.045264</td>\n",
       "      <td>0.066819</td>\n",
       "      <td>0.094489</td>\n",
       "      <td>0.089250</td>\n",
       "      <td>0.098089</td>\n",
       "      <td>0.084133</td>\n",
       "      <td>0.068866</td>\n",
       "      <td>0.086224</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.016</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.977</td>\n",
       "      <td>0.977</td>\n",
       "      <td>0.976</td>\n",
       "      <td>0.979</td>\n",
       "      <td>0.977</td>\n",
       "      <td>0.980</td>\n",
       "      <td>0.971</td>\n",
       "      <td>0.978</td>\n",
       "      <td>0.960</td>\n",
       "      <td>0.974</td>\n",
       "      <td>0.968</td>\n",
       "      <td>0.984</td>\n",
       "      <td>0.192822</td>\n",
       "      <td>-0.671701</td>\n",
       "      <td>0.716431</td>\n",
       "      <td>1.177679</td>\n",
       "      <td>0.629193</td>\n",
       "      <td>0.478121</td>\n",
       "      <td>0.633280</td>\n",
       "      <td>0.862866</td>\n",
       "      <td>0.344354</td>\n",
       "      <td>0.264090</td>\n",
       "      <td>0.673426</td>\n",
       "      <td>0.461554</td>\n",
       "      <td>-1.079328</td>\n",
       "      <td>-0.744193</td>\n",
       "      <td>0.101992</td>\n",
       "      <td>0.862340</td>\n",
       "      <td>-0.294693</td>\n",
       "      <td>-0.819228</td>\n",
       "      <td>-0.859957</td>\n",
       "      <td>-0.540470</td>\n",
       "      <td>-1.287920</td>\n",
       "      <td>-1.182708</td>\n",
       "      <td>-0.566510</td>\n",
       "      <td>-0.931232</td>\n",
       "      <td>40.274662</td>\n",
       "      <td>6.508874</td>\n",
       "      <td>-7.265673</td>\n",
       "      <td>-4.340635</td>\n",
       "      <td>10.407271</td>\n",
       "      <td>-7.618135</td>\n",
       "      <td>5.729183</td>\n",
       "      <td>0.286108</td>\n",
       "      <td>10.888865</td>\n",
       "      <td>0.241682</td>\n",
       "      <td>-3.912109</td>\n",
       "      <td>1.756583</td>\n",
       "      <td>41.426998</td>\n",
       "      <td>7.836000</td>\n",
       "      <td>-10.480000</td>\n",
       "      <td>-5.742000</td>\n",
       "      <td>10.468000</td>\n",
       "      <td>-10.3800</td>\n",
       "      <td>5.9080</td>\n",
       "      <td>0.9060</td>\n",
       "      <td>12.0290</td>\n",
       "      <td>-1.319</td>\n",
       "      <td>-3.0490</td>\n",
       "      <td>0.9870</td>\n",
       "      <td>27.555271</td>\n",
       "      <td>1956.254395</td>\n",
       "      <td>1382.757202</td>\n",
       "      <td>1596.684204</td>\n",
       "      <td>983.111511</td>\n",
       "      <td>945.404419</td>\n",
       "      <td>659.338928</td>\n",
       "      <td>835.013123</td>\n",
       "      <td>399.154694</td>\n",
       "      <td>483.607666</td>\n",
       "      <td>378.464874</td>\n",
       "      <td>244.344467</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-138.438004</td>\n",
       "      <td>-137.203995</td>\n",
       "      <td>-126.450996</td>\n",
       "      <td>-89.247002</td>\n",
       "      <td>-198.056000</td>\n",
       "      <td>-78.342003</td>\n",
       "      <td>-89.745003</td>\n",
       "      <td>-50.596001</td>\n",
       "      <td>-81.189003</td>\n",
       "      <td>-57.694000</td>\n",
       "      <td>-57.312000</td>\n",
       "      <td>48.240002</td>\n",
       "      <td>211.490005</td>\n",
       "      <td>98.503998</td>\n",
       "      <td>316.550995</td>\n",
       "      <td>92.763000</td>\n",
       "      <td>212.559006</td>\n",
       "      <td>69.366997</td>\n",
       "      <td>126.810997</td>\n",
       "      <td>63.868000</td>\n",
       "      <td>73.842003</td>\n",
       "      <td>60.088001</td>\n",
       "      <td>70.402000</td>\n",
       "      <td>48.240002</td>\n",
       "      <td>349.928009</td>\n",
       "      <td>235.707993</td>\n",
       "      <td>443.001984</td>\n",
       "      <td>182.010010</td>\n",
       "      <td>410.614990</td>\n",
       "      <td>147.709000</td>\n",
       "      <td>216.556000</td>\n",
       "      <td>114.464005</td>\n",
       "      <td>155.031006</td>\n",
       "      <td>117.781998</td>\n",
       "      <td>127.714005</td>\n",
       "      <td>-2.893349</td>\n",
       "      <td>0.052129</td>\n",
       "      <td>0.169777</td>\n",
       "      <td>0.796858</td>\n",
       "      <td>-0.164510</td>\n",
       "      <td>0.464760</td>\n",
       "      <td>-0.211987</td>\n",
       "      <td>0.027119</td>\n",
       "      <td>-0.215240</td>\n",
       "      <td>0.083052</td>\n",
       "      <td>-0.004778</td>\n",
       "      <td>0.114815</td>\n",
       "      <td>12.998166</td>\n",
       "      <td>1.258411</td>\n",
       "      <td>-0.105143</td>\n",
       "      <td>5.284808</td>\n",
       "      <td>-0.250734</td>\n",
       "      <td>4.719755</td>\n",
       "      <td>-0.183342</td>\n",
       "      <td>0.340812</td>\n",
       "      <td>-0.295970</td>\n",
       "      <td>0.099103</td>\n",
       "      <td>0.098723</td>\n",
       "      <td>1.389372</td>\n",
       "      <td>-15.458095</td>\n",
       "      <td>-14.1050</td>\n",
       "      <td>35.955223</td>\n",
       "      <td>-60.000000</td>\n",
       "      <td>-7.248</td>\n",
       "      <td>52.751999</td>\n",
       "      <td>-2.505533</td>\n",
       "      <td>9.716598</td>\n",
       "      <td>0.058608</td>\n",
       "      <td>0.045700</td>\n",
       "      <td>0.001777</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.29497</td>\n",
       "      <td>0.29497</td>\n",
       "      <td>1.827837</td>\n",
       "      <td>5.253727</td>\n",
       "      <td>-24.523119</td>\n",
       "      <td>-24.367001</td>\n",
       "      <td>31.804546</td>\n",
       "      <td>-60.0</td>\n",
       "      <td>-12.582</td>\n",
       "      <td>47.417999</td>\n",
       "      <td>-2.288358</td>\n",
       "      <td>11.527109</td>\n",
       "      <td>0.256821</td>\n",
       "      <td>0.237820</td>\n",
       "      <td>0.060122</td>\n",
       "      <td>0.06014</td>\n",
       "      <td>5.92649</td>\n",
       "      <td>5.86635</td>\n",
       "      <td>16.013849</td>\n",
       "      <td>356.755737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>0.951670</td>\n",
       "      <td>0.658179</td>\n",
       "      <td>0.924525</td>\n",
       "      <td>0.965427</td>\n",
       "      <td>0.115474</td>\n",
       "      <td>0.032985</td>\n",
       "      <td>111.562</td>\n",
       "      <td>0.963590</td>\n",
       "      <td>2008-03-11</td>\n",
       "      <td>Constant Hitmaker</td>\n",
       "      <td>39.9523</td>\n",
       "      <td>Philadelphia, PA, US</td>\n",
       "      <td>-75.1624</td>\n",
       "      <td>Kurt Vile</td>\n",
       "      <td>Constant Hitmaker</td>\n",
       "      <td>2635.0</td>\n",
       "      <td>2544.0</td>\n",
       "      <td>397.0</td>\n",
       "      <td>115691.0</td>\n",
       "      <td>67609.0</td>\n",
       "      <td>0.557339</td>\n",
       "      <td>0.614272</td>\n",
       "      <td>0.798387</td>\n",
       "      <td>0.005158</td>\n",
       "      <td>0.354516</td>\n",
       "      <td>0.311404</td>\n",
       "      <td>0.711402</td>\n",
       "      <td>0.321914</td>\n",
       "      <td>0.500601</td>\n",
       "      <td>0.250963</td>\n",
       "      <td>0.321316</td>\n",
       "      <td>0.734250</td>\n",
       "      <td>0.325188</td>\n",
       "      <td>0.373012</td>\n",
       "      <td>0.235840</td>\n",
       "      <td>0.368756</td>\n",
       "      <td>0.440775</td>\n",
       "      <td>0.263</td>\n",
       "      <td>0.7360</td>\n",
       "      <td>0.273</td>\n",
       "      <td>0.4260</td>\n",
       "      <td>0.2140</td>\n",
       "      <td>0.2880</td>\n",
       "      <td>0.8100</td>\n",
       "      <td>0.246</td>\n",
       "      <td>0.2950</td>\n",
       "      <td>0.1640</td>\n",
       "      <td>0.311</td>\n",
       "      <td>0.3860</td>\n",
       "      <td>0.033969</td>\n",
       "      <td>0.070692</td>\n",
       "      <td>0.039161</td>\n",
       "      <td>0.095781</td>\n",
       "      <td>0.024102</td>\n",
       "      <td>0.028497</td>\n",
       "      <td>0.073847</td>\n",
       "      <td>0.045103</td>\n",
       "      <td>0.065468</td>\n",
       "      <td>0.041634</td>\n",
       "      <td>0.041619</td>\n",
       "      <td>0.084442</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.019</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.973</td>\n",
       "      <td>0.919</td>\n",
       "      <td>0.965</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.967</td>\n",
       "      <td>0.992</td>\n",
       "      <td>0.901</td>\n",
       "      <td>0.962</td>\n",
       "      <td>0.978</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.960</td>\n",
       "      <td>0.981</td>\n",
       "      <td>1.373789</td>\n",
       "      <td>-0.422791</td>\n",
       "      <td>1.210829</td>\n",
       "      <td>0.401990</td>\n",
       "      <td>1.838612</td>\n",
       "      <td>1.161654</td>\n",
       "      <td>-0.547191</td>\n",
       "      <td>1.386647</td>\n",
       "      <td>1.216524</td>\n",
       "      <td>2.320900</td>\n",
       "      <td>1.221092</td>\n",
       "      <td>0.474827</td>\n",
       "      <td>2.117700</td>\n",
       "      <td>-1.080739</td>\n",
       "      <td>1.304154</td>\n",
       "      <td>-1.201286</td>\n",
       "      <td>4.986209</td>\n",
       "      <td>1.730812</td>\n",
       "      <td>-1.124398</td>\n",
       "      <td>1.428501</td>\n",
       "      <td>0.490017</td>\n",
       "      <td>5.137411</td>\n",
       "      <td>1.125351</td>\n",
       "      <td>-0.976438</td>\n",
       "      <td>46.439899</td>\n",
       "      <td>6.738814</td>\n",
       "      <td>72.586601</td>\n",
       "      <td>20.438717</td>\n",
       "      <td>-32.377827</td>\n",
       "      <td>-3.392143</td>\n",
       "      <td>-1.747230</td>\n",
       "      <td>-9.380472</td>\n",
       "      <td>1.754843</td>\n",
       "      <td>-11.147522</td>\n",
       "      <td>5.879513</td>\n",
       "      <td>3.324857</td>\n",
       "      <td>47.187000</td>\n",
       "      <td>3.536000</td>\n",
       "      <td>74.306000</td>\n",
       "      <td>19.589001</td>\n",
       "      <td>-36.053001</td>\n",
       "      <td>-3.4970</td>\n",
       "      <td>-0.4180</td>\n",
       "      <td>-9.8420</td>\n",
       "      <td>0.8980</td>\n",
       "      <td>-9.960</td>\n",
       "      <td>6.1200</td>\n",
       "      <td>3.9110</td>\n",
       "      <td>22.193727</td>\n",
       "      <td>1392.732788</td>\n",
       "      <td>572.282837</td>\n",
       "      <td>694.788147</td>\n",
       "      <td>880.852112</td>\n",
       "      <td>344.104462</td>\n",
       "      <td>395.002258</td>\n",
       "      <td>306.603455</td>\n",
       "      <td>162.284821</td>\n",
       "      <td>283.120117</td>\n",
       "      <td>125.487427</td>\n",
       "      <td>139.459778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-174.085999</td>\n",
       "      <td>-61.278999</td>\n",
       "      <td>-95.272003</td>\n",
       "      <td>-91.910004</td>\n",
       "      <td>-56.064999</td>\n",
       "      <td>-101.487999</td>\n",
       "      <td>-70.795998</td>\n",
       "      <td>-40.389999</td>\n",
       "      <td>-58.234001</td>\n",
       "      <td>-95.463997</td>\n",
       "      <td>-43.458000</td>\n",
       "      <td>52.193001</td>\n",
       "      <td>171.130005</td>\n",
       "      <td>131.085999</td>\n",
       "      <td>298.596985</td>\n",
       "      <td>159.093002</td>\n",
       "      <td>105.116997</td>\n",
       "      <td>73.433998</td>\n",
       "      <td>79.760002</td>\n",
       "      <td>49.598999</td>\n",
       "      <td>63.230999</td>\n",
       "      <td>69.753998</td>\n",
       "      <td>35.388000</td>\n",
       "      <td>52.193001</td>\n",
       "      <td>345.216003</td>\n",
       "      <td>192.364990</td>\n",
       "      <td>393.868988</td>\n",
       "      <td>251.003006</td>\n",
       "      <td>161.181992</td>\n",
       "      <td>174.921997</td>\n",
       "      <td>150.556000</td>\n",
       "      <td>89.988998</td>\n",
       "      <td>121.464996</td>\n",
       "      <td>165.217987</td>\n",
       "      <td>78.846001</td>\n",
       "      <td>-4.515986</td>\n",
       "      <td>0.082999</td>\n",
       "      <td>-0.471422</td>\n",
       "      <td>2.171539</td>\n",
       "      <td>1.747735</td>\n",
       "      <td>0.435429</td>\n",
       "      <td>-0.603002</td>\n",
       "      <td>0.200987</td>\n",
       "      <td>0.127110</td>\n",
       "      <td>-0.005297</td>\n",
       "      <td>-0.956349</td>\n",
       "      <td>-0.287195</td>\n",
       "      <td>30.331905</td>\n",
       "      <td>2.051292</td>\n",
       "      <td>1.123436</td>\n",
       "      <td>22.177616</td>\n",
       "      <td>7.889378</td>\n",
       "      <td>1.809147</td>\n",
       "      <td>2.219095</td>\n",
       "      <td>1.518430</td>\n",
       "      <td>0.654815</td>\n",
       "      <td>0.650727</td>\n",
       "      <td>12.656473</td>\n",
       "      <td>0.406731</td>\n",
       "      <td>-10.244890</td>\n",
       "      <td>-9.4640</td>\n",
       "      <td>20.304308</td>\n",
       "      <td>-60.000000</td>\n",
       "      <td>-5.027</td>\n",
       "      <td>54.973000</td>\n",
       "      <td>-5.365219</td>\n",
       "      <td>41.201279</td>\n",
       "      <td>0.048938</td>\n",
       "      <td>0.040800</td>\n",
       "      <td>0.002591</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.89574</td>\n",
       "      <td>0.89574</td>\n",
       "      <td>10.539709</td>\n",
       "      <td>150.359985</td>\n",
       "      <td>-16.472773</td>\n",
       "      <td>-15.903000</td>\n",
       "      <td>27.539440</td>\n",
       "      <td>-60.0</td>\n",
       "      <td>-9.025</td>\n",
       "      <td>50.974998</td>\n",
       "      <td>-3.662988</td>\n",
       "      <td>21.508228</td>\n",
       "      <td>0.283352</td>\n",
       "      <td>0.267070</td>\n",
       "      <td>0.125704</td>\n",
       "      <td>0.08082</td>\n",
       "      <td>8.41401</td>\n",
       "      <td>8.33319</td>\n",
       "      <td>21.317064</td>\n",
       "      <td>483.403809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>134</td>\n",
       "      <td>0.452217</td>\n",
       "      <td>0.513238</td>\n",
       "      <td>0.560410</td>\n",
       "      <td>0.019443</td>\n",
       "      <td>0.096567</td>\n",
       "      <td>0.525519</td>\n",
       "      <td>114.290</td>\n",
       "      <td>0.894072</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.6783</td>\n",
       "      <td>Georgia, US</td>\n",
       "      <td>-83.2230</td>\n",
       "      <td>AWOL</td>\n",
       "      <td>AWOL - A Way Of Life</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.388990</td>\n",
       "      <td>0.386740</td>\n",
       "      <td>0.406370</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.610849</td>\n",
       "      <td>0.569169</td>\n",
       "      <td>0.428494</td>\n",
       "      <td>0.345796</td>\n",
       "      <td>0.376920</td>\n",
       "      <td>0.460590</td>\n",
       "      <td>0.401371</td>\n",
       "      <td>0.449900</td>\n",
       "      <td>0.428946</td>\n",
       "      <td>0.446736</td>\n",
       "      <td>0.479849</td>\n",
       "      <td>0.378221</td>\n",
       "      <td>0.614</td>\n",
       "      <td>0.5450</td>\n",
       "      <td>0.363</td>\n",
       "      <td>0.2800</td>\n",
       "      <td>0.3110</td>\n",
       "      <td>0.3970</td>\n",
       "      <td>0.3170</td>\n",
       "      <td>0.404</td>\n",
       "      <td>0.3560</td>\n",
       "      <td>0.3800</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.2920</td>\n",
       "      <td>0.085176</td>\n",
       "      <td>0.092242</td>\n",
       "      <td>0.073183</td>\n",
       "      <td>0.056354</td>\n",
       "      <td>0.062012</td>\n",
       "      <td>0.088343</td>\n",
       "      <td>0.077084</td>\n",
       "      <td>0.097942</td>\n",
       "      <td>0.101790</td>\n",
       "      <td>0.094533</td>\n",
       "      <td>0.089367</td>\n",
       "      <td>0.088544</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.007</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.988</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.990</td>\n",
       "      <td>0.985</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.994</td>\n",
       "      <td>0.984</td>\n",
       "      <td>0.986</td>\n",
       "      <td>0.987</td>\n",
       "      <td>0.993</td>\n",
       "      <td>-0.099188</td>\n",
       "      <td>0.033726</td>\n",
       "      <td>0.699476</td>\n",
       "      <td>1.039602</td>\n",
       "      <td>0.898852</td>\n",
       "      <td>0.505001</td>\n",
       "      <td>0.625378</td>\n",
       "      <td>0.394606</td>\n",
       "      <td>0.554671</td>\n",
       "      <td>0.524494</td>\n",
       "      <td>0.358942</td>\n",
       "      <td>0.864621</td>\n",
       "      <td>-1.255710</td>\n",
       "      <td>-1.326045</td>\n",
       "      <td>-0.515872</td>\n",
       "      <td>0.517893</td>\n",
       "      <td>-0.001662</td>\n",
       "      <td>-0.967700</td>\n",
       "      <td>-0.747463</td>\n",
       "      <td>-1.202028</td>\n",
       "      <td>-1.038381</td>\n",
       "      <td>-1.039335</td>\n",
       "      <td>-1.076293</td>\n",
       "      <td>-0.390393</td>\n",
       "      <td>40.358280</td>\n",
       "      <td>24.115547</td>\n",
       "      <td>20.227261</td>\n",
       "      <td>6.252143</td>\n",
       "      <td>-21.950306</td>\n",
       "      <td>5.266994</td>\n",
       "      <td>-5.009126</td>\n",
       "      <td>0.108464</td>\n",
       "      <td>12.665721</td>\n",
       "      <td>11.327140</td>\n",
       "      <td>-4.692913</td>\n",
       "      <td>-0.311117</td>\n",
       "      <td>41.785000</td>\n",
       "      <td>19.591999</td>\n",
       "      <td>7.035000</td>\n",
       "      <td>1.393000</td>\n",
       "      <td>-23.080000</td>\n",
       "      <td>-2.6930</td>\n",
       "      <td>-3.4090</td>\n",
       "      <td>-1.2800</td>\n",
       "      <td>13.7490</td>\n",
       "      <td>11.034</td>\n",
       "      <td>-3.8390</td>\n",
       "      <td>-1.8220</td>\n",
       "      <td>26.853050</td>\n",
       "      <td>4123.289551</td>\n",
       "      <td>4842.126953</td>\n",
       "      <td>2033.496216</td>\n",
       "      <td>1603.765381</td>\n",
       "      <td>1318.972656</td>\n",
       "      <td>709.537720</td>\n",
       "      <td>1050.203857</td>\n",
       "      <td>758.473206</td>\n",
       "      <td>563.139587</td>\n",
       "      <td>366.803772</td>\n",
       "      <td>409.632477</td>\n",
       "      <td>10.065</td>\n",
       "      <td>-162.100998</td>\n",
       "      <td>-137.953995</td>\n",
       "      <td>-137.983002</td>\n",
       "      <td>-146.548996</td>\n",
       "      <td>-105.698997</td>\n",
       "      <td>-84.077003</td>\n",
       "      <td>-99.051003</td>\n",
       "      <td>-71.302002</td>\n",
       "      <td>-76.054001</td>\n",
       "      <td>-78.458000</td>\n",
       "      <td>-64.713997</td>\n",
       "      <td>47.804001</td>\n",
       "      <td>243.921005</td>\n",
       "      <td>251.854004</td>\n",
       "      <td>212.216003</td>\n",
       "      <td>107.835999</td>\n",
       "      <td>192.679001</td>\n",
       "      <td>82.653999</td>\n",
       "      <td>107.737000</td>\n",
       "      <td>107.257004</td>\n",
       "      <td>89.804001</td>\n",
       "      <td>75.475998</td>\n",
       "      <td>71.638000</td>\n",
       "      <td>37.739002</td>\n",
       "      <td>406.022003</td>\n",
       "      <td>389.807983</td>\n",
       "      <td>350.199005</td>\n",
       "      <td>254.384995</td>\n",
       "      <td>298.377991</td>\n",
       "      <td>166.731003</td>\n",
       "      <td>206.787994</td>\n",
       "      <td>178.559006</td>\n",
       "      <td>165.858002</td>\n",
       "      <td>153.933990</td>\n",
       "      <td>136.351990</td>\n",
       "      <td>-1.652579</td>\n",
       "      <td>0.301870</td>\n",
       "      <td>0.665983</td>\n",
       "      <td>0.784960</td>\n",
       "      <td>0.107662</td>\n",
       "      <td>1.039750</td>\n",
       "      <td>-0.137514</td>\n",
       "      <td>0.217578</td>\n",
       "      <td>-0.044490</td>\n",
       "      <td>0.011159</td>\n",
       "      <td>-0.265695</td>\n",
       "      <td>0.331218</td>\n",
       "      <td>3.168006</td>\n",
       "      <td>0.141561</td>\n",
       "      <td>-0.047710</td>\n",
       "      <td>1.916984</td>\n",
       "      <td>-0.139364</td>\n",
       "      <td>2.251030</td>\n",
       "      <td>-0.224826</td>\n",
       "      <td>0.050703</td>\n",
       "      <td>0.188019</td>\n",
       "      <td>0.249750</td>\n",
       "      <td>0.931698</td>\n",
       "      <td>0.766069</td>\n",
       "      <td>-15.145472</td>\n",
       "      <td>-14.1510</td>\n",
       "      <td>19.988146</td>\n",
       "      <td>-40.209999</td>\n",
       "      <td>-7.351</td>\n",
       "      <td>32.859001</td>\n",
       "      <td>-1.632508</td>\n",
       "      <td>3.340982</td>\n",
       "      <td>0.059470</td>\n",
       "      <td>0.048560</td>\n",
       "      <td>0.001586</td>\n",
       "      <td>0.01079</td>\n",
       "      <td>0.42006</td>\n",
       "      <td>0.40927</td>\n",
       "      <td>2.763948</td>\n",
       "      <td>13.718324</td>\n",
       "      <td>-24.336575</td>\n",
       "      <td>-22.448999</td>\n",
       "      <td>52.783905</td>\n",
       "      <td>-60.0</td>\n",
       "      <td>-13.128</td>\n",
       "      <td>46.872002</td>\n",
       "      <td>-1.452696</td>\n",
       "      <td>2.356398</td>\n",
       "      <td>0.234686</td>\n",
       "      <td>0.199550</td>\n",
       "      <td>0.149332</td>\n",
       "      <td>0.06440</td>\n",
       "      <td>11.26707</td>\n",
       "      <td>11.20267</td>\n",
       "      <td>26.454180</td>\n",
       "      <td>751.147705</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  track_id  acousticness  danceability    energy  instrumentalness  liveness  \\\n",
       "1        2      0.416675      0.675894  0.634476          0.010628  0.177647   \n",
       "2        3      0.374408      0.528643  0.817461          0.001851  0.105880   \n",
       "3        5      0.043567      0.745566  0.701470          0.000697  0.373143   \n",
       "4       10      0.951670      0.658179  0.924525          0.965427  0.115474   \n",
       "5      134      0.452217      0.513238  0.560410          0.019443  0.096567   \n",
       "\n",
       "   speechiness    tempo   valence  album_date         album_name  \\\n",
       "1     0.159310  165.922  0.576661         NaN                NaN   \n",
       "2     0.461818  126.957  0.269240         NaN                NaN   \n",
       "3     0.124595  100.260  0.621661         NaN                NaN   \n",
       "4     0.032985  111.562  0.963590  2008-03-11  Constant Hitmaker   \n",
       "5     0.525519  114.290  0.894072         NaN                NaN   \n",
       "\n",
       "   artist_latitude       artist_location  artist_longitude artist_name  \\\n",
       "1          32.6783           Georgia, US          -83.2230        AWOL   \n",
       "2          32.6783           Georgia, US          -83.2230        AWOL   \n",
       "3          32.6783           Georgia, US          -83.2230        AWOL   \n",
       "4          39.9523  Philadelphia, PA, US          -75.1624   Kurt Vile   \n",
       "5          32.6783           Georgia, US          -83.2230        AWOL   \n",
       "\n",
       "                release  artist_discovery_rank  artist_familiarity_rank  \\\n",
       "1  AWOL - A Way Of Life                    NaN                      NaN   \n",
       "2  AWOL - A Way Of Life                    NaN                      NaN   \n",
       "3  AWOL - A Way Of Life                    NaN                      NaN   \n",
       "4     Constant Hitmaker                 2635.0                   2544.0   \n",
       "5  AWOL - A Way Of Life                    NaN                      NaN   \n",
       "\n",
       "   artist_hotttnesss_rank  song_currency_rank  song_hotttnesss_rank  \\\n",
       "1                     NaN                 NaN                   NaN   \n",
       "2                     NaN                 NaN                   NaN   \n",
       "3                     NaN                 NaN                   NaN   \n",
       "4                   397.0            115691.0               67609.0   \n",
       "5                     NaN                 NaN                   NaN   \n",
       "\n",
       "   artist_discovery  artist_familiarity  artist_hotttnesss  song_currency  \\\n",
       "1          0.388990            0.386740           0.406370       0.000000   \n",
       "2          0.388990            0.386740           0.406370       0.000000   \n",
       "3          0.388990            0.386740           0.406370       0.000000   \n",
       "4          0.557339            0.614272           0.798387       0.005158   \n",
       "5          0.388990            0.386740           0.406370       0.000000   \n",
       "\n",
       "   song_hotttnesss       000       001       002       003       004  \\\n",
       "1         0.000000  0.877233  0.588911  0.354243  0.295090  0.298413   \n",
       "2         0.000000  0.534429  0.537414  0.443299  0.390879  0.344573   \n",
       "3         0.000000  0.548093  0.720192  0.389257  0.344934  0.361300   \n",
       "4         0.354516  0.311404  0.711402  0.321914  0.500601  0.250963   \n",
       "5         0.000000  0.610849  0.569169  0.428494  0.345796  0.376920   \n",
       "\n",
       "        005       006       007       008       009       010       011  \\\n",
       "1  0.309430  0.304496  0.334579  0.249495  0.259656  0.318376  0.371974   \n",
       "2  0.366448  0.419455  0.747766  0.460901  0.392379  0.474559  0.406729   \n",
       "3  0.402543  0.434044  0.388137  0.512487  0.525755  0.425371  0.446896   \n",
       "4  0.321316  0.734250  0.325188  0.373012  0.235840  0.368756  0.440775   \n",
       "5  0.460590  0.401371  0.449900  0.428946  0.446736  0.479849  0.378221   \n",
       "\n",
       "     012     013    014     015     016     017     018    019     020  \\\n",
       "1  1.000  0.5710  0.278  0.2100  0.2150  0.2285  0.2375  0.279  0.1685   \n",
       "2  0.506  0.5145  0.387  0.3235  0.2805  0.3135  0.3455  0.898  0.4365   \n",
       "3  0.511  0.7720  0.361  0.2880  0.3310  0.3720  0.3590  0.279  0.4430   \n",
       "4  0.263  0.7360  0.273  0.4260  0.2140  0.2880  0.8100  0.246  0.2950   \n",
       "5  0.614  0.5450  0.363  0.2800  0.3110  0.3970  0.3170  0.404  0.3560   \n",
       "\n",
       "      021    022     023       024       025       026       027       028  \\\n",
       "1  0.1685  0.279  0.3325  0.049848  0.104212  0.060230  0.052290  0.047403   \n",
       "2  0.3385  0.398  0.3480  0.079207  0.083319  0.073595  0.071024  0.056679   \n",
       "3  0.4840  0.368  0.3970  0.081051  0.078300  0.048697  0.056922  0.045264   \n",
       "4  0.1640  0.311  0.3860  0.033969  0.070692  0.039161  0.095781  0.024102   \n",
       "5  0.3800  0.420  0.2920  0.085176  0.092242  0.073183  0.056354  0.062012   \n",
       "\n",
       "        029       030       031       032       033       034       035  \\\n",
       "1  0.052815  0.052733  0.062216  0.051613  0.057399  0.053199  0.062583   \n",
       "2  0.066113  0.073889  0.088100  0.071305  0.059275  0.088222  0.067298   \n",
       "3  0.066819  0.094489  0.089250  0.098089  0.084133  0.068866  0.086224   \n",
       "4  0.028497  0.073847  0.045103  0.065468  0.041634  0.041619  0.084442   \n",
       "5  0.088343  0.077084  0.097942  0.101790  0.094533  0.089367  0.088544   \n",
       "\n",
       "     036    037    038    039    040    041    042    043    044    045  \\\n",
       "1  0.036  0.018  0.017  0.021  0.021  0.010  0.015  0.041  0.010  0.009   \n",
       "2  0.040  0.040  0.029  0.021  0.009  0.020  0.020  0.053  0.022  0.032   \n",
       "3  0.023  0.023  0.024  0.021  0.023  0.020  0.029  0.022  0.040  0.026   \n",
       "4  0.027  0.081  0.035  0.025  0.033  0.008  0.099  0.038  0.022  0.009   \n",
       "5  0.003  0.012  0.003  0.004  0.010  0.015  0.005  0.006  0.016  0.014   \n",
       "\n",
       "     046    047  048  049  050  051  052  053  054  055  056  057  058  059  \\\n",
       "1  0.021  0.013  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0   \n",
       "2  0.034  0.028  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0   \n",
       "3  0.032  0.016  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0   \n",
       "4  0.040  0.019  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0   \n",
       "5  0.013  0.007  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0   \n",
       "\n",
       "     060    061    062    063    064    065    066    067    068    069  \\\n",
       "1  0.964  0.982  0.983  0.979  0.979  0.990  0.985  0.959  0.990  0.991   \n",
       "2  0.960  0.960  0.971  0.979  0.991  0.980  0.980  0.947  0.978  0.968   \n",
       "3  0.977  0.977  0.976  0.979  0.977  0.980  0.971  0.978  0.960  0.974   \n",
       "4  0.973  0.919  0.965  0.975  0.967  0.992  0.901  0.962  0.978  0.991   \n",
       "5  0.997  0.988  0.997  0.996  0.990  0.985  0.995  0.994  0.984  0.986   \n",
       "\n",
       "     070    071       072       073       074       075       076       077  \\\n",
       "1  0.979  0.987 -1.899342 -0.032654  0.878469  1.147538  0.950856  0.948257   \n",
       "2  0.966  0.972  0.197378  0.182772  0.608394  0.785822  0.773479  0.650546   \n",
       "3  0.968  0.984  0.192822 -0.671701  0.716431  1.177679  0.629193  0.478121   \n",
       "4  0.960  0.981  1.373789 -0.422791  1.210829  0.401990  1.838612  1.161654   \n",
       "5  0.987  0.993 -0.099188  0.033726  0.699476  1.039602  0.898852  0.505001   \n",
       "\n",
       "        078       079       080       081       082       083       084  \\\n",
       "1  1.157887  1.147911  1.646318  1.530193  1.197568  0.745673  2.510038   \n",
       "2  0.574605 -0.767804  0.326266  0.808435  0.506893  0.682139 -1.157548   \n",
       "3  0.633280  0.862866  0.344354  0.264090  0.673426  0.461554 -1.079328   \n",
       "4 -0.547191  1.386647  1.216524  2.320900  1.221092  0.474827  2.117700   \n",
       "5  0.625378  0.394606  0.554671  0.524494  0.358942  0.864621 -1.255710   \n",
       "\n",
       "        085       086       087       088       089       090       091  \\\n",
       "1 -1.500183  0.030540  0.694242  0.170432  0.064695  0.874727  0.722576   \n",
       "2 -1.170147 -0.683188 -0.366574 -0.245357 -0.547616 -0.809711 -0.894495   \n",
       "3 -0.744193  0.101992  0.862340 -0.294693 -0.819228 -0.859957 -0.540470   \n",
       "4 -1.080739  1.304154 -1.201286  4.986209  1.730812 -1.124398  1.428501   \n",
       "5 -1.326045 -0.515872  0.517893 -0.001662 -0.967700 -0.747463 -1.202028   \n",
       "\n",
       "        092       093       094       095        096        097        098  \\\n",
       "1  2.251320  1.708159  1.054857  0.020675  42.949131  44.387436  32.409389   \n",
       "2 -0.808738 -0.130317 -1.056485 -0.510763  44.097282  48.790241  10.584317   \n",
       "3 -1.287920 -1.182708 -0.566510 -0.931232  40.274662   6.508874  -7.265673   \n",
       "4  0.490017  5.137411  1.125351 -0.976438  46.439899   6.738814  72.586601   \n",
       "5 -1.038381 -1.039335 -1.076293 -0.390393  40.358280  24.115547  20.227261   \n",
       "\n",
       "         099        100       101       102       103        104        105  \\\n",
       "1  15.668667  10.114028 -4.069252  2.042353  2.188321  -3.805923  -0.494699   \n",
       "2  -2.565454  -1.292831 -4.490149 -0.633741  2.112556  -7.288055  -5.132599   \n",
       "3  -4.340635  10.407271 -7.618135  5.729183  0.286108  10.888865   0.241682   \n",
       "4  20.438717 -32.377827 -3.392143 -1.747230 -9.380472   1.754843 -11.147522   \n",
       "5   6.252143 -21.950306  5.266994 -5.009126  0.108464  12.665721  11.327140   \n",
       "\n",
       "        106        107        108        109        110        111        112  \\\n",
       "1  6.024670  10.692599  44.442501  42.388500  31.684999   9.987500   9.568501   \n",
       "2 -2.390409  11.447978  45.414501  51.979000   6.598000   0.280000  -0.012000   \n",
       "3 -3.912109   1.756583  41.426998   7.836000 -10.480000  -5.742000  10.468000   \n",
       "4  5.879513   3.324857  47.187000   3.536000  74.306000  19.589001 -36.053001   \n",
       "5 -4.692913  -0.311117  41.785000  19.591999   7.035000   1.393000 -23.080000   \n",
       "\n",
       "       113     114     115      116     117     118      119        120  \\\n",
       "1  -7.1485  3.8315  1.8505  -2.6875  -0.800  5.4615  10.2565  39.494820   \n",
       "2  -8.0495 -0.6830  1.0310  -7.0775  -4.321 -0.9275   9.0895  22.519407   \n",
       "3 -10.3800  5.9080  0.9060  12.0290  -1.319 -3.0490   0.9870  27.555271   \n",
       "4  -3.4970 -0.4180 -9.8420   0.8980  -9.960  6.1200   3.9110  22.193727   \n",
       "5  -2.6930 -3.4090 -1.2800  13.7490  11.034 -3.8390  -1.8220  26.853050   \n",
       "\n",
       "           121          122          123          124          125  \\\n",
       "1  1966.979126  1825.123047  1903.756714   828.810059   911.155823   \n",
       "2  1694.821167  1256.443848  1251.588257   907.545166   588.602051   \n",
       "3  1956.254395  1382.757202  1596.684204   983.111511   945.404419   \n",
       "4  1392.732788   572.282837   694.788147   880.852112   344.104462   \n",
       "5  4123.289551  4842.126953  2033.496216  1603.765381  1318.972656   \n",
       "\n",
       "          126          127         128         129         130         131  \\\n",
       "1  581.015320   722.001404  404.682556  315.528473  376.632416  229.282547   \n",
       "2  619.971680   679.588013  301.723358  401.670532  294.221619  411.299500   \n",
       "3  659.338928   835.013123  399.154694  483.607666  378.464874  244.344467   \n",
       "4  395.002258   306.603455  162.284821  283.120117  125.487427  139.459778   \n",
       "5  709.537720  1050.203857  758.473206  563.139587  366.803772  409.632477   \n",
       "\n",
       "      132         133         134         135         136         137  \\\n",
       "1   0.000 -110.367996 -100.605003 -112.581001  -75.882004  -89.160004   \n",
       "2   0.000  -82.774002 -137.591003 -131.729004 -106.242996  -73.445000   \n",
       "3   0.000 -138.438004 -137.203995 -126.450996  -89.247002 -198.056000   \n",
       "4   0.000 -174.085999  -61.278999  -95.272003  -91.910004  -56.064999   \n",
       "5  10.065 -162.100998 -137.953995 -137.983002 -146.548996 -105.698997   \n",
       "\n",
       "          138        139        140         141        142        143  \\\n",
       "1  -80.737999 -91.498001 -66.649002  -61.845001 -66.081001 -58.043999   \n",
       "2  -83.468002 -81.322998 -71.073997 -108.565002 -71.498001 -51.351002   \n",
       "3  -78.342003 -89.745003 -50.596001  -81.189003 -57.694000 -57.312000   \n",
       "4 -101.487999 -70.795998 -40.389999  -58.234001 -95.463997 -43.458000   \n",
       "5  -84.077003 -99.051003 -71.302002  -76.054001 -78.458000 -64.713997   \n",
       "\n",
       "         144         145         146         147         148         149  \\\n",
       "1  52.006001  216.237000  208.423004  145.194000   97.482002   98.723000   \n",
       "2  51.366001  190.334000  114.844002  359.941986  101.862000  111.082001   \n",
       "3  48.240002  211.490005   98.503998  316.550995   92.763000  212.559006   \n",
       "4  52.193001  171.130005  131.085999  298.596985  159.093002  105.116997   \n",
       "5  47.804001  243.921005  251.854004  212.216003  107.835999  192.679001   \n",
       "\n",
       "         150         151         152        153        154        155  \\\n",
       "1  68.091003  101.588997   69.505997  58.227001  69.262001  58.175999   \n",
       "2  78.648003  103.000999   46.223000  53.993999  61.181000  90.429001   \n",
       "3  69.366997  126.810997   63.868000  73.842003  60.088001  70.402000   \n",
       "4  73.433998   79.760002   49.598999  63.230999  69.753998  35.388000   \n",
       "5  82.653999  107.737000  107.257004  89.804001  75.475998  71.638000   \n",
       "\n",
       "         156         157         158         159         160         161  \\\n",
       "1  52.006001  326.604980  309.028015  257.774994  173.364014  187.882996   \n",
       "2  51.366001  273.108002  252.434998  491.670990  208.104996  184.527008   \n",
       "3  48.240002  349.928009  235.707993  443.001984  182.010010  410.614990   \n",
       "4  52.193001  345.216003  192.364990  393.868988  251.003006  161.181992   \n",
       "5  37.739002  406.022003  389.807983  350.199005  254.384995  298.377991   \n",
       "\n",
       "          162         163         164         165         166         167  \\\n",
       "1  148.829010  193.087006  136.154999  120.072006  135.343002  116.220001   \n",
       "2  162.115997  184.324005  117.296997  162.559006  132.679001  141.779999   \n",
       "3  147.709000  216.556000  114.464005  155.031006  117.781998  127.714005   \n",
       "4  174.921997  150.556000   89.988998  121.464996  165.217987   78.846001   \n",
       "5  166.731003  206.787994  178.559006  165.858002  153.933990  136.351990   \n",
       "\n",
       "        168       169       170       171       172       173       174  \\\n",
       "1 -2.952152  0.060379  0.525976  0.365915  0.018182  0.454431 -0.330007   \n",
       "2 -1.827564 -0.083561  0.162382  0.829534 -0.164874  0.897740 -0.058807   \n",
       "3 -2.893349  0.052129  0.169777  0.796858 -0.164510  0.464760 -0.211987   \n",
       "4 -4.515986  0.082999 -0.471422  2.171539  1.747735  0.435429 -0.603002   \n",
       "5 -1.652579  0.301870  0.665983  0.784960  0.107662  1.039750 -0.137514   \n",
       "\n",
       "        175       176       177       178       179        180       181  \\\n",
       "1  0.149395 -0.214859  0.030427 -0.153877 -0.150132  13.206213  1.009934   \n",
       "2  0.365381 -0.131389 -0.245579 -0.335280  0.613120   8.424043  0.230834   \n",
       "3  0.027119 -0.215240  0.083052 -0.004778  0.114815  12.998166  1.258411   \n",
       "4  0.200987  0.127110 -0.005297 -0.956349 -0.287195  30.331905  2.051292   \n",
       "5  0.217578 -0.044490  0.011159 -0.265695  0.331218   3.168006  0.141561   \n",
       "\n",
       "        182        183       184       185       186       187       188  \\\n",
       "1  1.577194   0.337023  0.097149  0.401260  0.006324  0.643486  0.012059   \n",
       "2  0.614212  11.627348  1.015813  1.627731  0.032318  0.819126 -0.030998   \n",
       "3 -0.105143   5.284808 -0.250734  4.719755 -0.183342  0.340812 -0.295970   \n",
       "4  1.123436  22.177616  7.889378  1.809147  2.219095  1.518430  0.654815   \n",
       "5 -0.047710   1.916984 -0.139364  2.251030 -0.224826  0.050703  0.188019   \n",
       "\n",
       "        189        190       191        192      193        194        195  \\\n",
       "1  0.237947   0.655938  1.213864 -12.486146 -11.2695  46.031261 -60.000000   \n",
       "2  0.734610   0.458883  0.999964 -12.502044 -11.4205  26.468552 -60.000000   \n",
       "3  0.099103   0.098723  1.389372 -15.458095 -14.1050  35.955223 -60.000000   \n",
       "4  0.650727  12.656473  0.406731 -10.244890  -9.4640  20.304308 -60.000000   \n",
       "5  0.249750   0.931698  0.766069 -15.145472 -14.1510  19.988146 -40.209999   \n",
       "\n",
       "     196        197       198        199       200       201       202  \\\n",
       "1 -3.933  56.067001 -2.587475  11.802585  0.047970  0.038275  0.000988   \n",
       "2 -5.789  54.210999 -1.755855   7.895351  0.057707  0.045360  0.001397   \n",
       "3 -7.248  52.751999 -2.505533   9.716598  0.058608  0.045700  0.001777   \n",
       "4 -5.027  54.973000 -5.365219  41.201279  0.048938  0.040800  0.002591   \n",
       "5 -7.351  32.859001 -1.632508   3.340982  0.059470  0.048560  0.001586   \n",
       "\n",
       "       203      204      205        206         207        208        209  \\\n",
       "1  0.00000  0.20730  0.20730   1.603659    2.984276 -21.812077 -20.312000   \n",
       "2  0.00000  0.33950  0.33950   2.271021    9.186051 -20.185032 -19.868000   \n",
       "3  0.00000  0.29497  0.29497   1.827837    5.253727 -24.523119 -24.367001   \n",
       "4  0.00000  0.89574  0.89574  10.539709  150.359985 -16.472773 -15.903000   \n",
       "5  0.01079  0.42006  0.40927   2.763948   13.718324 -24.336575 -22.448999   \n",
       "\n",
       "         210   211     212        213       214        215       216  \\\n",
       "1  49.157482 -60.0  -9.691  50.308998 -1.992303   6.805694  0.233070   \n",
       "2  24.002327 -60.0  -9.679  50.320999 -1.582331   8.889308  0.258464   \n",
       "3  31.804546 -60.0 -12.582  47.417999 -2.288358  11.527109  0.256821   \n",
       "4  27.539440 -60.0  -9.025  50.974998 -3.662988  21.508228  0.283352   \n",
       "5  52.783905 -60.0 -13.128  46.872002 -1.452696   2.356398  0.234686   \n",
       "\n",
       "        217       218      219       220       221        222         223  \n",
       "1  0.192880  0.027455  0.06408   3.67696   3.61288  13.316690  262.929749  \n",
       "2  0.220905  0.081368  0.06413   6.08277   6.01864  16.673548  325.581085  \n",
       "3  0.237820  0.060122  0.06014   5.92649   5.86635  16.013849  356.755737  \n",
       "4  0.267070  0.125704  0.08082   8.41401   8.33319  21.317064  483.403809  \n",
       "5  0.199550  0.149332  0.06440  11.26707  11.20267  26.454180  751.147705  "
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tracks = tracks.rename(columns={'Unnamed: 0': 'track_id'})\n",
    "tracks = tracks.drop(tracks.index[0])\n",
    "echonest = echonest.rename(columns={'Unnamed: 0': 'track_id'})\n",
    "echonest = echonest.drop(echonest.index[0])\n",
    "echonest.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((106574, 53), (109727, 39), (164, 5), (14511, 250))"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tracks.shape, raw_tracks.shape, genres.shape, echonest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['track_id', 'comments', 'date_created', 'date_released', 'engineer',\n",
       "       'favorites', 'id', 'information', 'listens', 'producer', 'tags',\n",
       "       'title', 'tracks', 'type', 'active_year_begin', 'active_year_end',\n",
       "       'associated_labels', 'bio', 'comments.1', 'date_created.1',\n",
       "       'favorites.1', 'id.1', 'latitude', 'location', 'longitude', 'members',\n",
       "       'name', 'related_projects', 'tags.1', 'website', 'wikipedia_page',\n",
       "       'split', 'subset', 'bit_rate', 'comments.2', 'composer',\n",
       "       'date_created.2', 'date_recorded', 'duration', 'favorites.2',\n",
       "       'genre_top', 'genres', 'genres_all', 'information.1', 'interest',\n",
       "       'language_code', 'license', 'listens.1', 'lyricist', 'number',\n",
       "       'publisher', 'tags.2', 'title.1'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tracks.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['track_id', 'album_id', 'album_title', 'album_url', 'artist_id',\n",
       "       'artist_name', 'artist_url', 'artist_website', 'license_image_file',\n",
       "       'license_image_file_large', 'license_parent_id', 'license_title',\n",
       "       'license_url', 'tags', 'track_bit_rate', 'track_comments',\n",
       "       'track_composer', 'track_copyright_c', 'track_copyright_p',\n",
       "       'track_date_created', 'track_date_recorded', 'track_disc_number',\n",
       "       'track_duration', 'track_explicit', 'track_explicit_notes',\n",
       "       'track_favorites', 'track_file', 'track_genres', 'track_image_file',\n",
       "       'track_information', 'track_instrumental', 'track_interest',\n",
       "       'track_language_code', 'track_listens', 'track_lyricist',\n",
       "       'track_number', 'track_publisher', 'track_title', 'track_url'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_tracks.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['genre_id', 'genre_color', 'genre_handle', 'genre_parent_id',\n",
       "       'genre_title'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genres.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['track_id', 'acousticness', 'danceability', 'energy',\n",
       "       'instrumentalness', 'liveness', 'speechiness', 'tempo', 'valence',\n",
       "       'album_date',\n",
       "       ...\n",
       "       '214', '215', '216', '217', '218', '219', '220', '221', '222', '223'],\n",
       "      dtype='object', length=250)"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "echonest.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>track_id</th>\n",
       "      <th>artist_id</th>\n",
       "      <th>artist_name</th>\n",
       "      <th>track_duration</th>\n",
       "      <th>genre_top</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>tempo</th>\n",
       "      <th>valence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26533</td>\n",
       "      <td>6718</td>\n",
       "      <td>Roman Stolyar &amp; Ilia Belorukov</td>\n",
       "      <td>11:28</td>\n",
       "      <td>Jazz</td>\n",
       "      <td>0.992052</td>\n",
       "      <td>0.449904</td>\n",
       "      <td>0.187091</td>\n",
       "      <td>0.926967</td>\n",
       "      <td>0.153883</td>\n",
       "      <td>0.184544</td>\n",
       "      <td>82.752</td>\n",
       "      <td>0.143674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26534</td>\n",
       "      <td>6718</td>\n",
       "      <td>Roman Stolyar &amp; Ilia Belorukov</td>\n",
       "      <td>13:02</td>\n",
       "      <td>Jazz</td>\n",
       "      <td>0.988513</td>\n",
       "      <td>0.299413</td>\n",
       "      <td>0.212382</td>\n",
       "      <td>0.472631</td>\n",
       "      <td>0.064746</td>\n",
       "      <td>0.141513</td>\n",
       "      <td>74.872</td>\n",
       "      <td>0.126995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26614</td>\n",
       "      <td>6732</td>\n",
       "      <td>Former Selv</td>\n",
       "      <td>04:05</td>\n",
       "      <td>Electronic</td>\n",
       "      <td>0.031106</td>\n",
       "      <td>0.793853</td>\n",
       "      <td>0.321140</td>\n",
       "      <td>0.885180</td>\n",
       "      <td>0.103475</td>\n",
       "      <td>0.075847</td>\n",
       "      <td>131.997</td>\n",
       "      <td>0.111437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26615</td>\n",
       "      <td>6733</td>\n",
       "      <td>Lissom</td>\n",
       "      <td>05:03</td>\n",
       "      <td>Electronic</td>\n",
       "      <td>0.845697</td>\n",
       "      <td>0.061916</td>\n",
       "      <td>0.015739</td>\n",
       "      <td>0.930647</td>\n",
       "      <td>0.107575</td>\n",
       "      <td>0.040837</td>\n",
       "      <td>191.853</td>\n",
       "      <td>0.037751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26616</td>\n",
       "      <td>6734</td>\n",
       "      <td>Yann Novak</td>\n",
       "      <td>05:32</td>\n",
       "      <td>Electronic</td>\n",
       "      <td>0.804590</td>\n",
       "      <td>0.164799</td>\n",
       "      <td>0.013755</td>\n",
       "      <td>0.934238</td>\n",
       "      <td>0.097474</td>\n",
       "      <td>0.053641</td>\n",
       "      <td>125.515</td>\n",
       "      <td>0.035151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>26617</td>\n",
       "      <td>6735</td>\n",
       "      <td>Clinker</td>\n",
       "      <td>05:44</td>\n",
       "      <td>Electronic</td>\n",
       "      <td>0.705879</td>\n",
       "      <td>0.148969</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.897722</td>\n",
       "      <td>0.086542</td>\n",
       "      <td>NaN</td>\n",
       "      <td>132.192</td>\n",
       "      <td>0.126690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>26620</td>\n",
       "      <td>3148</td>\n",
       "      <td>Kamran Sadeghi</td>\n",
       "      <td>05:00</td>\n",
       "      <td>Electronic</td>\n",
       "      <td>0.915280</td>\n",
       "      <td>0.186919</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>0.903295</td>\n",
       "      <td>0.111539</td>\n",
       "      <td>0.082420</td>\n",
       "      <td>131.667</td>\n",
       "      <td>0.579779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>26638</td>\n",
       "      <td>6742</td>\n",
       "      <td>Flowerheads</td>\n",
       "      <td>02:40</td>\n",
       "      <td>Rock</td>\n",
       "      <td>0.683410</td>\n",
       "      <td>0.375122</td>\n",
       "      <td>0.520872</td>\n",
       "      <td>0.972870</td>\n",
       "      <td>0.111027</td>\n",
       "      <td>0.094277</td>\n",
       "      <td>80.180</td>\n",
       "      <td>0.219157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>26639</td>\n",
       "      <td>6742</td>\n",
       "      <td>Flowerheads</td>\n",
       "      <td>02:30</td>\n",
       "      <td>Rock</td>\n",
       "      <td>0.725093</td>\n",
       "      <td>0.475448</td>\n",
       "      <td>0.448794</td>\n",
       "      <td>0.971753</td>\n",
       "      <td>0.110724</td>\n",
       "      <td>0.035687</td>\n",
       "      <td>119.978</td>\n",
       "      <td>0.188971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>26640</td>\n",
       "      <td>6742</td>\n",
       "      <td>Flowerheads</td>\n",
       "      <td>01:17</td>\n",
       "      <td>Rock</td>\n",
       "      <td>0.907694</td>\n",
       "      <td>0.696932</td>\n",
       "      <td>0.334671</td>\n",
       "      <td>0.967341</td>\n",
       "      <td>0.100748</td>\n",
       "      <td>0.138263</td>\n",
       "      <td>220.256</td>\n",
       "      <td>0.900989</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  track_id  artist_id                     artist_name track_duration  \\\n",
       "0    26533       6718  Roman Stolyar & Ilia Belorukov          11:28   \n",
       "1    26534       6718  Roman Stolyar & Ilia Belorukov          13:02   \n",
       "2    26614       6732                     Former Selv          04:05   \n",
       "3    26615       6733                          Lissom          05:03   \n",
       "4    26616       6734                      Yann Novak          05:32   \n",
       "5    26617       6735                         Clinker          05:44   \n",
       "6    26620       3148                  Kamran Sadeghi          05:00   \n",
       "7    26638       6742                     Flowerheads          02:40   \n",
       "8    26639       6742                     Flowerheads          02:30   \n",
       "9    26640       6742                     Flowerheads          01:17   \n",
       "\n",
       "    genre_top  acousticness  danceability    energy  instrumentalness  \\\n",
       "0        Jazz      0.992052      0.449904  0.187091          0.926967   \n",
       "1        Jazz      0.988513      0.299413  0.212382          0.472631   \n",
       "2  Electronic      0.031106      0.793853  0.321140          0.885180   \n",
       "3  Electronic      0.845697      0.061916  0.015739          0.930647   \n",
       "4  Electronic      0.804590      0.164799  0.013755          0.934238   \n",
       "5  Electronic      0.705879      0.148969  0.000020          0.897722   \n",
       "6  Electronic      0.915280      0.186919  0.000191          0.903295   \n",
       "7        Rock      0.683410      0.375122  0.520872          0.972870   \n",
       "8        Rock      0.725093      0.475448  0.448794          0.971753   \n",
       "9        Rock      0.907694      0.696932  0.334671          0.967341   \n",
       "\n",
       "   liveness  speechiness    tempo   valence  \n",
       "0  0.153883     0.184544   82.752  0.143674  \n",
       "1  0.064746     0.141513   74.872  0.126995  \n",
       "2  0.103475     0.075847  131.997  0.111437  \n",
       "3  0.107575     0.040837  191.853  0.037751  \n",
       "4  0.097474     0.053641  125.515  0.035151  \n",
       "5  0.086542          NaN  132.192  0.126690  \n",
       "6  0.111539     0.082420  131.667  0.579779  \n",
       "7  0.111027     0.094277   80.180  0.219157  \n",
       "8  0.110724     0.035687  119.978  0.188971  \n",
       "9  0.100748     0.138263  220.256  0.900989  "
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine seperate CSVs into a 'master' DataFrame\n",
    "fma_df = raw_tracks[['track_id', 'artist_id', 'artist_name', 'track_duration']]\n",
    "fma_df = pd.merge(fma_df, tracks[['track_id', 'genre_top']], on='track_id', how='inner')\n",
    "fma_df = pd.merge(fma_df, echonest[['track_id','acousticness','danceability','energy',\n",
    "                                    'instrumentalness','liveness','speechiness','tempo','valence']],\n",
    "                  on='track_id', how='inner')\n",
    "fma_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist_id</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>tempo</th>\n",
       "      <th>valence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7029.000000</td>\n",
       "      <td>7.029000e+03</td>\n",
       "      <td>7014.000000</td>\n",
       "      <td>7029.000000</td>\n",
       "      <td>7029.000000</td>\n",
       "      <td>7029.000000</td>\n",
       "      <td>6928.000000</td>\n",
       "      <td>7029.000000</td>\n",
       "      <td>7013.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>9565.798122</td>\n",
       "      <td>4.486711e-01</td>\n",
       "      <td>0.501338</td>\n",
       "      <td>0.562889</td>\n",
       "      <td>0.595034</td>\n",
       "      <td>0.181570</td>\n",
       "      <td>0.092247</td>\n",
       "      <td>122.241313</td>\n",
       "      <td>0.434712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4642.427364</td>\n",
       "      <td>3.812348e-01</td>\n",
       "      <td>0.190171</td>\n",
       "      <td>0.269048</td>\n",
       "      <td>0.381595</td>\n",
       "      <td>0.149778</td>\n",
       "      <td>0.125025</td>\n",
       "      <td>34.282646</td>\n",
       "      <td>0.271589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>9.491000e-07</td>\n",
       "      <td>0.051435</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025916</td>\n",
       "      <td>0.022795</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>7134.000000</td>\n",
       "      <td>4.641772e-02</td>\n",
       "      <td>0.358666</td>\n",
       "      <td>0.364828</td>\n",
       "      <td>0.115453</td>\n",
       "      <td>0.099536</td>\n",
       "      <td>0.036290</td>\n",
       "      <td>95.989000</td>\n",
       "      <td>0.198255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>9076.000000</td>\n",
       "      <td>3.927885e-01</td>\n",
       "      <td>0.506497</td>\n",
       "      <td>0.583819</td>\n",
       "      <td>0.814408</td>\n",
       "      <td>0.117868</td>\n",
       "      <td>0.048258</td>\n",
       "      <td>120.011000</td>\n",
       "      <td>0.411838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>11423.000000</td>\n",
       "      <td>8.506437e-01</td>\n",
       "      <td>0.646359</td>\n",
       "      <td>0.789315</td>\n",
       "      <td>0.909902</td>\n",
       "      <td>0.205161</td>\n",
       "      <td>0.081441</td>\n",
       "      <td>142.746000</td>\n",
       "      <td>0.653344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>20818.000000</td>\n",
       "      <td>9.957965e-01</td>\n",
       "      <td>0.968645</td>\n",
       "      <td>0.999964</td>\n",
       "      <td>0.998016</td>\n",
       "      <td>0.980330</td>\n",
       "      <td>0.964377</td>\n",
       "      <td>249.616000</td>\n",
       "      <td>0.999990</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          artist_id  acousticness  danceability       energy  \\\n",
       "count   7029.000000  7.029000e+03   7014.000000  7029.000000   \n",
       "mean    9565.798122  4.486711e-01      0.501338     0.562889   \n",
       "std     4642.427364  3.812348e-01      0.190171     0.269048   \n",
       "min        4.000000  9.491000e-07      0.051435     0.000020   \n",
       "25%     7134.000000  4.641772e-02      0.358666     0.364828   \n",
       "50%     9076.000000  3.927885e-01      0.506497     0.583819   \n",
       "75%    11423.000000  8.506437e-01      0.646359     0.789315   \n",
       "max    20818.000000  9.957965e-01      0.968645     0.999964   \n",
       "\n",
       "       instrumentalness     liveness  speechiness        tempo      valence  \n",
       "count       7029.000000  7029.000000  6928.000000  7029.000000  7013.000000  \n",
       "mean           0.595034     0.181570     0.092247   122.241313     0.434712  \n",
       "std            0.381595     0.149778     0.125025    34.282646     0.271589  \n",
       "min            0.000000     0.025916     0.022795     0.000000     0.008695  \n",
       "25%            0.115453     0.099536     0.036290    95.989000     0.198255  \n",
       "50%            0.814408     0.117868     0.048258   120.011000     0.411838  \n",
       "75%            0.909902     0.205161     0.081441   142.746000     0.653344  \n",
       "max            0.998016     0.980330     0.964377   249.616000     0.999990  "
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fma_df.describe(include='number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>track_id</th>\n",
       "      <th>artist_name</th>\n",
       "      <th>track_duration</th>\n",
       "      <th>genre_top</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7029</td>\n",
       "      <td>7029</td>\n",
       "      <td>7029</td>\n",
       "      <td>4411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>7029</td>\n",
       "      <td>1666</td>\n",
       "      <td>672</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>107986</td>\n",
       "      <td>51%</td>\n",
       "      <td>03:06</td>\n",
       "      <td>Rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>52</td>\n",
       "      <td>48</td>\n",
       "      <td>1820</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        track_id artist_name track_duration genre_top\n",
       "count       7029        7029           7029      4411\n",
       "unique      7029        1666            672        12\n",
       "top       107986         51%          03:06      Rock\n",
       "freq           1          52             48      1820"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fma_df.describe(exclude='number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(strategy='mean')\n",
    "imputer.fit(fma_df.select_dtypes(include='number'))\n",
    "fma_df_imputed = imputer.transform(fma_df.select_dtypes(include='number'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7029, 9)"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fma_df_imputed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 7029 entries, 0 to 7028\n",
      "Data columns (total 13 columns):\n",
      "track_id            7029 non-null object\n",
      "artist_id           7029 non-null float64\n",
      "artist_name         7029 non-null object\n",
      "track_duration      7029 non-null object\n",
      "genre_top           4411 non-null object\n",
      "acousticness        7029 non-null float64\n",
      "danceability        7029 non-null float64\n",
      "energy              7029 non-null float64\n",
      "instrumentalness    7029 non-null float64\n",
      "liveness            7029 non-null float64\n",
      "speechiness         7029 non-null float64\n",
      "tempo               7029 non-null float64\n",
      "valence             7029 non-null float64\n",
      "dtypes: float64(9), object(4)\n",
      "memory usage: 768.8+ KB\n"
     ]
    }
   ],
   "source": [
    "imputed_features = fma_df.select_dtypes(include='number').columns\n",
    "fma_df[imputed_features] = fma_df_imputed\n",
    "fma_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop observations where there is a np.NaN, then proceed to log-reg\n",
    "fma_df = fma_df.dropna(how='any')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Rock                   0.412605\n",
       "Electronic             0.257311\n",
       "Hip-Hop                0.102471\n",
       "Folk                   0.070279\n",
       "Pop                    0.047382\n",
       "Jazz                   0.029018\n",
       "Classical              0.025391\n",
       "Instrumental           0.019950\n",
       "Old-Time / Historic    0.015189\n",
       "International          0.014056\n",
       "Experimental           0.004081\n",
       "Blues                  0.002267\n",
       "Name: genre_top, dtype: float64"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fma_df['genre_top'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3087, 9), (1324, 9), (3087,), (1324,))"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = fma_df['genre_top']\n",
    "X = fma_df.drop(columns='genre_top').select_dtypes(include='number')\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.70, test_size=0.30, \n",
    "                                                    random_state=42)\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Miniconda\\envs\\JupyterLab\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "D:\\Miniconda\\envs\\JupyterLab\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg = LogisticRegression(solver='lbfgs')\n",
    "log_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy score:  0.5322319403952057\n"
     ]
    }
   ],
   "source": [
    "score = log_reg.score(X_train, y_train)\n",
    "print('Train accuracy score: ', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Miniconda\\envs\\JupyterLab\\lib\\site-packages\\sklearn\\model_selection\\_split.py:652: Warning: The least populated class in y has only 6 members, which is too few. The minimum number of members in any class cannot be less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "D:\\Miniconda\\envs\\JupyterLab\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "D:\\Miniconda\\envs\\JupyterLab\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "D:\\Miniconda\\envs\\JupyterLab\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "D:\\Miniconda\\envs\\JupyterLab\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "D:\\Miniconda\\envs\\JupyterLab\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "D:\\Miniconda\\envs\\JupyterLab\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "D:\\Miniconda\\envs\\JupyterLab\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "D:\\Miniconda\\envs\\JupyterLab\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "D:\\Miniconda\\envs\\JupyterLab\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "D:\\Miniconda\\envs\\JupyterLab\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "D:\\Miniconda\\envs\\JupyterLab\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "D:\\Miniconda\\envs\\JupyterLab\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "D:\\Miniconda\\envs\\JupyterLab\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "D:\\Miniconda\\envs\\JupyterLab\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "D:\\Miniconda\\envs\\JupyterLab\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "D:\\Miniconda\\envs\\JupyterLab\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "D:\\Miniconda\\envs\\JupyterLab\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "D:\\Miniconda\\envs\\JupyterLab\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "D:\\Miniconda\\envs\\JupyterLab\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "D:\\Miniconda\\envs\\JupyterLab\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "D:\\Miniconda\\envs\\JupyterLab\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "D:\\Miniconda\\envs\\JupyterLab\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "D:\\Miniconda\\envs\\JupyterLab\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "D:\\Miniconda\\envs\\JupyterLab\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "D:\\Miniconda\\envs\\JupyterLab\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "D:\\Miniconda\\envs\\JupyterLab\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "D:\\Miniconda\\envs\\JupyterLab\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "D:\\Miniconda\\envs\\JupyterLab\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "D:\\Miniconda\\envs\\JupyterLab\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Accuracy Scores:\n",
      " [0.47133758 0.55414013 0.5        0.55339806 0.56818182 0.5487013\n",
      " 0.54071661 0.56393443 0.50328947 0.54934211]\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(log_reg, X_train, y_train, cv=10)\n",
    "print('Cross-Validation Accuracy Scores:\\n', scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4713375796178344, 0.535304149969664, 0.5681818181818182)"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = pd.Series(scores)\n",
    "scores.min(), scores.mean(), scores.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kQUVlUKQMPPW"
   },
   "source": [
    "This is the biggest data you've played with so far, and while it does generally fit in Colab, it can take awhile to run. That's part of the challenge!\n",
    "\n",
    "Your tasks:\n",
    "- Clean up the variable names in the dataframe\n",
    "- Use logistic regression to fit a model predicting (primary/top) genre\n",
    "- Inspect, iterate, and improve your model\n",
    "- Answer the following questions (written, ~paragraph each):\n",
    "  - What are the best predictors of genre?\n",
    "  - What information isn't very useful for predicting genre?\n",
    "  - What surprised you the most about your results?\n",
    "\n",
    "*Important caveats*:\n",
    "- This is going to be difficult data to work with - don't let the perfect be the enemy of the good!\n",
    "- Be creative in cleaning it up - if the best way you know how to do it is download it locally and edit as a spreadsheet, that's OK!\n",
    "- If the data size becomes problematic, consider sampling/subsetting\n",
    "- You do not need perfect or complete results - just something plausible that runs, and that supports the reasoning in your written answers\n",
    "\n",
    "If you find that fitting a model to classify *all* genres isn't very good, it's totally OK to limit to the most frequent genres, or perhaps trying to combine or cluster genres as a preprocessing step. Even then, there will be limits to how good a model can be with just this metadata - if you really want to train an effective genre classifier, you'll have to involve the other data (see stretch goals).\n",
    "\n",
    "This is real data - there is no \"one correct answer\", so you can take this in a variety of directions. Just make sure to support your findings, and feel free to share them as well! This is meant to be practice for dealing with other \"messy\" data, a common task in data science."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wlI5OXfSag9C"
   },
   "source": [
    "## Resources and stretch goals\n",
    "\n",
    "- Check out the other .csv files from the FMA dataset, and see if you can join them or otherwise fit interesting models with them\n",
    "- [Logistic regression from scratch in numpy](https://blog.goodaudience.com/logistic-regression-from-scratch-in-numpy-5841c09e425f) - if you want to dig in a bit more to both the code and math (also takes a gradient descent approach, introducing the logistic loss function)\n",
    "- Create a visualization to show predictions of your model - ideally show a confidence interval based on error!\n",
    "- Check out and compare classification models from scikit-learn, such as [SVM](https://scikit-learn.org/stable/modules/svm.html#classification), [decision trees](https://scikit-learn.org/stable/modules/tree.html#classification), and [naive Bayes](https://scikit-learn.org/stable/modules/naive_bayes.html). The underlying math will vary significantly, but the API (how you write the code) and interpretation will actually be fairly similar.\n",
    "- Sign up for [Kaggle](https://kaggle.com), and find a competition to try logistic regression with\n",
    "- (Not logistic regression related) If you enjoyed the assignment, you may want to read up on [music informatics](https://en.wikipedia.org/wiki/Music_informatics), which is how those audio features were actually calculated. The FMA includes the actual raw audio, so (while this is more of a longterm project than a stretch goal, and won't fit in Colab) if you'd like you can check those out and see what sort of deeper analysis you can do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@inproceedings{fma_dataset,\n",
    "  title = {FMA: A Dataset for Music Analysis},\n",
    "  author = {Defferrard, Micha\\\"el and Benzi, Kirell and Vandergheynst, Pierre and Bresson, Xavier},\n",
    "  booktitle = {18th International Society for Music Information Retrieval Conference},\n",
    "  year = {2017},\n",
    "  url = {https://arxiv.org/abs/1612.01840},\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "LS_DS_231_Logistic_Regression.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
