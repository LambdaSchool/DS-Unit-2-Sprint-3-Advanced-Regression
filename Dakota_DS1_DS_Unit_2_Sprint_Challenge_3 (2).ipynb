{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ayDccRP01GJD"
   },
   "source": [
    "# Data Science Unit 2 Sprint Challenge 3\n",
    "\n",
    "## Logistic Regression and Beyond\n",
    "\n",
    "In this sprint challenge you will fit a logistic regression modeling the probability of an adult having an income above 50K. The dataset is available at UCI:\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/adult\n",
    "\n",
    "Your goal is to:\n",
    "\n",
    "1. Load, validate, and clean/prepare the data.\n",
    "2. Fit a logistic regression model\n",
    "3. Answer questions based on the results (as well as a few extra questions about the other modules)\n",
    "\n",
    "Don't let the perfect be the enemy of the good! Manage your time, and make sure to get to all parts. If you get stuck wrestling with the data, simplify it (if necessary, drop features or rows) so you're able to move on. If you have time at the end, you can go back and try to fix/improve.\n",
    "\n",
    "### Hints\n",
    "\n",
    "It has a variety of features - some are continuous, but many are categorical. You may find [pandas.get_dummies](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html) (a method to one-hot encode) helpful!\n",
    "\n",
    "The features have dramatically different ranges. You may find [sklearn.preprocessing.minmax_scale](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.minmax_scale.html#sklearn.preprocessing.minmax_scale) helpful!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U22R1Ud51hxb"
   },
   "source": [
    "## Part 1 - Load, validate, and prepare data\n",
    "\n",
    "The data is available at: https://archive.ics.uci.edu/ml/datasets/adult\n",
    "\n",
    "Load it, name the columns, and make sure that you've loaded the data successfully. Note that missing values for categorical variables can essentially be considered another category (\"unknown\"), and may not need to be dropped.\n",
    "\n",
    "You should also prepare the data for logistic regression - one-hot encode categorical features as appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SeOByIkht-NS"
   },
   "outputs": [],
   "source": [
    "#import all the things!!!!\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mms = MinMaxScaler()\n",
    "lr = LogisticRegression()\n",
    "def dummyEncode(df):\n",
    "        columnsToEncode = list(df.select_dtypes(include=['category','object']))\n",
    "        le = LabelEncoder()\n",
    "        for feature in columnsToEncode:\n",
    "            try:\n",
    "                df[feature] = le.fit_transform(df[feature])\n",
    "            except:\n",
    "                print('Error encoding '+feature)\n",
    "        return df\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['age','workclass','fnlwgt','education','education-num','marital-status','occupation','relationship',\n",
    "        'race','sex','capital-gain','capital-loss','hours-per-week','native-country','50k']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">50K, <=50K.\n",
    "\n",
    "age: continuous.\n",
    "workclass: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.\n",
    "fnlwgt: continuous.\n",
    "education: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.\n",
    "education-num: continuous.\n",
    "marital-status: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.\n",
    "occupation: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.\n",
    "relationship: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.\n",
    "race: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black.\n",
    "sex: Female, Male.\n",
    "capital-gain: continuous.\n",
    "capital-loss: continuous.\n",
    "hours-per-week: continuous.\n",
    "native-country: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32561, 15)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data'\n",
    "df1 = pd.read_csv(url, header =None)\n",
    "df1.columns = cols\n",
    "#Encode all objects\n",
    "df2 = dummyEncode(df1)\n",
    "# Scale data with min max transform\n",
    "df = pd.DataFrame(mms.fit_transform(df1))\n",
    "#Check progress\n",
    "df.head()\n",
    "df.describe()\n",
    "# Check for NaN's \n",
    "df.isna().sum()\n",
    "# no nans what about '?'\n",
    "df1 = df1.replace('?',np.nan)\n",
    "df.isna().sum()\n",
    "# check column headers\n",
    "df.columns = cols\n",
    "df.head()\n",
    "#check shape \n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RT1LFnFO1lo6"
   },
   "source": [
    "## Part 2 - Fit and present a Logistic Regression\n",
    "\n",
    "Your data should now be in a state to fit a logistic regression. Use scikit-learn, define your `X` (independent variable) and `y`, and fit a model.\n",
    "\n",
    "Then, present results - display coefficients in as interpretible a way as you can (hint - scaling the numeric features will help, as it will at least make coefficients more comparable to each other). If you find it helpful for interpretation, you can also generate predictions for cases (like our 5 year old rich kid on the Titanic) or make visualizations - but the goal is your exploration to be able to answer the question, not any particular plot (i.e. don't worry about polishing it).\n",
    "\n",
    "It is *optional* to use `train_test_split` or validate your model more generally - that is not the core focus for this week. So, it is suggested you focus on fitting a model first, and if you have time at the end you can do further validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s7fTRDXguD7N",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8216830466830467\n",
      "Index(['age', 'workclass', 'fnlwgt', 'education', 'education-num',\n",
      "       'marital-status', 'occupation', 'relationship', 'race', 'sex',\n",
      "       'capital-gain', 'capital-loss', 'hours-per-week', 'native-country'],\n",
      "      dtype='object')\n",
      "[[ 2.45058752 -0.20568339  0.41509639  0.04329489  4.70335415 -1.38727443\n",
      "   0.19613341 -0.53379997  0.40951506  0.85253637 14.45474339  2.4449571\n",
      "   2.68615816  0.06138984]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.92881513, 0.07118487])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1= df.drop('50k', axis = 1)\n",
    "y1= df['50k']\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X1, y1, test_size=.5, random_state=42)\n",
    "X = X_train\n",
    "y= Y_train\n",
    "log_reg = LogisticRegression().fit(X, y)\n",
    "def sigmoid(x):\n",
    "  return 1 / (1 + np.e**(-x))\n",
    "print(log_reg.score(X,y))\n",
    "print(df.columns.drop('50k'))\n",
    "print(log_reg.coef_)\n",
    "log_reg.predict_proba(X)[0]\n",
    "#sigmoid(log_reg.intercept_ + np.dot(log_reg.coef_, np.transpose(x)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8218782630059579\n",
      "Index(['age', 'workclass', 'fnlwgt', 'education', 'education-num',\n",
      "       'marital-status', 'occupation', 'relationship', 'race', 'sex',\n",
      "       'capital-gain', 'capital-loss', 'hours-per-week', 'native-country'],\n",
      "      dtype='object')\n",
      "[[ 2.39251316 -0.17235352  0.72761037  0.30738876  4.80333261 -1.39007316\n",
      "   0.08286293 -0.7406207   0.30559772  0.81779479 14.18355575  2.82862534\n",
      "   2.77474398  0.06755968]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.88632774, 0.11367226])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X1, y1, test_size=.5, random_state=42)\n",
    "X = X_test\n",
    "y= Y_test\n",
    "log_reg = LogisticRegression().fit(X, y)\n",
    "def sigmoid(x):\n",
    "  return 1 / (1 + np.e**(-x))\n",
    "print(log_reg.score(X,y))\n",
    "print(df.columns.drop('50k'))\n",
    "print(log_reg.coef_)\n",
    "log_reg.predict_proba(X)[0]\n",
    "#sigmoid(log_reg.intercept_ + np.dot(log_reg.coef_, np.transpose(x)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BkIa-Sa21qdC"
   },
   "source": [
    "## Part 3 - Analysis, Interpretation, and Questions\n",
    "\n",
    "### Based on your above model, answer the following questions\n",
    "\n",
    "1. What are 3 features positively correlated with income above 50k?\n",
    "2. What are 3 features negatively correlated with income above 50k?\n",
    "3. Overall, how well does the model explain the data and what insights do you derive from it?\n",
    "\n",
    "*These answers count* - that is, make sure to spend some time on them, connecting to your analysis above. There is no single right answer, but as long as you support your reasoning with evidence you are on the right track.\n",
    "\n",
    "Note - scikit-learn logistic regression does *not* automatically perform a hypothesis test on coefficients. That is OK - if you scale the data they are more comparable in weight.\n",
    "\n",
    "### Match the following situation descriptions with the model most appropriate to addressing them\n",
    "\n",
    "In addition to logistic regression, a number of other approaches were covered this week. Pair them with the situations they are most appropriate for, and briefly explain why.\n",
    "\n",
    "Situations:\n",
    "1. You are given data on academic performance of primary school students, and asked to fit a model to help predict \"at-risk\" students who are likely to receive the bottom tier of grades.\n",
    "2. You are studying tech companies and their patterns in releasing new products, and would like to be able to model and predict when a new product is likely to be launched.\n",
    "3. You are working on modeling expected plant size and yield with a laboratory that is able to capture fantastically detailed physical data about plants, but only of a few dozen plants at a time.\n",
    "\n",
    "Approaches:\n",
    "1. Ridge Regression\n",
    "2. Quantile Regression\n",
    "3. Survival Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yjj0sseiuHib"
   },
   "source": [
    "# Answers:\n",
    " **1. The three features that are positvely correlated to having an income above 50k are as follows:**\n",
    "     \n",
    "     Capital Gain-Intuitively a measurement of capital gain would assist a higher income. \n",
    "     \n",
    "     Education Num- I believe this is a numeric value for level of education. So the higher the education \n",
    "                         the more likely to make above 50K. \n",
    "     Capital Loss - This is less intuitive than the previous two. I believe in order to report a capital loss \n",
    "                         you must have more than 50k to invest capital and subsequently lose it. Most lower income households\n",
    "                         are less likely to invest and therefor less likely to report a capital loss. \n",
    "\n",
    " **2. The three features that are negatively correlated with income above 50 are as follows:**\n",
    "     \n",
    "     Marital Status- Married couples are more likely to have dual incomes which when combined are more likely to be\n",
    "                     above 50k, while single income households as well as no income would negatively impact income\n",
    "                     thresholds.\n",
    "                     \n",
    "     Relationship- Not quite sure how to interpret this. A child would obviously not be likely to contribute adn therefor \n",
    "                    hamper the ability to reach the threshold of 50k or above.\n",
    "     \n",
    "     Work Class - Intuitively the type of work weighs heavily on the amount of income recieved. \n",
    "\n",
    " **3. The Model overall explains the data quite well. I think many more insights could be derived from this data if the labels\n",
    "         were better explained. But some insight can be derived from the model, such as a dual income household will be more\n",
    "         likely to exceed the threshold of 50 k. As well as the class of work an individaul works in can impact income.**\n",
    "     \n",
    "     \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Situations:\n",
    "\n",
    "        1.The best approach for situation 1 would be Quantile Regression, This method best predicts when a certain range of\n",
    "            outcome becomes likely. Such as the indicators of \"at-risk' students. \n",
    "        \n",
    "        2. The best approach for situation 2 would be Survival analysis. This method best predicts the timeframe an event \n",
    "            will likely happen. \n",
    "        \n",
    "        3. The best approach for situation 3 would be ridge regression. This method is best suited for data that is well\n",
    "            featured but low in observations, such as extremely detailed information of a certain plant but very few\n",
    "            instances. \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "DS_Unit_2_Sprint_Challenge_3.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
