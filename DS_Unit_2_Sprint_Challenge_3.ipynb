{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DS_Unit_2_Sprint_Challenge_3.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/macscheffer/DS-Unit-2-Sprint-3-Advanced-Regression/blob/master/DS_Unit_2_Sprint_Challenge_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "ayDccRP01GJD"
      },
      "cell_type": "markdown",
      "source": [
        "# Data Science Unit 2 Sprint Challenge 3\n",
        "\n",
        "## Logistic Regression and Beyond\n",
        "\n",
        "In this sprint challenge you will fit a logistic regression modeling the probability of an adult having an income above 50K. The dataset is available at UCI:\n",
        "\n",
        "https://archive.ics.uci.edu/ml/datasets/adult\n",
        "\n",
        "Your goal is to:\n",
        "\n",
        "1. Load, validate, and clean/prepare the data.\n",
        "2. Fit a logistic regression model\n",
        "3. Answer questions based on the results (as well as a few extra questions about the other modules)\n",
        "\n",
        "Don't let the perfect be the enemy of the good! Manage your time, and make sure to get to all parts. If you get stuck wrestling with the data, simplify it (if necessary, drop features or rows) so you're able to move on. If you have time at the end, you can go back and try to fix/improve.\n",
        "\n",
        "### Hints\n",
        "\n",
        "It has a variety of features - some are continuous, but many are categorical. You may find [pandas.get_dummies](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html) (a method to one-hot encode) helpful!\n",
        "\n",
        "The features have dramatically different ranges. You may find [sklearn.preprocessing.minmax_scale](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.minmax_scale.html#sklearn.preprocessing.minmax_scale) helpful!"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "U22R1Ud51hxb"
      },
      "cell_type": "markdown",
      "source": [
        "## Part 1 - Load, validate, and prepare data\n",
        "\n",
        "The data is available at: https://archive.ics.uci.edu/ml/datasets/adult\n",
        "\n",
        "Load it, name the columns, and make sure that you've loaded the data successfully. Note that missing values for categorical variables can essentially be considered another category (\"unknown\"), and may not need to be dropped.\n",
        "\n",
        "You should also prepare the data for logistic regression - one-hot encode categorical features as appropriate."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "SeOByIkht-NS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "c4e6b625-35eb-43ae-89f7-73bbf9258514"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "names = ['age', 'workclass', 'fnlwgt', 'education', 'education_numeric', 'marital',\n",
        "         'occupation', 'relationship', 'race','gender', 'capital_gain','capital_loss',\n",
        "         'work_hours', 'native_country','target']\n",
        "\n",
        "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data',\n",
        "                header=None,\n",
        "                names = names)\n",
        "df.head()"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>workclass</th>\n",
              "      <th>fnlwgt</th>\n",
              "      <th>education</th>\n",
              "      <th>education_numeric</th>\n",
              "      <th>marital</th>\n",
              "      <th>occupation</th>\n",
              "      <th>relationship</th>\n",
              "      <th>race</th>\n",
              "      <th>gender</th>\n",
              "      <th>capital_gain</th>\n",
              "      <th>capital_loss</th>\n",
              "      <th>work_hours</th>\n",
              "      <th>native_country</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>39</td>\n",
              "      <td>State-gov</td>\n",
              "      <td>77516</td>\n",
              "      <td>Bachelors</td>\n",
              "      <td>13</td>\n",
              "      <td>Never-married</td>\n",
              "      <td>Adm-clerical</td>\n",
              "      <td>Not-in-family</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>2174</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>50</td>\n",
              "      <td>Self-emp-not-inc</td>\n",
              "      <td>83311</td>\n",
              "      <td>Bachelors</td>\n",
              "      <td>13</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Exec-managerial</td>\n",
              "      <td>Husband</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>13</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>38</td>\n",
              "      <td>Private</td>\n",
              "      <td>215646</td>\n",
              "      <td>HS-grad</td>\n",
              "      <td>9</td>\n",
              "      <td>Divorced</td>\n",
              "      <td>Handlers-cleaners</td>\n",
              "      <td>Not-in-family</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>53</td>\n",
              "      <td>Private</td>\n",
              "      <td>234721</td>\n",
              "      <td>11th</td>\n",
              "      <td>7</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Handlers-cleaners</td>\n",
              "      <td>Husband</td>\n",
              "      <td>Black</td>\n",
              "      <td>Male</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>28</td>\n",
              "      <td>Private</td>\n",
              "      <td>338409</td>\n",
              "      <td>Bachelors</td>\n",
              "      <td>13</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Prof-specialty</td>\n",
              "      <td>Wife</td>\n",
              "      <td>Black</td>\n",
              "      <td>Female</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>Cuba</td>\n",
              "      <td>&lt;=50K</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   age          workclass  fnlwgt   education  education_numeric  \\\n",
              "0   39          State-gov   77516   Bachelors                 13   \n",
              "1   50   Self-emp-not-inc   83311   Bachelors                 13   \n",
              "2   38            Private  215646     HS-grad                  9   \n",
              "3   53            Private  234721        11th                  7   \n",
              "4   28            Private  338409   Bachelors                 13   \n",
              "\n",
              "               marital          occupation    relationship    race   gender  \\\n",
              "0        Never-married        Adm-clerical   Not-in-family   White     Male   \n",
              "1   Married-civ-spouse     Exec-managerial         Husband   White     Male   \n",
              "2             Divorced   Handlers-cleaners   Not-in-family   White     Male   \n",
              "3   Married-civ-spouse   Handlers-cleaners         Husband   Black     Male   \n",
              "4   Married-civ-spouse      Prof-specialty            Wife   Black   Female   \n",
              "\n",
              "   capital_gain  capital_loss  work_hours  native_country  target  \n",
              "0          2174             0          40   United-States   <=50K  \n",
              "1             0             0          13   United-States   <=50K  \n",
              "2             0             0          40   United-States   <=50K  \n",
              "3             0             0          40   United-States   <=50K  \n",
              "4             0             0          40            Cuba   <=50K  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "metadata": {
        "id": "rhZgQkeGCTim",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b52f871c-e571-4f00-e0ff-3280fb7c6fc2"
      },
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(32561, 15)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "metadata": {
        "id": "_l0aXMhXD_eq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9e93ff61-10b0-41a0-ce2b-e2317ef9695c"
      },
      "cell_type": "code",
      "source": [
        "# validating we didn't lose columns when naming\n",
        "pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data').shape"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(32560, 15)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "metadata": {
        "id": "DkZwLNR4EPNN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "c4b48628-d941-4598-cd96-461ee3cbfd63"
      },
      "cell_type": "code",
      "source": [
        "df.isna().sum()"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "age                  0\n",
              "workclass            0\n",
              "fnlwgt               0\n",
              "education            0\n",
              "education_numeric    0\n",
              "marital              0\n",
              "occupation           0\n",
              "relationship         0\n",
              "race                 0\n",
              "gender               0\n",
              "capital_gain         0\n",
              "capital_loss         0\n",
              "work_hours           0\n",
              "native_country       0\n",
              "target               0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "metadata": {
        "id": "iKmauXA3HwTZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "a6588ef9-1d7c-4a79-b3bb-2b340c3f3ed9"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "df.dtypes"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "age                   int64\n",
              "workclass            object\n",
              "fnlwgt                int64\n",
              "education            object\n",
              "education_numeric     int64\n",
              "marital              object\n",
              "occupation           object\n",
              "relationship         object\n",
              "race                 object\n",
              "gender               object\n",
              "capital_gain          int64\n",
              "capital_loss          int64\n",
              "work_hours            int64\n",
              "native_country       object\n",
              "target               object\n",
              "target_boolean         bool\n",
              "dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "metadata": {
        "id": "UVkbjm5CHi_b",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df['target_boolean'] = (df.target == True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UbtRx1ADEapW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "categorical = ['workclass', 'education', 'marital', 'occupation', 'relationship', 'race', 'gender', 'native_country']\n",
        "\n",
        "df = pd.get_dummies(df, columns=categorical)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "RT1LFnFO1lo6"
      },
      "cell_type": "markdown",
      "source": [
        "## Part 2 - Fit and present a Logistic Regression\n",
        "\n",
        "Your data should now be in a state to fit a logistic regression. Use scikit-learn, define your `X` (independent variable) and `y`, and fit a model.\n",
        "\n",
        "Then, present results - display coefficients in as interpretible a way as you can (hint - scaling the numeric features will help, as it will at least make coefficients more comparable to each other). If you find it helpful for interpretation, you can also generate predictions for cases (like our 5 year old rich kid on the Titanic) or make visualizations - but the goal is your exploration to be able to answer the question, not any particular plot (i.e. don't worry about polishing it).\n",
        "\n",
        "It is *optional* to use `train_test_split` or validate your model more generally - that is not the core focus for this week. So, it is suggested you focus on fitting a model first, and if you have time at the end you can do further validation."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "s7fTRDXguD7N",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "a1c8d10c-df0e-4fcd-9bef-2102429934d1"
      },
      "cell_type": "code",
      "source": [
        "# TODO - your work!\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import scale\n",
        "\n",
        "X = df.drop(['target', 'target_boolean'], axis='columns')\n",
        "y = df['target'].values\n",
        "model = LogisticRegression()\n",
        "model.fit(X, y)"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
              "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
              "          tol=0.0001, verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "metadata": {
        "id": "_rfdFVkKKrSz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1896
        },
        "outputId": "bacaa050-2922-4aa8-9ea8-548072c4fdb0"
      },
      "cell_type": "code",
      "source": [
        "for feature,coef in zip(df.drop(['target', 'target_boolean'], axis='columns').columns, model.coef_[0]):\n",
        "  print(feature,coef)"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "age -0.007064751863442657\n",
            "fnlwgt -3.703325344603052e-06\n",
            "education_numeric -0.00175788040091257\n",
            "capital_gain 0.0003382607219922047\n",
            "capital_loss 0.0007789392740877216\n",
            "work_hours -0.008220105573905541\n",
            "workclass_ ? -0.00010364399048753715\n",
            "workclass_ Federal-gov 2.7595619741184013e-05\n",
            "workclass_ Local-gov 3.938840462128243e-06\n",
            "workclass_ Never-worked -5.092906159509893e-07\n",
            "workclass_ Private -0.0004565008454817164\n",
            "workclass_ Self-emp-inc 6.956181203121531e-05\n",
            "workclass_ Self-emp-not-inc -2.0457147036526127e-05\n",
            "workclass_ State-gov -8.83099655572965e-06\n",
            "workclass_ Without-pay -1.2920171319452327e-06\n",
            "education_ 10th -5.8574208572862546e-05\n",
            "education_ 11th -8.157177555247133e-05\n",
            "education_ 12th -2.6404898739842276e-05\n",
            "education_ 1st-4th -9.858172513423676e-06\n",
            "education_ 5th-6th -1.931960034808881e-05\n",
            "education_ 7th-8th -4.282179380599777e-05\n",
            "education_ 9th -3.257248746672989e-05\n",
            "education_ Assoc-acdm -1.1081516052918152e-05\n",
            "education_ Assoc-voc -1.4270726469762316e-05\n",
            "education_ Bachelors 0.0001795910406121413\n",
            "education_ Doctorate 4.982959343529308e-05\n",
            "education_ HS-grad -0.00038506500979555854\n",
            "education_ Masters 0.00011796595424711683\n",
            "education_ Preschool -3.863661869756734e-06\n",
            "education_ Prof-school 5.6952660241900357e-05\n",
            "education_ Some-college -0.0002090734124239193\n",
            "marital_ Divorced -0.00025172966354599887\n",
            "marital_ Married-AF-spouse 1.235072890521894e-06\n",
            "marital_ Married-civ-spouse 0.000683407867202868\n",
            "marital_ Married-spouse-absent -2.5331957896836102e-05\n",
            "marital_ Never-married -0.000769672504109171\n",
            "marital_ Separated -6.37934565598666e-05\n",
            "marital_ Widowed -6.425337305640049e-05\n",
            "occupation_ ? -0.00010415328110348813\n",
            "occupation_ Adm-clerical -0.0001670318748059435\n",
            "occupation_ Armed-Forces -3.715881911636606e-07\n",
            "occupation_ Craft-repair -6.681535658698466e-05\n",
            "occupation_ Exec-managerial 0.00020899646755011126\n",
            "occupation_ Farming-fishing -5.819110956655664e-05\n",
            "occupation_ Handlers-cleaners -8.509765065449811e-05\n",
            "occupation_ Machine-op-inspct -8.882104321816006e-05\n",
            "occupation_ Other-service -0.00023738321796836395\n",
            "occupation_ Priv-house-serv -1.176976744712105e-05\n",
            "occupation_ Prof-specialty 0.00016510409010014028\n",
            "occupation_ Protective-serv 1.2738411978521866e-05\n",
            "occupation_ Sales -2.5608895012190194e-05\n",
            "occupation_ Tech-support 6.591510562650944e-06\n",
            "occupation_ Transport-moving -3.832471071183399e-05\n",
            "relationship_ Husband 0.0006126645955966987\n",
            "relationship_ Not-in-family -0.00047596289135528545\n",
            "relationship_ Other-relative -6.866166054493965e-05\n",
            "relationship_ Own-child -0.0004114685013560551\n",
            "relationship_ Unmarried -0.00022735408577277571\n",
            "relationship_ Wife 8.064452835747322e-05\n",
            "race_ Amer-Indian-Eskimo -2.371278494799798e-05\n",
            "race_ Asian-Pac-Islander -1.6660482296625295e-05\n",
            "race_ Black -0.00012041859455508226\n",
            "race_ Other -1.4934544205301003e-05\n",
            "race_ White -0.0003144116090698656\n",
            "gender_ Female -0.0005911093464905154\n",
            "gender_ Male 0.00010097133141564165\n",
            "native_country_ ? -7.840001531886798e-06\n",
            "native_country_ Cambodia 3.4252999053920656e-07\n",
            "native_country_ Canada 6.739525236552738e-07\n",
            "native_country_ China -5.944442874801637e-07\n",
            "native_country_ Columbia -3.8128232207326747e-06\n",
            "native_country_ Cuba 1.7830026163498626e-06\n",
            "native_country_ Dominican-Republic -5.02055521817681e-06\n",
            "native_country_ Ecuador -1.1498088570683033e-06\n",
            "native_country_ El-Salvador -5.064655974731466e-06\n",
            "native_country_ England 1.0785838860814074e-06\n",
            "native_country_ France 1.4508209494451023e-06\n",
            "native_country_ Germany 1.4351902352896009e-06\n",
            "native_country_ Greece -1.1149938907396713e-06\n",
            "native_country_ Guatemala -3.3066332160224566e-06\n",
            "native_country_ Haiti -1.9412954695487824e-06\n",
            "native_country_ Holand-Netherlands -2.603387428460495e-07\n",
            "native_country_ Honduras -5.589543907033058e-07\n",
            "native_country_ Hong 5.711450299688073e-07\n",
            "native_country_ Hungary -3.12646525027712e-07\n",
            "native_country_ India 2.244224379118836e-06\n",
            "native_country_ Iran 1.1285922565955929e-06\n",
            "native_country_ Ireland -8.277247732991017e-07\n",
            "native_country_ Italy 1.6517374729053812e-06\n",
            "native_country_ Jamaica -2.931230971966777e-06\n",
            "native_country_ Japan 1.4366062880980984e-06\n",
            "native_country_ Laos -6.960203048828505e-07\n",
            "native_country_ Mexico -2.5908308671293806e-05\n",
            "native_country_ Nicaragua -1.4387025823683165e-06\n",
            "native_country_ Outlying-US(Guam-USVI-etc) -1.0139302988781158e-06\n",
            "native_country_ Peru -1.265622152151096e-06\n",
            "native_country_ Philippines 3.6129521076740563e-07\n",
            "native_country_ Poland -1.4662759596693894e-06\n",
            "native_country_ Portugal -2.2108688122999244e-06\n",
            "native_country_ Puerto-Rico -5.3736319030726025e-06\n",
            "native_country_ Scotland -1.235235569590029e-07\n",
            "native_country_ South -3.2736233569321724e-06\n",
            "native_country_ Taiwan 1.7242210746030364e-06\n",
            "native_country_ Thailand -4.0230071202629427e-07\n",
            "native_country_ Trinadad&Tobago -9.749017399518008e-07\n",
            "native_country_ United-States -0.00042258462143218104\n",
            "native_country_ Vietnam -5.3011362132005655e-06\n",
            "native_country_ Yugoslavia 7.496577777994238e-07\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Z9dAFh_vMcMh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "73f7df9c-c5fe-401f-a640-237a2af574ec"
      },
      "cell_type": "code",
      "source": [
        "X = scale(df.drop(['target', 'target_boolean'], axis='columns'))\n",
        "y = df['target'].values\n",
        "model = LogisticRegression()\n",
        "model.fit(X, y)"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DataConversionWarning: Data with input dtype uint8, int64 were all converted to float64 by the scale function.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
              "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
              "          tol=0.0001, verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 114
        }
      ]
    },
    {
      "metadata": {
        "id": "5w2N2v8iK4tS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1896
        },
        "outputId": "673efa7c-14b0-49b5-f5e8-c6dbbd3e93e4"
      },
      "cell_type": "code",
      "source": [
        "# now these are more comparable.\n",
        "for feature,coef in zip(df.drop(['target', 'target_boolean'], axis='columns').columns, model.coef_[0]):\n",
        "  print(feature,coef)"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "age 0.3476167888101695\n",
            "fnlwgt 0.07459018463715505\n",
            "education_numeric 0.36868722023046935\n",
            "capital_gain 2.3440980022746203\n",
            "capital_loss 0.2605377609492467\n",
            "work_hours 0.366349801290465\n",
            "workclass_ ? -0.06696418340816496\n",
            "workclass_ Federal-gov 0.1066841883953388\n",
            "workclass_ Local-gov -0.013127693056097766\n",
            "workclass_ Never-worked -0.06666315735143594\n",
            "workclass_ Private 0.059189513142862596\n",
            "workclass_ Self-emp-inc 0.05534722015344521\n",
            "workclass_ Self-emp-not-inc -0.0968813844703171\n",
            "workclass_ State-gov -0.03555201294669378\n",
            "workclass_ Without-pay -0.1317133628101588\n",
            "education_ 10th -0.09014664431053228\n",
            "education_ 11th -0.11145953797962313\n",
            "education_ 12th -0.03865111587685638\n",
            "education_ 1st-4th -0.03562605370480885\n",
            "education_ 5th-6th -0.03505273084354038\n",
            "education_ 7th-8th -0.10156325185415242\n",
            "education_ 9th -0.07364903456616254\n",
            "education_ Assoc-acdm -0.011717309115826278\n",
            "education_ Assoc-voc 0.018944631973980545\n",
            "education_ Bachelors 0.14491505421793222\n",
            "education_ Doctorate 0.11341895619024603\n",
            "education_ HS-grad -0.07377938716279102\n",
            "education_ Masters 0.1343950431922642\n",
            "education_ Preschool -0.46612311542377977\n",
            "education_ Prof-school 0.12663766041029437\n",
            "education_ Some-college 0.018403885859004248\n",
            "marital_ Divorced -0.2262117301239976\n",
            "marital_ Married-AF-spouse 0.05293962684132502\n",
            "marital_ Married-civ-spouse 0.7544890570916828\n",
            "marital_ Married-spouse-absent -0.07541258299996963\n",
            "marital_ Never-married -0.5355581821150861\n",
            "marital_ Separated -0.13827894739384408\n",
            "marital_ Widowed -0.09129602985551193\n",
            "occupation_ ? -0.07107389922290196\n",
            "occupation_ Adm-clerical -0.007670587382253793\n",
            "occupation_ Armed-Forces -0.019807249999666616\n",
            "occupation_ Craft-repair 0.015894498268210194\n",
            "occupation_ Exec-managerial 0.2519982234773394\n",
            "occupation_ Farming-fishing -0.17464127687846367\n",
            "occupation_ Handlers-cleaners -0.1410062798771759\n",
            "occupation_ Machine-op-inspct -0.07344878602154774\n",
            "occupation_ Other-service -0.25544326009546114\n",
            "occupation_ Priv-house-serv -0.27355248006145827\n",
            "occupation_ Prof-specialty 0.16363123381776323\n",
            "occupation_ Protective-serv 0.07716287518406162\n",
            "occupation_ Sales 0.08110500095189546\n",
            "occupation_ Tech-support 0.10504134113439982\n",
            "occupation_ Transport-moving -0.02887139469618794\n",
            "relationship_ Husband -0.06842738514306095\n",
            "relationship_ Not-in-family 0.1731190766400813\n",
            "relationship_ Other-relative -0.09010497353788413\n",
            "relationship_ Own-child -0.29732553320707267\n",
            "relationship_ Unmarried 0.08253750027249451\n",
            "relationship_ Wife 0.26123689999995886\n",
            "race_ Amer-Indian-Eskimo -0.047571724720386156\n",
            "race_ Asian-Pac-Islander 0.03060522665590477\n",
            "race_ Black -0.02839840604013771\n",
            "race_ Other -0.02875753805441767\n",
            "race_ White 0.028977345192519845\n",
            "gender_ Female -0.20216328558495347\n",
            "gender_ Male 0.20216328558496602\n",
            "native_country_ ? -0.018285529084194282\n",
            "native_country_ Cambodia 0.03242658020204205\n",
            "native_country_ Canada 0.023049039999492676\n",
            "native_country_ China -0.03088714446906659\n",
            "native_country_ Columbia -0.08769609096749444\n",
            "native_country_ Cuba 0.021313309501919887\n",
            "native_country_ Dominican-Republic -0.08215822420551258\n",
            "native_country_ Ecuador -0.0067729337664206675\n",
            "native_country_ El-Salvador -0.03180281004228105\n",
            "native_country_ England 0.018781821426114197\n",
            "native_country_ France 0.01896125625668492\n",
            "native_country_ Germany 0.03114535593635502\n",
            "native_country_ Greece -0.027838607255236403\n",
            "native_country_ Guatemala -0.008882214318540929\n",
            "native_country_ Haiti -0.00010082287839371332\n",
            "native_country_ Holand-Netherlands -0.020165934229643023\n",
            "native_country_ Honduras -0.02426378388538205\n",
            "native_country_ Hong -0.0012770650286538607\n",
            "native_country_ Hungary -0.0012850337344039846\n",
            "native_country_ India -0.018037251105471087\n",
            "native_country_ Iran 0.0034505013306806043\n",
            "native_country_ Ireland 0.015803215391734984\n",
            "native_country_ Italy 0.04045342966139419\n",
            "native_country_ Jamaica 0.004470619630424624\n",
            "native_country_ Japan 0.019272482362310137\n",
            "native_country_ Laos -0.013135770362085863\n",
            "native_country_ Mexico -0.06977335015508461\n",
            "native_country_ Nicaragua -0.02431964871892492\n",
            "native_country_ Outlying-US(Guam-USVI-etc) -0.12124757770411058\n",
            "native_country_ Peru -0.024277036257307132\n",
            "native_country_ Philippines 0.0367390064429724\n",
            "native_country_ Poland 0.0018934708156091021\n",
            "native_country_ Portugal 0.0005242926528584537\n",
            "native_country_ Puerto-Rico -0.01688102937400517\n",
            "native_country_ Scotland 0.0010170630597388622\n",
            "native_country_ South -0.050436287856860045\n",
            "native_country_ Taiwan 0.0034086808814331733\n",
            "native_country_ Thailand -0.012141586943897628\n",
            "native_country_ Trinadad&Tobago -0.008117674140086196\n",
            "native_country_ United-States 0.07436626016403833\n",
            "native_country_ Vietnam -0.04960533138580268\n",
            "native_country_ Yugoslavia 0.016242134256307827\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DUzOT3SwNNy1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# instead of continuing the analysis here i will show analysis as i answer questions below and give more insights from there.\n",
        "# this way i dont over analyze before answering the questions."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "BkIa-Sa21qdC"
      },
      "cell_type": "markdown",
      "source": [
        "## Part 3 - Analysis, Interpretation, and Questions\n",
        "\n",
        "### Based on your above model, answer the following questions\n",
        "\n",
        "1. What are 3 features positively correlated with income above 50k?\n",
        "2. What are 3 features negatively correlated with income above 50k?\n",
        "3. Overall, how well does the model explain the data and what insights do you derive from it?\n",
        "\n",
        "*These answers count* - that is, make sure to spend some time on them, connecting to your analysis above. There is no single right answer, but as long as you support your reasoning with evidence you are on the right track.\n",
        "\n",
        "Note - scikit-learn logistic regression does *not* automatically perform a hypothesis test on coefficients. That is OK - if you scale the data they are more comparable in weight.\n",
        "\n",
        "### Match the following situation descriptions with the model most appropriate to addressing them\n",
        "\n",
        "In addition to logistic regression, a number of other approaches were covered this week. Pair them with the situations they are most appropriate for, and briefly explain why.\n",
        "\n",
        "Situations:\n",
        "1. You are given data on academic performance of primary school students, and asked to fit a model to help predict \"at-risk\" students who are likely to receive the bottom tier of grades.\n",
        "2. You are studying tech companies and their patterns in releasing new products, and would like to be able to model and predict when a new product is likely to be launched.\n",
        "3. You are working on modeling expected plant size and yield with a laboratory that is able to capture fantastically detailed physical data about plants, but only of a few dozen plants at a time.\n",
        "\n",
        "Approaches:\n",
        "1. Ridge Regression\n",
        "2. Quantile Regression\n",
        "3. Survival Analysis"
      ]
    },
    {
      "metadata": {
        "id": "b1t049ZcR14Y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Matching answers:\n",
        "\n",
        "1. Quantile Regression: this lets us look into certain quantiles of a distribution. In this case we'd want to look into what causes students to be in the lower quantile. It is robust to outliers and we can see how the coefficients change when focusing on a certain quantile. for example divorced parents may not be important for predicting whether someone is in the 50th or 60th percentile but could be important for predicting whether someone is in the 10th or 90th quantile\n",
        "\n",
        "2. Survival Analysis: it has a start and end date. for example if XYZ company released their 7th product last week and you want to get on idea on when the 8th product will be launched. You could look at the intervals between product launches of their first 7 products.\n",
        "\n",
        "3. Ridge Regression: Ridge is able to generalize data well by not only minimizing squared errors but by pentalizing the slope of coefficients. With small amounts of data relative to the amount of features - LinearRegression would overfit these features causing steep coefficients. Ridge Regression would counteract the reduction of squared errors for using these steep coefficients with the penalty for steep coefficients. Ridge also does well with collinearity for this reason and fantastically detailed plant data gives me the impression that there are a lot of features relative to the only few dozen plants at a time."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Yjj0sseiuHib"
      },
      "cell_type": "markdown",
      "source": [
        "**TODO - your answers!**"
      ]
    },
    {
      "metadata": {
        "id": "xFzHJwRKNdsj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Answers\n",
        "## 1 & 2: Correlations to income being above and below 50k."
      ]
    },
    {
      "metadata": {
        "id": "ZEZZ_GL4NcvW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 834
        },
        "outputId": "cbb2d184-a303-479a-ddfa-e065f40847a5"
      },
      "cell_type": "code",
      "source": [
        "# positive correlations\n",
        "\n",
        "for feature,coef in zip(df.drop(['target', 'target_boolean'], axis='columns').columns, model.coef_[0]):\n",
        "  if coef > 0:\n",
        "    print(feature,coef)"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "age 0.3476167888101695\n",
            "fnlwgt 0.07459018463715505\n",
            "education_numeric 0.36868722023046935\n",
            "capital_gain 2.3440980022746203\n",
            "capital_loss 0.2605377609492467\n",
            "work_hours 0.366349801290465\n",
            "workclass_ Federal-gov 0.1066841883953388\n",
            "workclass_ Private 0.059189513142862596\n",
            "workclass_ Self-emp-inc 0.05534722015344521\n",
            "education_ Assoc-voc 0.018944631973980545\n",
            "education_ Bachelors 0.14491505421793222\n",
            "education_ Doctorate 0.11341895619024603\n",
            "education_ Masters 0.1343950431922642\n",
            "education_ Prof-school 0.12663766041029437\n",
            "education_ Some-college 0.018403885859004248\n",
            "marital_ Married-AF-spouse 0.05293962684132502\n",
            "marital_ Married-civ-spouse 0.7544890570916828\n",
            "occupation_ Craft-repair 0.015894498268210194\n",
            "occupation_ Exec-managerial 0.2519982234773394\n",
            "occupation_ Prof-specialty 0.16363123381776323\n",
            "occupation_ Protective-serv 0.07716287518406162\n",
            "occupation_ Sales 0.08110500095189546\n",
            "occupation_ Tech-support 0.10504134113439982\n",
            "relationship_ Not-in-family 0.1731190766400813\n",
            "relationship_ Unmarried 0.08253750027249451\n",
            "relationship_ Wife 0.26123689999995886\n",
            "race_ Asian-Pac-Islander 0.03060522665590477\n",
            "race_ White 0.028977345192519845\n",
            "gender_ Male 0.20216328558496602\n",
            "native_country_ Cambodia 0.03242658020204205\n",
            "native_country_ Canada 0.023049039999492676\n",
            "native_country_ Cuba 0.021313309501919887\n",
            "native_country_ England 0.018781821426114197\n",
            "native_country_ France 0.01896125625668492\n",
            "native_country_ Germany 0.03114535593635502\n",
            "native_country_ Iran 0.0034505013306806043\n",
            "native_country_ Ireland 0.015803215391734984\n",
            "native_country_ Italy 0.04045342966139419\n",
            "native_country_ Jamaica 0.004470619630424624\n",
            "native_country_ Japan 0.019272482362310137\n",
            "native_country_ Philippines 0.0367390064429724\n",
            "native_country_ Poland 0.0018934708156091021\n",
            "native_country_ Portugal 0.0005242926528584537\n",
            "native_country_ Scotland 0.0010170630597388622\n",
            "native_country_ Taiwan 0.0034086808814331733\n",
            "native_country_ United-States 0.07436626016403833\n",
            "native_country_ Yugoslavia 0.016242134256307827\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OcGDKmasO-x8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**for 3 specific positively correlated features: age, education_numeric and gender male**\n",
        "\n",
        "all three of these are very intuitive (although gender being male should be).\n",
        "-> As people get older they make more money.\n",
        "-> As people become more educated (years of education for example highschool education => value of 12) \n",
        "-> Society has let it become the norm for males to make more money than females.\n",
        "\n",
        "-> we can connect the education intuition to our category labels. someone who gets a masters (.134 coefficient from normalized model)is more likely to make over 50k than someone with \"Some-College\"(.018 coefficient model)"
      ]
    },
    {
      "metadata": {
        "id": "vVlMyT0qOySc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1078
        },
        "outputId": "e3eff6fd-0fc2-4ae5-8908-d8a1875996ea"
      },
      "cell_type": "code",
      "source": [
        "for feature,coef in zip(df.drop(['target', 'target_boolean'], axis='columns').columns, model.coef_[0]):\n",
        "  if coef < 0:\n",
        "    print(feature,coef)"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "workclass_ ? -0.06696418340816496\n",
            "workclass_ Local-gov -0.013127693056097766\n",
            "workclass_ Never-worked -0.06666315735143594\n",
            "workclass_ Self-emp-not-inc -0.0968813844703171\n",
            "workclass_ State-gov -0.03555201294669378\n",
            "workclass_ Without-pay -0.1317133628101588\n",
            "education_ 10th -0.09014664431053228\n",
            "education_ 11th -0.11145953797962313\n",
            "education_ 12th -0.03865111587685638\n",
            "education_ 1st-4th -0.03562605370480885\n",
            "education_ 5th-6th -0.03505273084354038\n",
            "education_ 7th-8th -0.10156325185415242\n",
            "education_ 9th -0.07364903456616254\n",
            "education_ Assoc-acdm -0.011717309115826278\n",
            "education_ HS-grad -0.07377938716279102\n",
            "education_ Preschool -0.46612311542377977\n",
            "marital_ Divorced -0.2262117301239976\n",
            "marital_ Married-spouse-absent -0.07541258299996963\n",
            "marital_ Never-married -0.5355581821150861\n",
            "marital_ Separated -0.13827894739384408\n",
            "marital_ Widowed -0.09129602985551193\n",
            "occupation_ ? -0.07107389922290196\n",
            "occupation_ Adm-clerical -0.007670587382253793\n",
            "occupation_ Armed-Forces -0.019807249999666616\n",
            "occupation_ Farming-fishing -0.17464127687846367\n",
            "occupation_ Handlers-cleaners -0.1410062798771759\n",
            "occupation_ Machine-op-inspct -0.07344878602154774\n",
            "occupation_ Other-service -0.25544326009546114\n",
            "occupation_ Priv-house-serv -0.27355248006145827\n",
            "occupation_ Transport-moving -0.02887139469618794\n",
            "relationship_ Husband -0.06842738514306095\n",
            "relationship_ Other-relative -0.09010497353788413\n",
            "relationship_ Own-child -0.29732553320707267\n",
            "race_ Amer-Indian-Eskimo -0.047571724720386156\n",
            "race_ Black -0.02839840604013771\n",
            "race_ Other -0.02875753805441767\n",
            "gender_ Female -0.20216328558495347\n",
            "native_country_ ? -0.018285529084194282\n",
            "native_country_ China -0.03088714446906659\n",
            "native_country_ Columbia -0.08769609096749444\n",
            "native_country_ Dominican-Republic -0.08215822420551258\n",
            "native_country_ Ecuador -0.0067729337664206675\n",
            "native_country_ El-Salvador -0.03180281004228105\n",
            "native_country_ Greece -0.027838607255236403\n",
            "native_country_ Guatemala -0.008882214318540929\n",
            "native_country_ Haiti -0.00010082287839371332\n",
            "native_country_ Holand-Netherlands -0.020165934229643023\n",
            "native_country_ Honduras -0.02426378388538205\n",
            "native_country_ Hong -0.0012770650286538607\n",
            "native_country_ Hungary -0.0012850337344039846\n",
            "native_country_ India -0.018037251105471087\n",
            "native_country_ Laos -0.013135770362085863\n",
            "native_country_ Mexico -0.06977335015508461\n",
            "native_country_ Nicaragua -0.02431964871892492\n",
            "native_country_ Outlying-US(Guam-USVI-etc) -0.12124757770411058\n",
            "native_country_ Peru -0.024277036257307132\n",
            "native_country_ Puerto-Rico -0.01688102937400517\n",
            "native_country_ South -0.050436287856860045\n",
            "native_country_ Thailand -0.012141586943897628\n",
            "native_country_ Trinadad&Tobago -0.008117674140086196\n",
            "native_country_ Vietnam -0.04960533138580268\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BkV60XiDQtGw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**for 3 specific negativley correlations we see: gender female, relationship Husband, education HS\n",
        "\n",
        "- gender for same reasons as stated above\n",
        "- relationship husband because of the norm for women to be more focused on raising childeren while the husband works\n",
        "- education HS because they didn't get a college education.\n"
      ]
    },
    {
      "metadata": {
        "id": "Dk1mwytJU0GA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Answer 3: In depth analysis"
      ]
    },
    {
      "metadata": {
        "id": "R55KJ1GwVLAd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f84e57be-3e5c-42d6-d62e-7b8281a2047e"
      },
      "cell_type": "code",
      "source": [
        "# unscaled results.\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "X = df.drop(['target', 'target_boolean'], axis='columns')\n",
        "y = df['target'].values\n",
        "model = LogisticRegression()\n",
        "model.fit(X, y)\n",
        "model.score(X,y)"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7977949080187955"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 131
        }
      ]
    },
    {
      "metadata": {
        "id": "GQK8tLUKP761",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1e56c907-5455-403e-f32b-88b0b793ba43"
      },
      "cell_type": "code",
      "source": [
        "# scaled results.\n",
        "X = scale(df.drop(['target', 'target_boolean'], axis='columns'))\n",
        "y = df['target'].values\n",
        "model = LogisticRegression()\n",
        "model.fit(X, y)\n",
        "model.score(X,y)"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8533521697736556"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 132
        }
      ]
    },
    {
      "metadata": {
        "id": "IaVCOLp2VE76",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# by scaling the results we are able to get an accuracy of about 85%.\n",
        "\n",
        "# however this is using the data we trained the model with - for a better explanation lets use test/train split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eLM3DhI3WRXC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dbeef41d-4bb5-4daf-b792-e45904b4605d"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df.drop(['target', 'target_boolean'], axis='columns')\n",
        "y = df['target'].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.1, random_state=43)\n",
        "\n",
        "\n",
        "model = LogisticRegression(solver='lbfgs')\n",
        "model.fit(X_train, y_train)\n",
        "model.score(X_test,y_test)"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7933681301811483"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 139
        }
      ]
    },
    {
      "metadata": {
        "id": "W2r3C4BOWyMI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8796dcb0-2a2b-4b69-9a6d-f11b0f064a5c"
      },
      "cell_type": "code",
      "source": [
        "X = scale(df.drop(['target', 'target_boolean'], axis='columns'))\n",
        "y = df['target'].values\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.1, random_state=43)\n",
        "\n",
        "\n",
        "\n",
        "model = LogisticRegression(solver='lbfgs')\n",
        "model.fit(X_train, y_train)\n",
        "model.score(X_test, y_test)"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8431071538225361"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 144
        }
      ]
    },
    {
      "metadata": {
        "id": "jSZNXsqkXGnW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1896
        },
        "outputId": "cd02ccad-02fc-48b5-9216-bd6a9aa6772e"
      },
      "cell_type": "code",
      "source": [
        "# notice that both scores decreased a little as we predicted out of sample but the regularized model still does much better.\n",
        "\n",
        "for feature,coef in zip(df.drop(['target', 'target_boolean'], axis='columns').columns, model.coef_[0]):\n",
        "  print(feature,coef)"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "age 0.34676401387210787\n",
            "fnlwgt 0.0750993179403168\n",
            "education_numeric 0.3724539821293186\n",
            "capital_gain 2.3525952138837507\n",
            "capital_loss 0.2706126032333683\n",
            "work_hours 0.367591989724454\n",
            "workclass_ ? -0.06625367199928169\n",
            "workclass_ Federal-gov 0.10544891569102635\n",
            "workclass_ Local-gov -0.02066598694087707\n",
            "workclass_ Never-worked -0.0797190353920086\n",
            "workclass_ Private 0.06411292365532252\n",
            "workclass_ Self-emp-inc 0.05344604862553791\n",
            "workclass_ Self-emp-not-inc -0.09428994270869132\n",
            "workclass_ State-gov -0.03665897804990826\n",
            "workclass_ Without-pay -0.14666363094855234\n",
            "education_ 10th -0.10538941483032432\n",
            "education_ 11th -0.11972398088096517\n",
            "education_ 12th -0.04284128842240964\n",
            "education_ 1st-4th -0.030463314426949297\n",
            "education_ 5th-6th -0.028499022561068028\n",
            "education_ 7th-8th -0.10109391257530229\n",
            "education_ 9th -0.06963496358261548\n",
            "education_ Assoc-acdm -0.013670977742981831\n",
            "education_ Assoc-voc 0.023978026605657586\n",
            "education_ Bachelors 0.13690863046001273\n",
            "education_ Doctorate 0.1155676256545244\n",
            "education_ HS-grad -0.06838310270451878\n",
            "education_ Masters 0.13971763081490787\n",
            "education_ Preschool -0.4793718196242961\n",
            "education_ Prof-school 0.13359650384414012\n",
            "education_ Some-college 0.020616787434455892\n",
            "marital_ Divorced -0.24994768351942878\n",
            "marital_ Married-AF-spouse 0.05339394362815565\n",
            "marital_ Married-civ-spouse 0.7995468382156153\n",
            "marital_ Married-spouse-absent -0.07522566995079195\n",
            "marital_ Never-married -0.5524278959267863\n",
            "marital_ Separated -0.15900605274152255\n",
            "marital_ Widowed -0.10759150667050832\n",
            "occupation_ ? -0.0711929776189074\n",
            "occupation_ Adm-clerical -0.007545746392879915\n",
            "occupation_ Armed-Forces -0.10662638380715313\n",
            "occupation_ Craft-repair 0.009360977498164539\n",
            "occupation_ Exec-managerial 0.2521678135923197\n",
            "occupation_ Farming-fishing -0.18002365128516953\n",
            "occupation_ Handlers-cleaners -0.15033559631293078\n",
            "occupation_ Machine-op-inspct -0.06354978127279898\n",
            "occupation_ Other-service -0.2511635692066679\n",
            "occupation_ Priv-house-serv -0.2731422291718563\n",
            "occupation_ Prof-specialty 0.1706755253983783\n",
            "occupation_ Protective-serv 0.07777896942685807\n",
            "occupation_ Sales 0.08054656726591224\n",
            "occupation_ Tech-support 0.10337281382540041\n",
            "occupation_ Transport-moving -0.025789283156154252\n",
            "relationship_ Husband -0.10870436099519508\n",
            "relationship_ Not-in-family 0.2055295889737377\n",
            "relationship_ Other-relative -0.08588652069560282\n",
            "relationship_ Own-child -0.3008968057791886\n",
            "relationship_ Unmarried 0.11276619533973897\n",
            "relationship_ Wife 0.24685070882845098\n",
            "race_ Amer-Indian-Eskimo -0.039023722083480976\n",
            "race_ Asian-Pac-Islander 0.017711741234864038\n",
            "race_ Black -0.02140242207823213\n",
            "race_ Other -0.03411301442054865\n",
            "race_ White 0.028583023901621263\n",
            "gender_ Female -0.19985399268229614\n",
            "gender_ Male 0.19985399268229784\n",
            "native_country_ ? -0.014635999710895723\n",
            "native_country_ Cambodia 0.026197061949629133\n",
            "native_country_ Canada 0.019111240694126085\n",
            "native_country_ China -0.03368666014386259\n",
            "native_country_ Columbia -0.08744833257973628\n",
            "native_country_ Cuba 0.010562608960898735\n",
            "native_country_ Dominican-Republic -0.08181196397994464\n",
            "native_country_ Ecuador -0.0226477681194924\n",
            "native_country_ El-Salvador -0.040174273391273996\n",
            "native_country_ England 0.021552088357567372\n",
            "native_country_ France 0.018449748814824245\n",
            "native_country_ Germany 0.033274574143887965\n",
            "native_country_ Greece -0.023356245176233997\n",
            "native_country_ Guatemala -0.00601955332828523\n",
            "native_country_ Haiti -0.014642485312004247\n",
            "native_country_ Holand-Netherlands -0.026135737248142516\n",
            "native_country_ Honduras -0.022823673960399637\n",
            "native_country_ Hong -0.0008489115117479871\n",
            "native_country_ Hungary 0.0026243336118229343\n",
            "native_country_ India -0.018015329525614535\n",
            "native_country_ Iran -0.00039135838137891463\n",
            "native_country_ Ireland 0.008822321600218185\n",
            "native_country_ Italy 0.03918574958129264\n",
            "native_country_ Jamaica 0.009854321208773772\n",
            "native_country_ Japan 0.021831708747876207\n",
            "native_country_ Laos -0.012002767047646162\n",
            "native_country_ Mexico -0.06652840583063477\n",
            "native_country_ Nicaragua -0.011040102639981415\n",
            "native_country_ Outlying-US(Guam-USVI-etc) -0.13637978423920222\n",
            "native_country_ Peru -0.016483313854824308\n",
            "native_country_ Philippines 0.045979547112239644\n",
            "native_country_ Poland -0.008335689855084547\n",
            "native_country_ Portugal 0.0004999785606427355\n",
            "native_country_ Puerto-Rico -0.013574217933963208\n",
            "native_country_ Scotland 0.002797183999371889\n",
            "native_country_ South -0.037265039370672125\n",
            "native_country_ Taiwan 0.011623883099272182\n",
            "native_country_ Thailand -0.0009489315097986593\n",
            "native_country_ Trinadad&Tobago -0.028815738245399607\n",
            "native_country_ United-States 0.07201838716733021\n",
            "native_country_ Vietnam -0.05158192618054502\n",
            "native_country_ Yugoslavia 0.024794814699366927\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7MGeyMC8YZTS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5b8c2061-4dcf-4248-a0e0-8743c9bc1193"
      },
      "cell_type": "code",
      "source": [
        "# lets see if we drop fnlwgt. as this is a prediction by the census on how many people the entry represents and shouldn't be predictive in either direction\n",
        "# we were 84.3107 accurate before dropping fnlwgt\n",
        "X = scale(df.drop(['target', 'target_boolean', 'fnlwgt'], axis='columns'))\n",
        "y = df['target'].values\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.1, random_state=43)\n",
        "\n",
        "\n",
        "\n",
        "model = LogisticRegression(solver='lbfgs')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "model.score(X_test, y_test)"
      ],
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8458704329137243"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 161
        }
      ]
    },
    {
      "metadata": {
        "id": "PX_onBRkYz6-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# we increased our accuracy \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fNbo4zDqZ3DL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}