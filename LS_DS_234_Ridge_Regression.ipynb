{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LS_DS_234_Ridge_Regression.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "-eFju4_DDKeX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Lambda School Data Science - Ridge Regression\n",
        "\n",
        "Regularize your way to a better tomorrow."
      ]
    },
    {
      "metadata": {
        "id": "5v5cBm19JxOj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Lecture\n",
        "\n",
        "Data science depends on math, and math is generally focused on situations where:\n",
        "\n",
        "1. a solution exists,\n",
        "2. the solution is unique,\n",
        "3. the solution's behavior changes continuously with the initial conditions.\n",
        "\n",
        "These are known as [well-posed problems](https://en.wikipedia.org/wiki/Well-posed_problem), and are the sorts of assumptions so core in traditional techniques that it is easy to forget about them. But they do matter, as there can be exceptions:\n",
        "\n",
        "1. no solution - e.g. no $x$ such that $Ax = b$\n",
        "2. multiple solutions - e.g. several $x_1, x_2, ...$ such that $Ax = b$\n",
        "3. \"chaotic\" systems - situations where small changes in initial conditions interact and reverberate in essentially unpredictable ways - for instance, the difficulty in longterm predictions of weather (N.B. not the same thing as longterm predictions of *climate*) - you can think of this as models that fail to generalize well, because they overfit on the training data (the initial conditions)\n",
        "\n",
        "Problems suffering from the above are called ill-posed problems. Relating to linear algebra and systems of equations, the only truly well-posed problems are those with a single unique solution.\n",
        "\n",
        "![Intersecting lines](https://upload.wikimedia.org/wikipedia/commons/c/c0/Intersecting_Lines.svg)\n",
        "\n",
        "Think for a moment - what would the above plot look like if there was no solution? If there were multiple solutions? And how would that generalize to higher dimensions?\n",
        "\n",
        "A lot of what you covered with linear regression was about getting matrices into the right shape for them to be solvable in this sense. But some matrices just won't submit to this, and other problems may technically \"fit\" linear regression but still be violating the above assumptions in subtle ways.\n",
        "\n",
        "[Overfitting](https://en.wikipedia.org/wiki/Overfitting) is in some ways a special case of this - an overfit model uses more features/parameters than is \"justified\" by the data (essentially by the *dimensionality* of the data, as measured by $n$ the number of observations). As the number of features approaches the number of observations, linear regression still \"works\", but it starts giving fairly perverse results. In particular, it results in a model that fails to *generalize* - and so the core goal of prediction and explanatory power is undermined.\n",
        "\n",
        "How is this related to well and ill-posed problems? It's not clearly a no solution or multiple solution case, but it does fall in the third category - overfitting results in fitting to the \"noise\" in the data, which means the particulars of one random sample or another (different initial conditions )will result in dramatically different models.\n",
        "\n",
        "## Stop and think - what are ways to address these issues?\n",
        "\n",
        "Let's examine in the context of housing data."
      ]
    },
    {
      "metadata": {
        "id": "TDh_Oz9HDHeR",
        "colab_type": "code",
        "outputId": "f3e4d42e-57c0-432b-c369-95522bc37dd3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_boston\n",
        "from sklearn.preprocessing import scale\n",
        "\n",
        "boston = load_boston()\n",
        "boston.data = scale(boston.data)  # Very helpful for regularization!\n",
        "df = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
        "df['Price'] = boston.target\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CRIM</th>\n",
              "      <th>ZN</th>\n",
              "      <th>INDUS</th>\n",
              "      <th>CHAS</th>\n",
              "      <th>NOX</th>\n",
              "      <th>RM</th>\n",
              "      <th>AGE</th>\n",
              "      <th>DIS</th>\n",
              "      <th>RAD</th>\n",
              "      <th>TAX</th>\n",
              "      <th>PTRATIO</th>\n",
              "      <th>B</th>\n",
              "      <th>LSTAT</th>\n",
              "      <th>Price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.419782</td>\n",
              "      <td>0.284830</td>\n",
              "      <td>-1.287909</td>\n",
              "      <td>-0.272599</td>\n",
              "      <td>-0.144217</td>\n",
              "      <td>0.413672</td>\n",
              "      <td>-0.120013</td>\n",
              "      <td>0.140214</td>\n",
              "      <td>-0.982843</td>\n",
              "      <td>-0.666608</td>\n",
              "      <td>-1.459000</td>\n",
              "      <td>0.441052</td>\n",
              "      <td>-1.075562</td>\n",
              "      <td>24.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.417339</td>\n",
              "      <td>-0.487722</td>\n",
              "      <td>-0.593381</td>\n",
              "      <td>-0.272599</td>\n",
              "      <td>-0.740262</td>\n",
              "      <td>0.194274</td>\n",
              "      <td>0.367166</td>\n",
              "      <td>0.557160</td>\n",
              "      <td>-0.867883</td>\n",
              "      <td>-0.987329</td>\n",
              "      <td>-0.303094</td>\n",
              "      <td>0.441052</td>\n",
              "      <td>-0.492439</td>\n",
              "      <td>21.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.417342</td>\n",
              "      <td>-0.487722</td>\n",
              "      <td>-0.593381</td>\n",
              "      <td>-0.272599</td>\n",
              "      <td>-0.740262</td>\n",
              "      <td>1.282714</td>\n",
              "      <td>-0.265812</td>\n",
              "      <td>0.557160</td>\n",
              "      <td>-0.867883</td>\n",
              "      <td>-0.987329</td>\n",
              "      <td>-0.303094</td>\n",
              "      <td>0.396427</td>\n",
              "      <td>-1.208727</td>\n",
              "      <td>34.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.416750</td>\n",
              "      <td>-0.487722</td>\n",
              "      <td>-1.306878</td>\n",
              "      <td>-0.272599</td>\n",
              "      <td>-0.835284</td>\n",
              "      <td>1.016303</td>\n",
              "      <td>-0.809889</td>\n",
              "      <td>1.077737</td>\n",
              "      <td>-0.752922</td>\n",
              "      <td>-1.106115</td>\n",
              "      <td>0.113032</td>\n",
              "      <td>0.416163</td>\n",
              "      <td>-1.361517</td>\n",
              "      <td>33.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.412482</td>\n",
              "      <td>-0.487722</td>\n",
              "      <td>-1.306878</td>\n",
              "      <td>-0.272599</td>\n",
              "      <td>-0.835284</td>\n",
              "      <td>1.228577</td>\n",
              "      <td>-0.511180</td>\n",
              "      <td>1.077737</td>\n",
              "      <td>-0.752922</td>\n",
              "      <td>-1.106115</td>\n",
              "      <td>0.113032</td>\n",
              "      <td>0.441052</td>\n",
              "      <td>-1.026501</td>\n",
              "      <td>36.2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       CRIM        ZN     INDUS      CHAS       NOX        RM       AGE  \\\n",
              "0 -0.419782  0.284830 -1.287909 -0.272599 -0.144217  0.413672 -0.120013   \n",
              "1 -0.417339 -0.487722 -0.593381 -0.272599 -0.740262  0.194274  0.367166   \n",
              "2 -0.417342 -0.487722 -0.593381 -0.272599 -0.740262  1.282714 -0.265812   \n",
              "3 -0.416750 -0.487722 -1.306878 -0.272599 -0.835284  1.016303 -0.809889   \n",
              "4 -0.412482 -0.487722 -1.306878 -0.272599 -0.835284  1.228577 -0.511180   \n",
              "\n",
              "        DIS       RAD       TAX   PTRATIO         B     LSTAT  Price  \n",
              "0  0.140214 -0.982843 -0.666608 -1.459000  0.441052 -1.075562   24.0  \n",
              "1  0.557160 -0.867883 -0.987329 -0.303094  0.441052 -0.492439   21.6  \n",
              "2  0.557160 -0.867883 -0.987329 -0.303094  0.396427 -1.208727   34.7  \n",
              "3  1.077737 -0.752922 -1.106115  0.113032  0.416163 -1.361517   33.4  \n",
              "4  1.077737 -0.752922 -1.106115  0.113032  0.441052 -1.026501   36.2  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "metadata": {
        "id": "3u24Yr-SkIhb",
        "colab_type": "code",
        "outputId": "3cc8f97f-96d0-4b08-ced9-e29d1c740a22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(506, 14)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "0vlZShpFkll2",
        "colab_type": "code",
        "outputId": "aeeeee4c-8dfc-4b63-e73a-98863358bdbb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Let's try good old least squares!\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "X = df.drop('Price', axis='columns')\n",
        "y = df.Price\n",
        "\n",
        "lin_reg = LinearRegression().fit(X, y)\n",
        "mean_squared_error(y, lin_reg.predict(X))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21.894831181729206"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "erOFuJKWlTad",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "That seems like a pretty good score, but...\n",
        "\n",
        "![Kitchen Sink](https://i.imgur.com/ZZxqhT1.jpg)\n",
        "\n",
        "Chances are this doesn't generalize very well. You can verify this by splitting the data to properly test model validity."
      ]
    },
    {
      "metadata": {
        "id": "CG6DZ1UcqbEx",
        "colab_type": "code",
        "outputId": "04af7cd1-5847-4531-b105-32a0cf449dd7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=43)\n",
        "lin_reg_split = LinearRegression().fit(X_train, y_train)\n",
        "print(mean_squared_error(y, lin_reg_split.predict(X)))\n",
        "print(mean_squared_error(y_test, lin_reg_split.predict(X_test)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "22.347018673376052\n",
            "26.273991426429014\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ILHGe53Iqehg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Oops! 💥\n",
        "\n",
        "### What can we do?\n",
        "\n",
        "- Use fewer features - sure, but it can be a lot of work to figure out *which* features, and (in cases like this) there may not be any good reason to really favor some features over another.\n",
        "- Get more data! This is actually a pretty good approach in tech, since apps generate lots of data all the time (and we made this situation by artificially constraining our data). But for case studies, existing data, etc. it won't work.\n",
        "- **Regularize!**\n",
        "\n",
        "## Regularization just means \"add bias\"\n",
        "\n",
        "OK, there's a bit more to it than that. But that's the core intuition - the problem is the model working \"too well\", so fix it by making it harder for the model!\n",
        "\n",
        "It may sound strange - a technique that is purposefully \"worse\" - but in certain situations, it can really get results.\n",
        "\n",
        "What's bias? In the context of statistics and machine learning, bias is when a predictive model fails to identify relationships between features and the output. In a word, bias is *underfitting*.\n",
        "\n",
        "We want to add bias to the model because of the [bias-variance tradeoff](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff) - variance is the sensitivity of a model to the random noise in its training data (i.e. *overfitting*), and bias and variance are naturally (inversely) related. Increasing one will always decrease the other, with regards to the overall generalization error (predictive accuracy on unseen data).\n",
        "\n",
        "Visually, the result looks like this:\n",
        "\n",
        "![Regularization example plot](https://upload.wikimedia.org/wikipedia/commons/0/02/Regularization.svg)\n",
        "\n",
        "The blue line is overfit, using more dimensions than are needed to explain the data and so much of the movement is based on noise and won't generalize well. The green line still fits the data, but is less susceptible to the noise - depending on how exactly we parameterize \"noise\" we may throw out actual correlation, but if we balance it right we keep that signal and greatly improve generalizability.\n",
        "\n",
        "### Look carefully at the above plot and think of ways you can quantify the difference between the blue and green lines...\n"
      ]
    },
    {
      "metadata": {
        "id": "7aQlX9e9lQLr",
        "colab_type": "code",
        "outputId": "d5cef801-efec-4c36-fe27-c4b3f02a6750",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Now with regularization via ridge regression\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "ridge_reg = Ridge().fit(X, y)\n",
        "mean_squared_error(y, ridge_reg.predict(X))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21.895862166800143"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "qiMXYAWGomcB",
        "colab_type": "code",
        "outputId": "5a583ecf-c93f-40f2-8502-41d9e561ba32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# The score is a bit worse than OLS - but that's expected (we're adding bias)\n",
        "# Let's try split\n",
        "\n",
        "ridge_reg_split = Ridge().fit(X_train, y_train)\n",
        "mean_squared_error(y_test, ridge_reg_split.predict(X_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "26.192201358877668"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "PJhjFFeF2uoA",
        "colab_type": "code",
        "outputId": "6289580e-aed7-4839-c3e6-2c643574e2ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4674
        }
      },
      "cell_type": "code",
      "source": [
        "# A little better (to same test split w/OLS) - can we improve it further?\n",
        "# We just went with defaults, but as always there's plenty of parameters\n",
        "help(Ridge)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Help on class Ridge in module sklearn.linear_model.ridge:\n",
            "\n",
            "class Ridge(_BaseRidge, sklearn.base.RegressorMixin)\n",
            " |  Linear least squares with l2 regularization.\n",
            " |  \n",
            " |  Minimizes the objective function::\n",
            " |  \n",
            " |  ||y - Xw||^2_2 + alpha * ||w||^2_2\n",
            " |  \n",
            " |  This model solves a regression model where the loss function is\n",
            " |  the linear least squares function and regularization is given by\n",
            " |  the l2-norm. Also known as Ridge Regression or Tikhonov regularization.\n",
            " |  This estimator has built-in support for multi-variate regression\n",
            " |  (i.e., when y is a 2d-array of shape [n_samples, n_targets]).\n",
            " |  \n",
            " |  Read more in the :ref:`User Guide <ridge_regression>`.\n",
            " |  \n",
            " |  Parameters\n",
            " |  ----------\n",
            " |  alpha : {float, array-like}, shape (n_targets)\n",
            " |      Regularization strength; must be a positive float. Regularization\n",
            " |      improves the conditioning of the problem and reduces the variance of\n",
            " |      the estimates. Larger values specify stronger regularization.\n",
            " |      Alpha corresponds to ``C^-1`` in other linear models such as\n",
            " |      LogisticRegression or LinearSVC. If an array is passed, penalties are\n",
            " |      assumed to be specific to the targets. Hence they must correspond in\n",
            " |      number.\n",
            " |  \n",
            " |  fit_intercept : boolean\n",
            " |      Whether to calculate the intercept for this model. If set\n",
            " |      to false, no intercept will be used in calculations\n",
            " |      (e.g. data is expected to be already centered).\n",
            " |  \n",
            " |  normalize : boolean, optional, default False\n",
            " |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
            " |      If True, the regressors X will be normalized before regression by\n",
            " |      subtracting the mean and dividing by the l2-norm.\n",
            " |      If you wish to standardize, please use\n",
            " |      :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
            " |      on an estimator with ``normalize=False``.\n",
            " |  \n",
            " |  copy_X : boolean, optional, default True\n",
            " |      If True, X will be copied; else, it may be overwritten.\n",
            " |  \n",
            " |  max_iter : int, optional\n",
            " |      Maximum number of iterations for conjugate gradient solver.\n",
            " |      For 'sparse_cg' and 'lsqr' solvers, the default value is determined\n",
            " |      by scipy.sparse.linalg. For 'sag' solver, the default value is 1000.\n",
            " |  \n",
            " |  tol : float\n",
            " |      Precision of the solution.\n",
            " |  \n",
            " |  solver : {'auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga'}\n",
            " |      Solver to use in the computational routines:\n",
            " |  \n",
            " |      - 'auto' chooses the solver automatically based on the type of data.\n",
            " |  \n",
            " |      - 'svd' uses a Singular Value Decomposition of X to compute the Ridge\n",
            " |        coefficients. More stable for singular matrices than\n",
            " |        'cholesky'.\n",
            " |  \n",
            " |      - 'cholesky' uses the standard scipy.linalg.solve function to\n",
            " |        obtain a closed-form solution.\n",
            " |  \n",
            " |      - 'sparse_cg' uses the conjugate gradient solver as found in\n",
            " |        scipy.sparse.linalg.cg. As an iterative algorithm, this solver is\n",
            " |        more appropriate than 'cholesky' for large-scale data\n",
            " |        (possibility to set `tol` and `max_iter`).\n",
            " |  \n",
            " |      - 'lsqr' uses the dedicated regularized least-squares routine\n",
            " |        scipy.sparse.linalg.lsqr. It is the fastest and uses an iterative\n",
            " |        procedure.\n",
            " |  \n",
            " |      - 'sag' uses a Stochastic Average Gradient descent, and 'saga' uses\n",
            " |        its improved, unbiased version named SAGA. Both methods also use an\n",
            " |        iterative procedure, and are often faster than other solvers when\n",
            " |        both n_samples and n_features are large. Note that 'sag' and\n",
            " |        'saga' fast convergence is only guaranteed on features with\n",
            " |        approximately the same scale. You can preprocess the data with a\n",
            " |        scaler from sklearn.preprocessing.\n",
            " |  \n",
            " |      All last five solvers support both dense and sparse data. However,\n",
            " |      only 'sag' and 'saga' supports sparse input when `fit_intercept` is\n",
            " |      True.\n",
            " |  \n",
            " |      .. versionadded:: 0.17\n",
            " |         Stochastic Average Gradient descent solver.\n",
            " |      .. versionadded:: 0.19\n",
            " |         SAGA solver.\n",
            " |  \n",
            " |  random_state : int, RandomState instance or None, optional, default None\n",
            " |      The seed of the pseudo random number generator to use when shuffling\n",
            " |      the data.  If int, random_state is the seed used by the random number\n",
            " |      generator; If RandomState instance, random_state is the random number\n",
            " |      generator; If None, the random number generator is the RandomState\n",
            " |      instance used by `np.random`. Used when ``solver`` == 'sag'.\n",
            " |  \n",
            " |      .. versionadded:: 0.17\n",
            " |         *random_state* to support Stochastic Average Gradient.\n",
            " |  \n",
            " |  Attributes\n",
            " |  ----------\n",
            " |  coef_ : array, shape (n_features,) or (n_targets, n_features)\n",
            " |      Weight vector(s).\n",
            " |  \n",
            " |  intercept_ : float | array, shape = (n_targets,)\n",
            " |      Independent term in decision function. Set to 0.0 if\n",
            " |      ``fit_intercept = False``.\n",
            " |  \n",
            " |  n_iter_ : array or None, shape (n_targets,)\n",
            " |      Actual number of iterations for each target. Available only for\n",
            " |      sag and lsqr solvers. Other solvers will return None.\n",
            " |  \n",
            " |      .. versionadded:: 0.17\n",
            " |  \n",
            " |  See also\n",
            " |  --------\n",
            " |  RidgeClassifier : Ridge classifier\n",
            " |  RidgeCV : Ridge regression with built-in cross validation\n",
            " |  :class:`sklearn.kernel_ridge.KernelRidge` : Kernel ridge regression\n",
            " |      combines ridge regression with the kernel trick\n",
            " |  \n",
            " |  Examples\n",
            " |  --------\n",
            " |  >>> from sklearn.linear_model import Ridge\n",
            " |  >>> import numpy as np\n",
            " |  >>> n_samples, n_features = 10, 5\n",
            " |  >>> np.random.seed(0)\n",
            " |  >>> y = np.random.randn(n_samples)\n",
            " |  >>> X = np.random.randn(n_samples, n_features)\n",
            " |  >>> clf = Ridge(alpha=1.0)\n",
            " |  >>> clf.fit(X, y) # doctest: +NORMALIZE_WHITESPACE\n",
            " |  Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
            " |        normalize=False, random_state=None, solver='auto', tol=0.001)\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      Ridge\n",
            " |      _BaseRidge\n",
            " |      abc.NewBase\n",
            " |      sklearn.linear_model.base.LinearModel\n",
            " |      abc.NewBase\n",
            " |      sklearn.base.BaseEstimator\n",
            " |      sklearn.base.RegressorMixin\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self, alpha=1.0, fit_intercept=True, normalize=False, copy_X=True, max_iter=None, tol=0.001, solver='auto', random_state=None)\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |  \n",
            " |  fit(self, X, y, sample_weight=None)\n",
            " |      Fit Ridge regression model\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
            " |          Training data\n",
            " |      \n",
            " |      y : array-like, shape = [n_samples] or [n_samples, n_targets]\n",
            " |          Target values\n",
            " |      \n",
            " |      sample_weight : float or numpy array of shape [n_samples]\n",
            " |          Individual weights for each sample\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      self : returns an instance of self.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes defined here:\n",
            " |  \n",
            " |  __abstractmethods__ = frozenset()\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from sklearn.linear_model.base.LinearModel:\n",
            " |  \n",
            " |  predict(self, X)\n",
            " |      Predict using the linear model\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : array_like or sparse matrix, shape (n_samples, n_features)\n",
            " |          Samples.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      C : array, shape (n_samples,)\n",
            " |          Returns predicted values.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from sklearn.base.BaseEstimator:\n",
            " |  \n",
            " |  __getstate__(self)\n",
            " |  \n",
            " |  __repr__(self)\n",
            " |      Return repr(self).\n",
            " |  \n",
            " |  __setstate__(self, state)\n",
            " |  \n",
            " |  get_params(self, deep=True)\n",
            " |      Get parameters for this estimator.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      deep : boolean, optional\n",
            " |          If True, will return the parameters for this estimator and\n",
            " |          contained subobjects that are estimators.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      params : mapping of string to any\n",
            " |          Parameter names mapped to their values.\n",
            " |  \n",
            " |  set_params(self, **params)\n",
            " |      Set the parameters of this estimator.\n",
            " |      \n",
            " |      The method works on simple estimators as well as on nested objects\n",
            " |      (such as pipelines). The latter have parameters of the form\n",
            " |      ``<component>__<parameter>`` so that it's possible to update each\n",
            " |      component of a nested object.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      self\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from sklearn.base.RegressorMixin:\n",
            " |  \n",
            " |  score(self, X, y, sample_weight=None)\n",
            " |      Returns the coefficient of determination R^2 of the prediction.\n",
            " |      \n",
            " |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
            " |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
            " |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
            " |      The best possible score is 1.0 and it can be negative (because the\n",
            " |      model can be arbitrarily worse). A constant model that always\n",
            " |      predicts the expected value of y, disregarding the input features,\n",
            " |      would get a R^2 score of 0.0.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : array-like, shape = (n_samples, n_features)\n",
            " |          Test samples. For some estimators this may be a\n",
            " |          precomputed kernel matrix instead, shape = (n_samples,\n",
            " |          n_samples_fitted], where n_samples_fitted is the number of\n",
            " |          samples used in the fitting for the estimator.\n",
            " |      \n",
            " |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
            " |          True values for X.\n",
            " |      \n",
            " |      sample_weight : array-like, shape = [n_samples], optional\n",
            " |          Sample weights.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      score : float\n",
            " |          R^2 of self.predict(X) wrt. y.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "F4eY9TKw4S4F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "How to tune alpha? For now, let's loop and try values.\n",
        "\n",
        "(For longterm/stretch/next week, check out [cross-validation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html#sklearn.linear_model.RidgeCV).)"
      ]
    },
    {
      "metadata": {
        "id": "DISx148Z4Sqi",
        "colab_type": "code",
        "outputId": "6df5438d-e168-4714-82c4-ae4688bfdd23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3490
        }
      },
      "cell_type": "code",
      "source": [
        "alphas = []\n",
        "mses = []\n",
        "\n",
        "for alpha in range(0, 200, 1):\n",
        "  ridge_reg_split = Ridge(alpha=alpha).fit(X_train, y_train)\n",
        "  mse = mean_squared_error(y_test, ridge_reg_split.predict(X_test))\n",
        "  print(alpha, mse)\n",
        "  alphas.append(alpha)\n",
        "  mses.append(mse)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 26.273991426429053\n",
            "1 26.192201358877668\n",
            "2 26.118328007697226\n",
            "3 26.051117952293595\n",
            "4 25.989569283205444\n",
            "5 25.93287356811407\n",
            "6 25.880372753122625\n",
            "7 25.831526788692837\n",
            "8 25.785889053385123\n",
            "9 25.743087513207442\n",
            "10 25.702810145277628\n",
            "11 25.66479356379559\n",
            "12 25.628814073392263\n",
            "13 25.59468057863174\n",
            "14 25.56222892458644\n",
            "15 25.53131734932431\n",
            "16 25.50182280665663\n",
            "17 25.473637974726547\n",
            "18 25.44666880864133\n",
            "19 25.420832527348388\n",
            "20 25.396055949160893\n",
            "21 25.372274108781504\n",
            "22 25.349429102822004\n",
            "23 25.327469121742823\n",
            "24 25.30634763462536\n",
            "25 25.286022699825878\n",
            "26 25.266456379775086\n",
            "27 25.24761424230921\n",
            "28 25.229464934192976\n",
            "29 25.211979815108453\n",
            "30 25.19513264248022\n",
            "31 25.178899299197408\n",
            "32 25.163257557659424\n",
            "33 25.14818687468406\n",
            "34 25.13366821272317\n",
            "35 25.11968388357409\n",
            "36 25.106217411385522\n",
            "37 25.093253412260974\n",
            "38 25.080777488180427\n",
            "39 25.068776133307583\n",
            "40 25.057236651039766\n",
            "41 25.046147080399017\n",
            "42 25.035496130566347\n",
            "43 25.02527312253186\n",
            "44 25.015467936977434\n",
            "45 25.006070967630855\n",
            "46 24.997073079433843\n",
            "47 24.98846557095439\n",
            "48 24.980240140548986\n",
            "49 24.972388855844812\n",
            "50 24.96490412616673\n",
            "51 24.95777867758141\n",
            "52 24.951005530271846\n",
            "53 24.944577977990345\n",
            "54 24.9384895693689\n",
            "55 24.9327340908919\n",
            "56 24.92730555135946\n",
            "57 24.92219816768905\n",
            "58 24.917406351921144\n",
            "59 24.912924699309215\n",
            "60 24.9087479773882\n",
            "61 24.904871115926827\n",
            "62 24.901289197679855\n",
            "63 24.897997449864803\n",
            "64 24.8949912362963\n",
            "65 24.892266050117623\n",
            "66 24.889817507075655\n",
            "67 24.88764133929069\n",
            "68 24.885733389477625\n",
            "69 24.88408960557926\n",
            "70 24.882706035776263\n",
            "71 24.88157882384208\n",
            "72 24.880704204813682\n",
            "73 24.880078500952195\n",
            "74 24.879698117969724\n",
            "75 24.879559541500758\n",
            "76 24.87965933379892\n",
            "77 24.879994130641087\n",
            "78 24.8805606384229\n",
            "79 24.881355631430836\n",
            "80 24.882375949277577\n",
            "81 24.88361849448833\n",
            "82 24.88508023022692\n",
            "83 24.886758178151386\n",
            "84 24.888649416389928\n",
            "85 24.89075107762813\n",
            "86 24.89306034730016\n",
            "87 24.895574461876226\n",
            "88 24.898290707239912\n",
            "89 24.90120641714914\n",
            "90 24.90431897177517\n",
            "91 24.907625796314402\n",
            "92 24.911124359668285\n",
            "93 24.914812173186736\n",
            "94 24.918686789471128\n",
            "95 24.92274580123304\n",
            "96 24.92698684020521\n",
            "97 24.93140757610152\n",
            "98 24.93600571562298\n",
            "99 24.94077900150688\n",
            "100 24.945725211616683\n",
            "101 24.950842158070053\n",
            "102 24.95612768640294\n",
            "103 24.96157967476758\n",
            "104 24.96719603316249\n",
            "105 24.972974702692664\n",
            "106 24.97891365485829\n",
            "107 24.985010890870456\n",
            "108 24.99126444099231\n",
            "109 24.99767236390434\n",
            "110 25.004232746092597\n",
            "111 25.010943701258537\n",
            "112 25.017803369749362\n",
            "113 25.02480991800798\n",
            "114 25.0319615380414\n",
            "115 25.03925644690685\n",
            "116 25.04669288621445\n",
            "117 25.05426912164612\n",
            "118 25.061983442489353\n",
            "119 25.069834161185728\n",
            "120 25.077819612893087\n",
            "121 25.085938155060855\n",
            "122 25.09418816701804\n",
            "123 25.102568049573183\n",
            "124 25.111076224625787\n",
            "125 25.119711134788766\n",
            "126 25.128471243021377\n",
            "127 25.137355032272303\n",
            "128 25.14636100513223\n",
            "129 25.155487683495902\n",
            "130 25.164733608232837\n",
            "131 25.174097338866744\n",
            "132 25.183577453263027\n",
            "133 25.193172547324206\n",
            "134 25.2028812346929\n",
            "135 25.212702146462046\n",
            "136 25.222633930892243\n",
            "137 25.232675253135735\n",
            "138 25.24282479496694\n",
            "139 25.25308125451928\n",
            "140 25.26344334602802\n",
            "141 25.273909799578966\n",
            "142 25.284479360862818\n",
            "143 25.29515079093497\n",
            "144 25.305922865980495\n",
            "145 25.31679437708437\n",
            "146 25.32776413000649\n",
            "147 25.338830944961526\n",
            "148 25.349993656403374\n",
            "149 25.361251112814\n",
            "150 25.37260217649681\n",
            "151 25.384045723373994\n",
            "152 25.39558064278813\n",
            "153 25.407205837307533\n",
            "154 25.418920222535693\n",
            "155 25.430722726924202\n",
            "156 25.442612291589445\n",
            "157 25.45458787013271\n",
            "158 25.466648428463827\n",
            "159 25.478792944627976\n",
            "160 25.491020408635883\n",
            "161 25.50332982229701\n",
            "162 25.515720199055906\n",
            "163 25.528190563831558\n",
            "164 25.540739952859465\n",
            "165 25.5533674135368\n",
            "166 25.56607200427009\n",
            "167 25.578852794325684\n",
            "168 25.591708863682904\n",
            "169 25.604639302889613\n",
            "170 25.617643212920317\n",
            "171 25.63071970503678\n",
            "172 25.643867900650903\n",
            "173 25.657086931189966\n",
            "174 25.670375937964163\n",
            "175 25.68373407203625\n",
            "176 25.697160494093474\n",
            "177 25.71065437432154\n",
            "178 25.724214892280617\n",
            "179 25.737841236783435\n",
            "180 25.751532605775324\n",
            "181 25.765288206216116\n",
            "182 25.779107253964067\n",
            "183 25.792988973661497\n",
            "184 25.8069325986223\n",
            "185 25.820937370721257\n",
            "186 25.83500254028498\n",
            "187 25.849127365984653\n",
            "188 25.863311114730404\n",
            "189 25.87755306156723\n",
            "190 25.891852489572667\n",
            "191 25.906208689755893\n",
            "192 25.920620960958406\n",
            "193 25.93508860975623\n",
            "194 25.949610950363557\n",
            "195 25.964187304537848\n",
            "196 25.97881700148633\n",
            "197 25.99349937777395\n",
            "198 26.008233777232597\n",
            "199 26.023019550871716\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "iRB3KHyWiO4y",
        "colab_type": "code",
        "outputId": "a98e6ff2-c184-4fe5-eb76-a64b2705c4b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        }
      },
      "cell_type": "code",
      "source": [
        "from matplotlib.pyplot import scatter\n",
        "scatter(alphas, mses);"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAecAAAFKCAYAAAAnj5dkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "WzgTBd-FcctM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## What's the intuition? What are we doing?\n",
        "\n",
        "The `alpha` parameter corresponds to the weight being given to the extra penalty being calculated by [Tikhonov regularization](https://en.wikipedia.org/wiki/Tikhonov_regularization) (this parameter is sometimes referred to as $\\lambda$ in the context of ridge regression).\n",
        "\n",
        "Normal linear regression (OLS) minimizes the **sum of square error of the residuals**.\n",
        "\n",
        "Ridge regression minimizes the **sum of square error of the residuals** *AND* **the squared slope of the fit model, times the alpha parameter**.\n",
        "\n",
        "This is why the MSE for the first model in the for loop (`alpha=0`) is the same as the MSE for linear regression - it's the same model!\n",
        "\n",
        "As `alpha` is increased, we give more and more penalty to a steep slope. In two or three dimensions this is fairly easy to visualize - beyond, think of it as penalizing coefficient size. Each coefficient represents the slope of an individual dimension (feature) of the model, so ridge regression is just squaring and summing those.\n",
        "\n",
        "So while `alpha=0` reduces to OLS, as `alpha` approaches infinity eventually the penalty gets so extreme that the model will always output every coefficient as 0 (any non-zero coefficient resulting in a penalty that outweighs whatever improvement in the residuals), and just fit a flat model with intercept at the mean of the dependent variable.\n",
        "\n",
        "Of course, what we want is somewhere in-between these extremes. Intuitively, what we want to do is apply an appropriate \"cost\" or penalty to the model for fitting parameters, much like adjusted $R^2$ takes into account the cost of adding complexity to a model. What exactly is an appropriate penalty will vary, so you'll have to put on your model comparison hat and give it a go!\n",
        "\n",
        "PS - scaling the data helps, as that way this cost is consistent and can be added uniformly across features, and it is simpler to search for the `alpha` parameter.\n",
        "\n",
        "### Bonus - magic! ✨\n",
        "\n",
        "Ridge regression doesn't just reduce overfitting and help with the third aspect of well-posed problems (poor generalizability). It can also fix the first two (no unique solution)!"
      ]
    },
    {
      "metadata": {
        "id": "rdogs9EMX6Vd",
        "colab_type": "code",
        "outputId": "eaf2492e-2a61-4e96-c2eb-a1baf6d2f4c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "df_tiny = df.sample(10, random_state=27)\n",
        "print(df_tiny.shape)\n",
        "X = df_tiny.drop('Price', axis='columns')\n",
        "y = df_tiny.Price\n",
        "\n",
        "lin_reg = LinearRegression().fit(X, y)\n",
        "lin_reg.score(X, y)  # Perfect multi-collinearity!\n",
        "# NOTE - True OLS would 💥 here\n",
        "# scikit protects us from actual error, but still gives a poor model"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10, 14)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "zesVR59NhA7A",
        "colab_type": "code",
        "outputId": "83b429ca-d564-4d0b-fe0c-0b8943b6275c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "ridge_reg = Ridge().fit(X, y)\n",
        "ridge_reg.score(X, y)  # More plausible (not \"perfect\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9760119331942763"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "WP6zwLtshaVR",
        "colab_type": "code",
        "outputId": "50f9033f-fbbc-4dcb-c96c-17e44bb3df81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Using our earlier test split\n",
        "mean_squared_error(y_test, lin_reg.predict(X_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "103.04429449784261"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "QeL_O8vNhSqj",
        "colab_type": "code",
        "outputId": "a3aefe55-881f-4667-869e-c04d8c17a95e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Ridge generalizes *way* better (and we've not even tuned alpha)\n",
        "mean_squared_error(y_test, ridge_reg.predict(X_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "41.79869373639458"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "x2N5WDV6nd3S",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## And a bit more math\n",
        "\n",
        "The regularization used by Ridge Regression is also known as **$L^2$ regularization**, due to the squaring of the slopes being summed. This corresponds to [$L^2$ space](https://en.wikipedia.org/wiki/Square-integrable_function), a metric space of square-integrable functions that generally measure what we intuitively think of as \"distance\" (at least, on a plane) - what is referred to as Euclidean distance.\n",
        "\n",
        "The other famous norm is $L^1$, also known as [taxicab geometry](https://en.wikipedia.org/wiki/Taxicab_geometry), because it follows the \"grid\" to measure distance like a car driving around city blocks (rather than going directly like $L^2$). When referred to as a distance this is called \"Manhattan distance\", and can be used for regularization (see [LASSO](https://en.wikipedia.org/wiki/Lasso_(statistics%29), which [uses the $L^1$ norm](https://www.quora.com/What-is-the-difference-between-L1-and-L2-regularization-How-does-it-solve-the-problem-of-overfitting-Which-regularizer-to-use-and-when)).\n",
        "\n",
        "All this comes down to - regularization means increasing model bias by \"watering down\" coefficients with a penalty typically based on some sort of distance metric, and thus reducing variance (overfitting the model to the noise in the data). It gives us another lever to try and another tool for our toolchest!\n",
        "\n",
        "## Putting it all together - one last example\n",
        "\n",
        "The official scikit-learn documentation has many excellent examples - [this one](https://scikit-learn.org/stable/auto_examples/linear_model/plot_ols_ridge_variance.html#sphx-glr-auto-examples-linear-model-plot-ols-ridge-variance-py) illustrates how ridge regression effectively reduces the variance, again by increasing the bias, penalizing coefficients to reduce the effectiveness of features (but also the impact of noise).\n",
        "\n",
        "```\n",
        "Due to the few points in each dimension and the straight line that linear regression uses to follow these points as well as it can, noise on the observations will cause great variance as shown in the first plot. Every line’s slope can vary quite a bit for each prediction due to the noise induced in the observations.\n",
        "\n",
        "Ridge regression is basically minimizing a penalised version of the least-squared function. The penalising shrinks the value of the regression coefficients. Despite the few data points in each dimension, the slope of the prediction is much more stable and the variance in the line itself is greatly reduced, in comparison to that of the standard linear regression\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "LaOYdswIB6Bo",
        "colab_type": "code",
        "outputId": "7081e218-bc17-478a-f6dd-37735fce78c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "cell_type": "code",
      "source": [
        "# Code source: Gaël Varoquaux\n",
        "# Modified for documentation by Jaques Grobler\n",
        "# License: BSD 3 clause\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn import linear_model\n",
        "\n",
        "X_train = np.c_[.5, 1].T\n",
        "y_train = [.5, 1]\n",
        "X_test = np.c_[0, 2].T\n",
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "classifiers = dict(ols=linear_model.LinearRegression(),\n",
        "                   ridge=linear_model.Ridge(alpha=.1))\n",
        "\n",
        "for name, clf in classifiers.items():\n",
        "    fig, ax = plt.subplots(figsize=(4, 3))\n",
        "\n",
        "    for _ in range(6):\n",
        "        this_X = .1 * np.random.normal(size=(2, 1)) + X_train\n",
        "        clf.fit(this_X, y_train)\n",
        "\n",
        "        ax.plot(X_test, clf.predict(X_test), color='gray')\n",
        "        ax.scatter(this_X, y_train, s=3, c='gray', marker='o', zorder=10)\n",
        "\n",
        "    clf.fit(X_train, y_train)\n",
        "    ax.plot(X_test, clf.predict(X_test), linewidth=2, color='blue')\n",
        "    ax.scatter(X_train, y_train, s=30, c='red', marker='+', zorder=10)\n",
        "\n",
        "    ax.set_title(name)\n",
        "    ax.set_xlim(0, 2)\n",
        "    ax.set_ylim((0, 1.6))\n",
        "    ax.set_xlabel('X')\n",
        "    ax.set_ylabel('y')\n",
        "\n",
        "    fig.tight_layout()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAARoAAADMCAYAAACso2w8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nY6MhIsX9DdC0SCVQwtcNAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 288x216 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAARoAAADMCAYAAACso2w8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
            "text/plain": [
              "<Figure size 288x216 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "PTFSK48TqeB-"
      },
      "cell_type": "markdown",
      "source": [
        "# Assignment\n",
        "\n",
        "Following is data describing characteristics of blog posts, with a target feature of how many comments will be posted in the following 24 hours.\n",
        "\n",
        "https://archive.ics.uci.edu/ml/datasets/BlogFeedback\n",
        "\n",
        "Investigate - you can try both linear and ridge. You can also sample to smaller data size and see if that makes ridge more important. Don't forget to scale!\n",
        "\n",
        "Focus on the training data, but if you want to load and compare to any of the test data files you can also do that.\n",
        "\n",
        "Note - Ridge may not be that fundamentally superior in this case. That's OK! It's still good to practice both, and see if you can find parameters or sample sizes where ridge does generalize and perform better.\n",
        "\n",
        "When you've fit models to your satisfaction, answer the following question:\n",
        "\n",
        "```\n",
        "Did you find cases where Ridge performed better? If so, describe (alpha parameter, sample size, any other relevant info/processing). If not, what do you think that tells you about the data?\n",
        "```\n",
        "\n",
        "You can create whatever plots, tables, or other results support your argument. In this case, your target audience is a fellow data scientist, *not* a layperson, so feel free to dig in!"
      ]
    },
    {
      "metadata": {
        "id": "PoZguwBeUlq_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.preprocessing import scale\n",
        "\n",
        "from sklearn import linear_model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import Ridge"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sUOAF_rDVwWT",
        "colab_type": "code",
        "outputId": "0c72de78-5745-4ceb-b742-b07270766740",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1314
        }
      },
      "cell_type": "code",
      "source": [
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00304/BlogFeedback.zip\n",
        "!unzip BlogFeedback.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-01-25 15:26:40--  https://archive.ics.uci.edu/ml/machine-learning-databases/00304/BlogFeedback.zip\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.249\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.249|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2583605 (2.5M) [application/zip]\n",
            "Saving to: ‘BlogFeedback.zip’\n",
            "\n",
            "BlogFeedback.zip    100%[===================>]   2.46M  3.34MB/s    in 0.7s    \n",
            "\n",
            "2019-01-25 15:26:46 (3.34 MB/s) - ‘BlogFeedback.zip’ saved [2583605/2583605]\n",
            "\n",
            "Archive:  BlogFeedback.zip\n",
            "  inflating: blogData_test-2012.02.01.00_00.csv  \n",
            "  inflating: blogData_test-2012.02.02.00_00.csv  \n",
            "  inflating: blogData_test-2012.02.03.00_00.csv  \n",
            "  inflating: blogData_test-2012.02.04.00_00.csv  \n",
            "  inflating: blogData_test-2012.02.05.00_00.csv  \n",
            "  inflating: blogData_test-2012.02.06.00_00.csv  \n",
            "  inflating: blogData_test-2012.02.07.00_00.csv  \n",
            "  inflating: blogData_test-2012.02.08.00_00.csv  \n",
            "  inflating: blogData_test-2012.02.09.00_00.csv  \n",
            "  inflating: blogData_test-2012.02.10.00_00.csv  \n",
            "  inflating: blogData_test-2012.02.11.00_00.csv  \n",
            "  inflating: blogData_test-2012.02.12.00_00.csv  \n",
            "  inflating: blogData_test-2012.02.13.00_00.csv  \n",
            "  inflating: blogData_test-2012.02.14.00_00.csv  \n",
            "  inflating: blogData_test-2012.02.15.00_00.csv  \n",
            "  inflating: blogData_test-2012.02.16.00_00.csv  \n",
            "  inflating: blogData_test-2012.02.17.00_00.csv  \n",
            "  inflating: blogData_test-2012.02.18.00_00.csv  \n",
            "  inflating: blogData_test-2012.02.19.00_00.csv  \n",
            "  inflating: blogData_test-2012.02.20.00_00.csv  \n",
            "  inflating: blogData_test-2012.02.21.00_00.csv  \n",
            "  inflating: blogData_test-2012.02.22.00_00.csv  \n",
            "  inflating: blogData_test-2012.02.23.00_00.csv  \n",
            "  inflating: blogData_test-2012.02.24.00_00.csv  \n",
            "  inflating: blogData_test-2012.02.25.00_00.csv  \n",
            "  inflating: blogData_test-2012.02.26.00_00.csv  \n",
            "  inflating: blogData_test-2012.02.27.00_00.csv  \n",
            "  inflating: blogData_test-2012.02.28.00_00.csv  \n",
            "  inflating: blogData_test-2012.02.29.00_00.csv  \n",
            "  inflating: blogData_test-2012.03.01.00_00.csv  \n",
            "  inflating: blogData_test-2012.03.02.00_00.csv  \n",
            "  inflating: blogData_test-2012.03.03.00_00.csv  \n",
            "  inflating: blogData_test-2012.03.04.00_00.csv  \n",
            "  inflating: blogData_test-2012.03.05.00_00.csv  \n",
            "  inflating: blogData_test-2012.03.06.00_00.csv  \n",
            "  inflating: blogData_test-2012.03.07.00_00.csv  \n",
            "  inflating: blogData_test-2012.03.08.00_00.csv  \n",
            "  inflating: blogData_test-2012.03.09.00_00.csv  \n",
            "  inflating: blogData_test-2012.03.10.00_00.csv  \n",
            "  inflating: blogData_test-2012.03.11.00_00.csv  \n",
            "  inflating: blogData_test-2012.03.12.00_00.csv  \n",
            "  inflating: blogData_test-2012.03.13.00_00.csv  \n",
            "  inflating: blogData_test-2012.03.14.00_00.csv  \n",
            "  inflating: blogData_test-2012.03.15.00_00.csv  \n",
            "  inflating: blogData_test-2012.03.16.00_00.csv  \n",
            "  inflating: blogData_test-2012.03.17.00_00.csv  \n",
            "  inflating: blogData_test-2012.03.18.00_00.csv  \n",
            "  inflating: blogData_test-2012.03.19.00_00.csv  \n",
            "  inflating: blogData_test-2012.03.20.00_00.csv  \n",
            "  inflating: blogData_test-2012.03.21.00_00.csv  \n",
            "  inflating: blogData_test-2012.03.22.00_00.csv  \n",
            "  inflating: blogData_test-2012.03.23.00_00.csv  \n",
            "  inflating: blogData_test-2012.03.24.00_00.csv  \n",
            "  inflating: blogData_test-2012.03.25.00_00.csv  \n",
            "  inflating: blogData_test-2012.03.26.01_00.csv  \n",
            "  inflating: blogData_test-2012.03.27.01_00.csv  \n",
            "  inflating: blogData_test-2012.03.28.01_00.csv  \n",
            "  inflating: blogData_test-2012.03.29.01_00.csv  \n",
            "  inflating: blogData_test-2012.03.30.01_00.csv  \n",
            "  inflating: blogData_test-2012.03.31.01_00.csv  \n",
            "  inflating: blogData_train.csv      \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "62kl65RVWq7J",
        "colab_type": "code",
        "outputId": "f54c3110-2e98-4af9-f985-f650aff43b1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        }
      },
      "cell_type": "code",
      "source": [
        "blog = pd.read_csv('blogData_train.csv', header=None)\n",
        "blog.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>271</th>\n",
              "      <th>272</th>\n",
              "      <th>273</th>\n",
              "      <th>274</th>\n",
              "      <th>275</th>\n",
              "      <th>276</th>\n",
              "      <th>277</th>\n",
              "      <th>278</th>\n",
              "      <th>279</th>\n",
              "      <th>280</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>40.30467</td>\n",
              "      <td>53.845657</td>\n",
              "      <td>0.0</td>\n",
              "      <td>401.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.52416</td>\n",
              "      <td>32.44188</td>\n",
              "      <td>0.0</td>\n",
              "      <td>377.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>40.30467</td>\n",
              "      <td>53.845657</td>\n",
              "      <td>0.0</td>\n",
              "      <td>401.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.52416</td>\n",
              "      <td>32.44188</td>\n",
              "      <td>0.0</td>\n",
              "      <td>377.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>40.30467</td>\n",
              "      <td>53.845657</td>\n",
              "      <td>0.0</td>\n",
              "      <td>401.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.52416</td>\n",
              "      <td>32.44188</td>\n",
              "      <td>0.0</td>\n",
              "      <td>377.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>40.30467</td>\n",
              "      <td>53.845657</td>\n",
              "      <td>0.0</td>\n",
              "      <td>401.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.52416</td>\n",
              "      <td>32.44188</td>\n",
              "      <td>0.0</td>\n",
              "      <td>377.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>40.30467</td>\n",
              "      <td>53.845657</td>\n",
              "      <td>0.0</td>\n",
              "      <td>401.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.52416</td>\n",
              "      <td>32.44188</td>\n",
              "      <td>0.0</td>\n",
              "      <td>377.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>27.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 281 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        0          1    2      3     4         5         6    7      8    9    \\\n",
              "0  40.30467  53.845657  0.0  401.0  15.0  15.52416  32.44188  0.0  377.0  3.0   \n",
              "1  40.30467  53.845657  0.0  401.0  15.0  15.52416  32.44188  0.0  377.0  3.0   \n",
              "2  40.30467  53.845657  0.0  401.0  15.0  15.52416  32.44188  0.0  377.0  3.0   \n",
              "3  40.30467  53.845657  0.0  401.0  15.0  15.52416  32.44188  0.0  377.0  3.0   \n",
              "4  40.30467  53.845657  0.0  401.0  15.0  15.52416  32.44188  0.0  377.0  3.0   \n",
              "\n",
              "   ...   271  272  273  274  275  276  277  278  279   280  \n",
              "0  ...   0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   1.0  \n",
              "1  ...   1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  \n",
              "2  ...   1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  \n",
              "3  ...   0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   1.0  \n",
              "4  ...   0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  27.0  \n",
              "\n",
              "[5 rows x 281 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "z8G1HPE6ZP3O",
        "colab_type": "code",
        "outputId": "a887366a-9089-4b41-c4b5-0ecf5d8ab6ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        }
      },
      "cell_type": "code",
      "source": [
        "#scale the data!\n",
        "blog = scale(blog)\n",
        "blog_df = pd.DataFrame(blog)\n",
        "blog_df.head()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>271</th>\n",
              "      <th>272</th>\n",
              "      <th>273</th>\n",
              "      <th>274</th>\n",
              "      <th>275</th>\n",
              "      <th>276</th>\n",
              "      <th>277</th>\n",
              "      <th>278</th>\n",
              "      <th>279</th>\n",
              "      <th>280</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.010876</td>\n",
              "      <td>0.112877</td>\n",
              "      <td>-0.052468</td>\n",
              "      <td>0.138521</td>\n",
              "      <td>-0.139108</td>\n",
              "      <td>0.009598</td>\n",
              "      <td>0.116182</td>\n",
              "      <td>-0.020836</td>\n",
              "      <td>0.368246</td>\n",
              "      <td>-0.119031</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.454696</td>\n",
              "      <td>2.272362</td>\n",
              "      <td>-0.427399</td>\n",
              "      <td>-0.326158</td>\n",
              "      <td>-0.312402</td>\n",
              "      <td>-0.08286</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.045171</td>\n",
              "      <td>-0.037836</td>\n",
              "      <td>-0.152885</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.010876</td>\n",
              "      <td>0.112877</td>\n",
              "      <td>-0.052468</td>\n",
              "      <td>0.138521</td>\n",
              "      <td>-0.139108</td>\n",
              "      <td>0.009598</td>\n",
              "      <td>0.116182</td>\n",
              "      <td>-0.020836</td>\n",
              "      <td>0.368246</td>\n",
              "      <td>-0.119031</td>\n",
              "      <td>...</td>\n",
              "      <td>2.199274</td>\n",
              "      <td>-0.440071</td>\n",
              "      <td>-0.427399</td>\n",
              "      <td>-0.326158</td>\n",
              "      <td>-0.312402</td>\n",
              "      <td>-0.08286</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.045171</td>\n",
              "      <td>-0.037836</td>\n",
              "      <td>-0.179406</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.010876</td>\n",
              "      <td>0.112877</td>\n",
              "      <td>-0.052468</td>\n",
              "      <td>0.138521</td>\n",
              "      <td>-0.139108</td>\n",
              "      <td>0.009598</td>\n",
              "      <td>0.116182</td>\n",
              "      <td>-0.020836</td>\n",
              "      <td>0.368246</td>\n",
              "      <td>-0.119031</td>\n",
              "      <td>...</td>\n",
              "      <td>2.199274</td>\n",
              "      <td>-0.440071</td>\n",
              "      <td>-0.427399</td>\n",
              "      <td>-0.326158</td>\n",
              "      <td>-0.312402</td>\n",
              "      <td>-0.08286</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.045171</td>\n",
              "      <td>-0.037836</td>\n",
              "      <td>-0.179406</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.010876</td>\n",
              "      <td>0.112877</td>\n",
              "      <td>-0.052468</td>\n",
              "      <td>0.138521</td>\n",
              "      <td>-0.139108</td>\n",
              "      <td>0.009598</td>\n",
              "      <td>0.116182</td>\n",
              "      <td>-0.020836</td>\n",
              "      <td>0.368246</td>\n",
              "      <td>-0.119031</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.454696</td>\n",
              "      <td>2.272362</td>\n",
              "      <td>-0.427399</td>\n",
              "      <td>-0.326158</td>\n",
              "      <td>-0.312402</td>\n",
              "      <td>-0.08286</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.045171</td>\n",
              "      <td>-0.037836</td>\n",
              "      <td>-0.152885</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.010876</td>\n",
              "      <td>0.112877</td>\n",
              "      <td>-0.052468</td>\n",
              "      <td>0.138521</td>\n",
              "      <td>-0.139108</td>\n",
              "      <td>0.009598</td>\n",
              "      <td>0.116182</td>\n",
              "      <td>-0.020836</td>\n",
              "      <td>0.368246</td>\n",
              "      <td>-0.119031</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.454696</td>\n",
              "      <td>2.272362</td>\n",
              "      <td>-0.427399</td>\n",
              "      <td>-0.326158</td>\n",
              "      <td>-0.312402</td>\n",
              "      <td>-0.08286</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.045171</td>\n",
              "      <td>-0.037836</td>\n",
              "      <td>0.536657</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 281 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        0         1         2         3         4         5         6    \\\n",
              "0  0.010876  0.112877 -0.052468  0.138521 -0.139108  0.009598  0.116182   \n",
              "1  0.010876  0.112877 -0.052468  0.138521 -0.139108  0.009598  0.116182   \n",
              "2  0.010876  0.112877 -0.052468  0.138521 -0.139108  0.009598  0.116182   \n",
              "3  0.010876  0.112877 -0.052468  0.138521 -0.139108  0.009598  0.116182   \n",
              "4  0.010876  0.112877 -0.052468  0.138521 -0.139108  0.009598  0.116182   \n",
              "\n",
              "        7         8         9      ...          271       272       273  \\\n",
              "0 -0.020836  0.368246 -0.119031    ...    -0.454696  2.272362 -0.427399   \n",
              "1 -0.020836  0.368246 -0.119031    ...     2.199274 -0.440071 -0.427399   \n",
              "2 -0.020836  0.368246 -0.119031    ...     2.199274 -0.440071 -0.427399   \n",
              "3 -0.020836  0.368246 -0.119031    ...    -0.454696  2.272362 -0.427399   \n",
              "4 -0.020836  0.368246 -0.119031    ...    -0.454696  2.272362 -0.427399   \n",
              "\n",
              "        274       275      276  277       278       279       280  \n",
              "0 -0.326158 -0.312402 -0.08286  0.0 -0.045171 -0.037836 -0.152885  \n",
              "1 -0.326158 -0.312402 -0.08286  0.0 -0.045171 -0.037836 -0.179406  \n",
              "2 -0.326158 -0.312402 -0.08286  0.0 -0.045171 -0.037836 -0.179406  \n",
              "3 -0.326158 -0.312402 -0.08286  0.0 -0.045171 -0.037836 -0.152885  \n",
              "4 -0.326158 -0.312402 -0.08286  0.0 -0.045171 -0.037836  0.536657  \n",
              "\n",
              "[5 rows x 281 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "0u-XILd6Yq4q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#the target column, listing the number of comments in the first 24 hous after a\n",
        "#blog was posted\n",
        "target = blog_df[blog_df.columns[53]]\n",
        "\n",
        "X = blog_df.drop(blog_df.columns[53], axis=1)\n",
        "y = blog_df[blog_df.columns[53]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Tj9YBGKXdRX4",
        "colab_type": "code",
        "outputId": "377b7648-e188-44e2-9467-406b713e87cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(52397,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "uitdCllNdVxl",
        "colab_type": "code",
        "outputId": "d9c54dd3-de25-43c1-e74c-ee71b828ee26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(52397, 280)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "ZowaxzRpXwtw",
        "colab_type": "code",
        "outputId": "b9a6b802-26c8-4f99-fcc1-191ef033888a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "#see the linear regression results:\n",
        "#split the data into training and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state=42)\n",
        "\n",
        "#fit the model\n",
        "lin_reg_split = LinearRegression().fit(X_train, y_train)\n",
        "\n",
        "#print the mse\n",
        "print(mean_squared_error(y_train, lin_reg_split.predict(X_train)))\n",
        "print(mean_squared_error(y_test, lin_reg_split.predict(X_test)))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.03758534172065772\n",
            "0.05356424166507031\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4_EjhNF2drbz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The test error with linear regression model is much higher, which tells us about model overfitting to training data. will try to mitigate this by using Ridge regression"
      ]
    },
    {
      "metadata": {
        "id": "Pl7STj6dd43f",
        "colab_type": "code",
        "outputId": "2cd941e1-ec9f-410f-9142-1db50106e45a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "ridge_reg = Ridge().fit(X_train, y_train)\n",
        "\n",
        "#print the mse\n",
        "print(mean_squared_error(y_train, ridge_reg.predict(X_train)))\n",
        "print(mean_squared_error(y_test, ridge_reg.predict(X_test)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.03758457695233604\n",
            "0.05356749922309746\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PxfVOpl3emuB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "the test error is slightly worse, which is expected, since some error is added to the data, let's try to alter alpha value to get better results"
      ]
    },
    {
      "metadata": {
        "id": "ASRhHXDveth1",
        "colab_type": "code",
        "outputId": "af8330d4-676b-4c6b-cafc-01cf292a63a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3572
        }
      },
      "cell_type": "code",
      "source": [
        "alphas = []\n",
        "mses = []\n",
        "\n",
        "for alpha in range(0, 200, 1):\n",
        "  ridge_reg_split = Ridge(alpha=alpha).fit(X_train, y_train)\n",
        "  mse = mean_squared_error(y_test, ridge_reg_split.predict(X_test))\n",
        "  print(alpha, mse)\n",
        "  alphas.append(alpha)\n",
        "  mses.append(mse)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0.0538180921362293\n",
            "1 0.05356749922309746\n",
            "2 0.053573006015058565\n",
            "3 0.053578237910733816\n",
            "4 0.0535834641416686\n",
            "5 0.053588694231711045\n",
            "6 0.053593907171781856\n",
            "7 0.053599085963544346\n",
            "8 0.05360422063996296\n",
            "9 0.05360930645675298\n",
            "10 0.05361434198233161\n",
            "11 0.05361932775094366\n",
            "12 0.05362426540400211\n",
            "13 0.053629157162861735\n",
            "14 0.05363400551124329\n",
            "15 0.05363881300706002\n",
            "16 0.05364358217341472\n",
            "17 0.05364831543788165\n",
            "18 0.053653015101127036\n",
            "19 0.05365768332325035\n",
            "20 0.05366232212067981\n",
            "21 0.053666933369204944\n",
            "22 0.053671518810413606\n",
            "23 0.05367608005985902\n",
            "24 0.05368061861592577\n",
            "25 0.05368513586878204\n",
            "26 0.0536896331090654\n",
            "27 0.053694111536095296\n",
            "28 0.053698572265527565\n",
            "29 0.05370301633640556\n",
            "30 0.05370744471761809\n",
            "31 0.053711858313784555\n",
            "32 0.05371625797060005\n",
            "33 0.05372064447968678\n",
            "34 0.05372501858298949\n",
            "35 0.05372938097675644\n",
            "36 0.053733732315148726\n",
            "37 0.05373807321350979\n",
            "38 0.05374240425133559\n",
            "39 0.05374672597496728\n",
            "40 0.05375103890004268\n",
            "41 0.053755343513724844\n",
            "42 0.05375964027673614\n",
            "43 0.053763929625211\n",
            "44 0.05376821197239057\n",
            "45 0.053772487710173095\n",
            "46 0.05377675721053505\n",
            "47 0.053781020826833\n",
            "48 0.05378527889500092\n",
            "49 0.05378953173465291\n",
            "50 0.053793779650095984\n",
            "51 0.05379802293126715\n",
            "52 0.05380226185459719\n",
            "53 0.0538064966838097\n",
            "54 0.05381072767066127\n",
            "55 0.05381495505562746\n",
            "56 0.053819179068539696\n",
            "57 0.05382339992917613\n",
            "58 0.05382761784781278\n",
            "59 0.053831833025735645\n",
            "60 0.05383604565571846\n",
            "61 0.053840255922468945\n",
            "62 0.05384446400304515\n",
            "63 0.05384867006724497\n",
            "64 0.0538528742779715\n",
            "65 0.05385707679157421\n",
            "66 0.05386127775816946\n",
            "67 0.05386547732194171\n",
            "68 0.05386967562142586\n",
            "69 0.05387387278977392\n",
            "70 0.05387806895500434\n",
            "71 0.053882264240237746\n",
            "72 0.05388645876392\n",
            "73 0.053890652640030956\n",
            "74 0.053894845978282806\n",
            "75 0.05389903888430734\n",
            "76 0.053903231459832036\n",
            "77 0.05390742380284799\n",
            "78 0.05391161600776854\n",
            "79 0.05391580816557881\n",
            "80 0.05392000036397837\n",
            "81 0.05392419268751661\n",
            "82 0.053928385217720295\n",
            "83 0.05393257803321587\n",
            "84 0.05393677120984528\n",
            "85 0.053940964820775435\n",
            "86 0.05394515893660375\n",
            "87 0.053949353625457275\n",
            "88 0.053953548953088716\n",
            "89 0.053957744982965246\n",
            "90 0.05396194177635685\n",
            "91 0.053966139392417366\n",
            "92 0.05397033788826371\n",
            "93 0.05397453731905092\n",
            "94 0.0539787377380441\n",
            "95 0.053982939196686654\n",
            "96 0.05398714174466568\n",
            "97 0.053991345429976204\n",
            "98 0.05399555029897933\n",
            "99 0.05399975639646114\n",
            "100 0.054003963765687016\n",
            "101 0.05400817244845517\n",
            "102 0.05401238248514662\n",
            "103 0.05401659391477428\n",
            "104 0.05402080677502902\n",
            "105 0.054025021102324815\n",
            "106 0.05402923693184179\n",
            "107 0.054033454297567084\n",
            "108 0.05403767323233406\n",
            "109 0.0540418937678618\n",
            "110 0.05404611593478983\n",
            "111 0.05405033976271552\n",
            "112 0.054054565280225904\n",
            "113 0.05405879251493068\n",
            "114 0.05406302149349526\n",
            "115 0.05406725224166876\n",
            "116 0.05407148478431422\n",
            "117 0.05407571914543653\n",
            "118 0.05407995534820931\n",
            "119 0.054084193415000835\n",
            "120 0.05408843336739995\n",
            "121 0.05409267522623836\n",
            "122 0.054096919011616056\n",
            "123 0.05410116474292277\n",
            "124 0.05410541243886014\n",
            "125 0.05410966211746215\n",
            "126 0.05411391379611595\n",
            "127 0.05411816749158061\n",
            "128 0.0541224232200079\n",
            "129 0.05412668099695787\n",
            "130 0.05413094083741907\n",
            "131 0.05413520275582361\n",
            "132 0.05413946676606482\n",
            "133 0.054143732881513106\n",
            "134 0.054148001115030535\n",
            "135 0.05415227147898716\n",
            "136 0.054156543985274065\n",
            "137 0.054160818645318104\n",
            "138 0.05416509547009568\n",
            "139 0.054169374470144714\n",
            "140 0.05417365565557885\n",
            "141 0.05417793903609822\n",
            "142 0.05418222462100223\n",
            "143 0.05418651241920159\n",
            "144 0.054190802439227664\n",
            "145 0.0541950946892452\n",
            "146 0.05419938917706163\n",
            "147 0.05420368591013818\n",
            "148 0.05420798489559847\n",
            "149 0.054212286140238986\n",
            "150 0.05421658965053853\n",
            "151 0.05422089543266613\n",
            "152 0.054225203492490805\n",
            "153 0.0542295138355897\n",
            "154 0.05423382646725628\n",
            "155 0.054238141392507946\n",
            "156 0.05424245861609401\n",
            "157 0.05424677814250375\n",
            "158 0.054251099975972696\n",
            "159 0.05425542412049044\n",
            "160 0.05425975057980691\n",
            "161 0.05426407935743992\n",
            "162 0.054268410456680724\n",
            "163 0.054272743880600854\n",
            "164 0.05427707963205779\n",
            "165 0.05428141771370133\n",
            "166 0.054285758127979575\n",
            "167 0.05429010087714319\n",
            "168 0.05429444596325309\n",
            "169 0.05429879338818291\n",
            "170 0.054303143153627095\n",
            "171 0.054307495261103934\n",
            "172 0.05431184971196077\n",
            "173 0.05431620650737929\n",
            "174 0.054320565648379615\n",
            "175 0.054324927135824895\n",
            "176 0.05432929097042617\n",
            "177 0.05433365715274526\n",
            "178 0.05433802568320044\n",
            "179 0.05434239656206957\n",
            "180 0.054346769789494175\n",
            "181 0.05435114536548322\n",
            "182 0.05435552328991676\n",
            "183 0.054359903562549494\n",
            "184 0.05436428618301482\n",
            "185 0.05436867115082697\n",
            "186 0.05437305846538597\n",
            "187 0.05437744812597906\n",
            "188 0.054381840131785424\n",
            "189 0.05438623448187868\n",
            "190 0.0543906311752291\n",
            "191 0.05439503021070753\n",
            "192 0.05439943158708806\n",
            "193 0.05440383530305\n",
            "194 0.05440824135718126\n",
            "195 0.05441264974798119\n",
            "196 0.05441706047386204\n",
            "197 0.05442147353315255\n",
            "198 0.0544258889240996\n",
            "199 0.05443030664487097\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hmEiXdCdfVea",
        "colab_type": "code",
        "outputId": "a2c0fa0f-a286-45e6-98bf-f01227316b12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        }
      },
      "cell_type": "code",
      "source": [
        "plt.scatter(alphas, mses);"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe0AAAFKCAYAAAAwrQetAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "CgWTEzNpfiUV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "the error was stable at rising, from the plot above it seems the best alpha value is 1 and mse = 0.05356, meaning linear regression could be simpler and better fit for the data. Let's try to sample the initial data and run the above code again"
      ]
    },
    {
      "metadata": {
        "id": "u8Bgmen5f5Nk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "blog_sample = blog_df.sample(frac=0.01, random_state=42)\n",
        "\n",
        "X_s = blog_sample.drop(blog_sample.columns[53], axis=1)\n",
        "y_s = blog_sample[blog_sample.columns[53]]\n",
        "X_s_train, X_s_test, y_s_train, y_s_test = train_test_split(X_s, y_s, test_size = 0.1, random_state=42)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FfKVlENMgXLL",
        "colab_type": "code",
        "outputId": "1f3f0e17-c3e7-4fd8-9a5d-02bd0c0ada36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3572
        }
      },
      "cell_type": "code",
      "source": [
        "alphas_s = []\n",
        "mses_s = []\n",
        "\n",
        "for alpha in range(0, 200, 1):\n",
        "  ridge_reg_split = Ridge(alpha=alpha).fit(X_s_train, y_s_train)\n",
        "  mse = mean_squared_error(y_s_test, ridge_reg_split.predict(X_s_test))\n",
        "  print(alpha, mse)\n",
        "  alphas_s.append(alpha)\n",
        "  mses_s.append(mse)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 7.887858024697067e+26\n",
            "1 0.043664245068698336\n",
            "2 0.04644006278815532\n",
            "3 0.049265467033699696\n",
            "4 0.05194441717670451\n",
            "5 0.05444791299429211\n",
            "6 0.05678141492424\n",
            "7 0.05895949863655366\n",
            "8 0.06099845300013593\n",
            "9 0.06291378493606652\n",
            "10 0.06471942056418474\n",
            "11 0.06642755331145163\n",
            "12 0.06804874002033694\n",
            "13 0.06959208159455717\n",
            "14 0.07106541973956951\n",
            "15 0.07247552209707965\n",
            "16 0.07382824602225574\n",
            "17 0.0751286791237061\n",
            "18 0.07638125797183389\n",
            "19 0.07758986756333773\n",
            "20 0.07875792436269677\n",
            "21 0.07988844557005191\n",
            "22 0.08098410694539535\n",
            "23 0.0820472911693935\n",
            "24 0.08308012839265162\n",
            "25 0.0840845303367957\n",
            "26 0.08506221906626904\n",
            "27 0.08601475134677801\n",
            "28 0.0869435393396415\n",
            "29 0.08784986824536939\n",
            "30 0.08873491139930798\n",
            "31 0.08959974323244704\n",
            "32 0.09044535043765145\n",
            "33 0.09127264162235904\n",
            "34 0.09208245568052645\n",
            "35 0.09287556907729067\n",
            "36 0.09365270220756919\n",
            "37 0.09441452496342317\n",
            "38 0.09516166162326434\n",
            "39 0.09589469515804086\n",
            "40 0.09661417103470342\n",
            "41 0.09732060058491808\n",
            "42 0.09801446399672943\n",
            "43 0.09869621297832125\n",
            "44 0.09936627313582895\n",
            "45 0.10002504610112954\n",
            "46 0.10067291144047653\n",
            "47 0.10131022837052188\n",
            "48 0.10193733730467373\n",
            "49 0.10255456124961115\n",
            "50 0.10316220706918373\n",
            "51 0.10376056663065851\n",
            "52 0.1043499178463629\n",
            "53 0.1049305256221227\n",
            "54 0.10550264272248187\n",
            "55 0.10606651056146535\n",
            "56 0.10662235992658153\n",
            "57 0.10717041164286877\n",
            "58 0.10771087718297452\n",
            "59 0.10824395922857545\n",
            "60 0.10876985218784795\n",
            "61 0.10928874267316753\n",
            "62 0.10980080994275553\n",
            "63 0.1103062263095947\n",
            "64 0.11080515752057672\n",
            "65 0.11129776310852861\n",
            "66 0.11178419671950166\n",
            "67 0.11226460641745188\n",
            "68 0.11273913496823275\n",
            "69 0.11320792010463124\n",
            "70 0.11367109477400288\n",
            "71 0.1141287873699169\n",
            "72 0.11458112194908888\n",
            "73 0.11502821843475121\n",
            "74 0.11547019280751651\n",
            "75 0.11590715728468275\n",
            "76 0.1163392204888477\n",
            "77 0.11676648760662822\n",
            "78 0.11718906053819803\n",
            "79 0.11760703803831081\n",
            "80 0.11802051584940554\n",
            "81 0.11842958682735043\n",
            "82 0.11883434106033057\n",
            "83 0.11923486598134465\n",
            "84 0.11963124647473726\n",
            "85 0.12002356497716322\n",
            "86 0.1204119015733435\n",
            "87 0.12079633408694668\n",
            "88 0.12117693816691187\n",
            "89 0.12155378736948544\n",
            "90 0.12192695323624915\n",
            "91 0.12229650536837645\n",
            "92 0.12266251149734496\n",
            "93 0.12302503755231706\n",
            "94 0.12338414772438261\n",
            "95 0.12373990452784753\n",
            "96 0.12409236885873491\n",
            "97 0.12444160005065637\n",
            "98 0.12478765592820464\n",
            "99 0.12513059285799627\n",
            "100 0.12547046579750176\n",
            "101 0.1258073283417776\n",
            "102 0.12614123276820802\n",
            "103 0.1264722300793712\n",
            "104 0.12680037004412095\n",
            "105 0.12712570123697484\n",
            "106 0.12744827107590034\n",
            "107 0.12776812585857536\n",
            "108 0.12808531079720165\n",
            "109 0.12839987005194142\n",
            "110 0.1287118467630453\n",
            "111 0.12902128308173313\n",
            "112 0.1293282201998898\n",
            "113 0.12963269837863026\n",
            "114 0.129934756975789\n",
            "115 0.13023443447237906\n",
            "116 0.13053176849807607\n",
            "117 0.1308267958557626\n",
            "118 0.1311195525451832\n",
            "119 0.13141007378574354\n",
            "120 0.13169839403849634\n",
            "121 0.13198454702734708\n",
            "122 0.13226856575951393\n",
            "123 0.13255048254527482\n",
            "124 0.13283032901703054\n",
            "125 0.13310813614771547\n",
            "126 0.13338393426858\n",
            "127 0.1336577530863732\n",
            "128 0.13392962169995026\n",
            "129 0.13419956861632618\n",
            "130 0.13446762176620194\n",
            "131 0.13473380851898012\n",
            "132 0.13499815569729323\n",
            "133 0.1352606895910636\n",
            "134 0.13552143597111244\n",
            "135 0.13578042010233576\n",
            "136 0.13603766675646428\n",
            "137 0.13629320022442454\n",
            "138 0.13654704432831194\n",
            "139 0.13679922243299847\n",
            "140 0.13704975745737769\n",
            "141 0.13729867188527164\n",
            "142 0.13754598777600568\n",
            "143 0.13779172677466361\n",
            "144 0.1380359101220375\n",
            "145 0.13827855866428215\n",
            "146 0.13851969286228302\n",
            "147 0.13875933280074987\n",
            "148 0.1389974981970471\n",
            "149 0.13923420840976497\n",
            "150 0.1394694824470465\n",
            "151 0.1397033389746752\n",
            "152 0.13993579632393288\n",
            "153 0.14016687249923532\n",
            "154 0.14039658518555292\n",
            "155 0.1406249517556246\n",
            "156 0.14085198927697087\n",
            "157 0.14107771451871376\n",
            "158 0.14130214395820884\n",
            "159 0.14152529378749648\n",
            "160 0.1417471799195776\n",
            "161 0.14196781799452138\n",
            "162 0.1421872233854064\n",
            "163 0.14240541120410646\n",
            "164 0.14262239630691928\n",
            "165 0.1428381933000496\n",
            "166 0.14305281654494623\n",
            "167 0.1432662801635003\n",
            "168 0.1434785980431091\n",
            "169 0.14368978384160777\n",
            "170 0.14389985099207464\n",
            "171 0.14410881270751436\n",
            "172 0.14431668198541991\n",
            "173 0.1445234716122218\n",
            "174 0.14472919416762306\n",
            "175 0.14493386202882513\n",
            "176 0.14513748737465104\n",
            "177 0.14534008218956357\n",
            "178 0.14554165826758492\n",
            "179 0.14574222721612104\n",
            "180 0.14594180045969082\n",
            "181 0.14614038924356496\n",
            "182 0.14633800463731816\n",
            "183 0.1465346575382935\n",
            "184 0.14673035867498427\n",
            "185 0.14692511861033591\n",
            "186 0.14711894774496972\n",
            "187 0.1473118563203268\n",
            "188 0.1475038544217422\n",
            "189 0.1476949519814464\n",
            "190 0.14788515878149275\n",
            "191 0.148074484456621\n",
            "192 0.14826293849705433\n",
            "193 0.14845053025122865\n",
            "194 0.14863726892846313\n",
            "195 0.14882316360156692\n",
            "196 0.14900822320938886\n",
            "197 0.14919245655930716\n",
            "198 0.14937587232966476\n",
            "199 0.14955847907215025\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RxIBvLtfg6Ng",
        "colab_type": "code",
        "outputId": "3358aa50-8cb0-41c9-c455-e7d0bf3c71ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "cell_type": "code",
      "source": [
        "plt.scatter(alphas_s, mses_s);"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdkAAAFVCAYAAACjNZWhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAMDI/wOltUzvcwKHagAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "VpwVo8Gchxl0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Indeed, on a sample data, the mse decreased down to  0.04366 with alpha size=1"
      ]
    },
    {
      "metadata": {
        "id": "6sHhs1xg9pnb",
        "colab_type": "code",
        "outputId": "f3d926cf-1f7a-47b5-805c-db13a576ae08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        }
      },
      "cell_type": "code",
      "source": [
        "#Try to test again real test data\n",
        "import glob\n",
        "# load all test data from current directory\n",
        "allFiles = glob.glob(\"*test*.csv\")\n",
        "list_ = []\n",
        "\n",
        "for file_ in allFiles:\n",
        "    df = pd.read_csv(file_,index_col=None, header=None)\n",
        "    list_.append(df)\n",
        "\n",
        "blog_test = pd.concat(list_, axis = 0, ignore_index = True)\n",
        "blog_test.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>271</th>\n",
              "      <th>272</th>\n",
              "      <th>273</th>\n",
              "      <th>274</th>\n",
              "      <th>275</th>\n",
              "      <th>276</th>\n",
              "      <th>277</th>\n",
              "      <th>278</th>\n",
              "      <th>279</th>\n",
              "      <th>280</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>34.571430</td>\n",
              "      <td>50.082855</td>\n",
              "      <td>0.0</td>\n",
              "      <td>190.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>14.523809</td>\n",
              "      <td>31.859990</td>\n",
              "      <td>0.0</td>\n",
              "      <td>156.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.333333</td>\n",
              "      <td>0.992032</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.849169</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>39.644764</td>\n",
              "      <td>77.917020</td>\n",
              "      <td>0.0</td>\n",
              "      <td>798.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>16.757700</td>\n",
              "      <td>48.670017</td>\n",
              "      <td>0.0</td>\n",
              "      <td>568.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>10.630660</td>\n",
              "      <td>17.882992</td>\n",
              "      <td>1.0</td>\n",
              "      <td>259.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4.018276</td>\n",
              "      <td>10.396790</td>\n",
              "      <td>0.0</td>\n",
              "      <td>235.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 281 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         0          1    2      3    4          5          6    7      8    \\\n",
              "0  34.571430  50.082855  0.0  190.0  7.0  14.523809  31.859990  0.0  156.0   \n",
              "1   1.333333   0.992032  0.0    4.0  1.0   0.571429   0.849169  0.0    3.0   \n",
              "2  39.644764  77.917020  0.0  798.0  4.0  16.757700  48.670017  0.0  568.0   \n",
              "3  10.630660  17.882992  1.0  259.0  5.0   4.018276  10.396790  0.0  235.0   \n",
              "4   0.000000   0.000000  0.0    0.0  0.0   0.000000   0.000000  0.0    0.0   \n",
              "\n",
              "   9   ...   271  272  273  274  275  276  277  278  279  280  \n",
              "0  2.0 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  \n",
              "1  0.0 ...   1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
              "2  1.0 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
              "3  1.0 ...   1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
              "4  0.0 ...   1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
              "\n",
              "[5 rows x 281 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "xAQuGySX_3Ce",
        "colab_type": "code",
        "outputId": "f5a42417-0a48-45b3-d124-f1ea725b5df5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        }
      },
      "cell_type": "code",
      "source": [
        "#scale the test data\n",
        "\n",
        "blog_test = scale(blog_test)\n",
        "blog_test_df = pd.DataFrame(blog_test)\n",
        "blog_test_df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>271</th>\n",
              "      <th>272</th>\n",
              "      <th>273</th>\n",
              "      <th>274</th>\n",
              "      <th>275</th>\n",
              "      <th>276</th>\n",
              "      <th>277</th>\n",
              "      <th>278</th>\n",
              "      <th>279</th>\n",
              "      <th>280</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.027237</td>\n",
              "      <td>0.230339</td>\n",
              "      <td>-0.088207</td>\n",
              "      <td>-0.201772</td>\n",
              "      <td>-0.185128</td>\n",
              "      <td>0.065487</td>\n",
              "      <td>0.231314</td>\n",
              "      <td>-0.087534</td>\n",
              "      <td>-0.183664</td>\n",
              "      <td>-0.091677</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.447284</td>\n",
              "      <td>-0.431348</td>\n",
              "      <td>-0.403394</td>\n",
              "      <td>-0.300907</td>\n",
              "      <td>-0.313689</td>\n",
              "      <td>-0.115268</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.041102</td>\n",
              "      <td>-0.029653</td>\n",
              "      <td>-0.142402</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.408824</td>\n",
              "      <td>-0.677760</td>\n",
              "      <td>-0.088207</td>\n",
              "      <td>-0.678264</td>\n",
              "      <td>-0.264585</td>\n",
              "      <td>-0.395584</td>\n",
              "      <td>-0.638753</td>\n",
              "      <td>-0.087534</td>\n",
              "      <td>-0.702009</td>\n",
              "      <td>-0.162461</td>\n",
              "      <td>...</td>\n",
              "      <td>2.235716</td>\n",
              "      <td>-0.431348</td>\n",
              "      <td>-0.403394</td>\n",
              "      <td>-0.300907</td>\n",
              "      <td>-0.313689</td>\n",
              "      <td>-0.115268</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.041102</td>\n",
              "      <td>-0.029653</td>\n",
              "      <td>-0.175189</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.093795</td>\n",
              "      <td>0.745225</td>\n",
              "      <td>-0.088207</td>\n",
              "      <td>1.355796</td>\n",
              "      <td>-0.224857</td>\n",
              "      <td>0.139308</td>\n",
              "      <td>0.702952</td>\n",
              "      <td>-0.087534</td>\n",
              "      <td>1.212140</td>\n",
              "      <td>-0.127069</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.447284</td>\n",
              "      <td>-0.431348</td>\n",
              "      <td>-0.403394</td>\n",
              "      <td>-0.300907</td>\n",
              "      <td>-0.313689</td>\n",
              "      <td>-0.115268</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.041102</td>\n",
              "      <td>-0.029653</td>\n",
              "      <td>-0.175189</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.286850</td>\n",
              "      <td>-0.365305</td>\n",
              "      <td>-0.058140</td>\n",
              "      <td>-0.025008</td>\n",
              "      <td>-0.211614</td>\n",
              "      <td>-0.281679</td>\n",
              "      <td>-0.370876</td>\n",
              "      <td>-0.087534</td>\n",
              "      <td>0.083978</td>\n",
              "      <td>-0.127069</td>\n",
              "      <td>...</td>\n",
              "      <td>2.235716</td>\n",
              "      <td>-0.431348</td>\n",
              "      <td>-0.403394</td>\n",
              "      <td>-0.300907</td>\n",
              "      <td>-0.313689</td>\n",
              "      <td>-0.115268</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.041102</td>\n",
              "      <td>-0.029653</td>\n",
              "      <td>-0.175189</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.426317</td>\n",
              "      <td>-0.696111</td>\n",
              "      <td>-0.088207</td>\n",
              "      <td>-0.688512</td>\n",
              "      <td>-0.277828</td>\n",
              "      <td>-0.414467</td>\n",
              "      <td>-0.662578</td>\n",
              "      <td>-0.087534</td>\n",
              "      <td>-0.712173</td>\n",
              "      <td>-0.162461</td>\n",
              "      <td>...</td>\n",
              "      <td>2.235716</td>\n",
              "      <td>-0.431348</td>\n",
              "      <td>-0.403394</td>\n",
              "      <td>-0.300907</td>\n",
              "      <td>-0.313689</td>\n",
              "      <td>-0.115268</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.041102</td>\n",
              "      <td>-0.029653</td>\n",
              "      <td>-0.175189</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 281 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        0         1         2         3         4         5         6    \\\n",
              "0  0.027237  0.230339 -0.088207 -0.201772 -0.185128  0.065487  0.231314   \n",
              "1 -0.408824 -0.677760 -0.088207 -0.678264 -0.264585 -0.395584 -0.638753   \n",
              "2  0.093795  0.745225 -0.088207  1.355796 -0.224857  0.139308  0.702952   \n",
              "3 -0.286850 -0.365305 -0.058140 -0.025008 -0.211614 -0.281679 -0.370876   \n",
              "4 -0.426317 -0.696111 -0.088207 -0.688512 -0.277828 -0.414467 -0.662578   \n",
              "\n",
              "        7         8         9      ...          271       272       273  \\\n",
              "0 -0.087534 -0.183664 -0.091677    ...    -0.447284 -0.431348 -0.403394   \n",
              "1 -0.087534 -0.702009 -0.162461    ...     2.235716 -0.431348 -0.403394   \n",
              "2 -0.087534  1.212140 -0.127069    ...    -0.447284 -0.431348 -0.403394   \n",
              "3 -0.087534  0.083978 -0.127069    ...     2.235716 -0.431348 -0.403394   \n",
              "4 -0.087534 -0.712173 -0.162461    ...     2.235716 -0.431348 -0.403394   \n",
              "\n",
              "        274       275       276  277       278       279       280  \n",
              "0 -0.300907 -0.313689 -0.115268  0.0 -0.041102 -0.029653 -0.142402  \n",
              "1 -0.300907 -0.313689 -0.115268  0.0 -0.041102 -0.029653 -0.175189  \n",
              "2 -0.300907 -0.313689 -0.115268  0.0 -0.041102 -0.029653 -0.175189  \n",
              "3 -0.300907 -0.313689 -0.115268  0.0 -0.041102 -0.029653 -0.175189  \n",
              "4 -0.300907 -0.313689 -0.115268  0.0 -0.041102 -0.029653 -0.175189  \n",
              "\n",
              "[5 rows x 281 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "id": "7Op2idbA_pVl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_tr = blog_df.drop(blog_df.columns[53], axis=1)\n",
        "y_tr = blog_df[blog_df.columns[53]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DiZAMRLwAJt5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_te = blog_test_df.drop(blog_test_df.columns[53], axis=1)\n",
        "y_te = blog_test_df[blog_test_df.columns[53]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ob7BUdDhAYOQ",
        "colab_type": "code",
        "outputId": "7d48fd27-4619-43e1-f553-adb431901b5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "#fit the linear regression model\n",
        "lin_reg = LinearRegression().fit(X_tr, y_tr)\n",
        "\n",
        "#print the mse\n",
        "print(mean_squared_error(y_tr, lin_reg.predict(X_tr)))\n",
        "print(mean_squared_error(y_te, lin_reg.predict(X_te)))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.03915768247930148\n",
            "2.7855672648222202e+20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "j0-zXh4CAlNC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Absolutely huge difference between train and test data!"
      ]
    },
    {
      "metadata": {
        "id": "-bVqTSkTAqg_",
        "colab_type": "code",
        "outputId": "3e9c00b7-11c2-4c24-bc32-3a0d7d767581",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1047
        }
      },
      "cell_type": "code",
      "source": [
        "alphas_f = []\n",
        "mses_f = []\n",
        "\n",
        "for alpha in range(0, 200, 1):\n",
        "  ridge_reg_split = Ridge(alpha=alpha).fit(X_tr, y_tr)\n",
        "  mse = mean_squared_error(y_te, ridge_reg_split.predict(X_te))\n",
        "  print(alpha, mse)\n",
        "  alphas_f.append(alpha)\n",
        "  mses_f.append(mse)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 3.02996918502618e+22\n",
            "1 0.03907400648151621\n",
            "2 0.03912953985044786\n",
            "3 0.03910901510194724\n",
            "4 0.03907734678508156\n",
            "5 0.03904921783615147\n",
            "6 0.03902764450856592\n",
            "7 0.03901256747582994\n",
            "8 0.03900316219911488\n",
            "9 0.03899851198508664\n",
            "10 0.03899779742119227\n",
            "11 0.03900033449886222\n",
            "12 0.03900556631862824\n",
            "13 0.039013043015259034\n",
            "14 0.03902240122663439\n",
            "15 0.0390333463979522\n",
            "16 0.03904563845467249\n",
            "17 0.03905908050130752\n",
            "18 0.03907350999721845\n",
            "19 0.039088791882790797\n",
            "20 0.03910481321541297\n",
            "21 0.03912147896664416\n",
            "22 0.039138708712015295\n",
            "23 0.03915643400831845\n",
            "24 0.039174596302853346\n",
            "25 0.039193145256248306\n",
            "26 0.03921203738869994\n",
            "27 0.039231234980628395\n",
            "28 0.03925070517468082\n",
            "29 0.03927041923802171\n",
            "30 0.03929035195301895\n",
            "31 0.0393104811113003\n",
            "32 0.03933078709157199\n",
            "33 0.03935125250562882\n",
            "34 0.039371861900148276\n",
            "35 0.03939260150445629\n",
            "36 0.03941345901618897\n",
            "37 0.03943442341848965\n",
            "38 0.03945548482347814\n",
            "39 0.03947663433776486\n",
            "40 0.03949786394641582\n",
            "41 0.039519166412596\n",
            "42 0.03954053519045053\n",
            "43 0.03956196434922635\n",
            "44 0.039583448507032576\n",
            "45 0.03960498277281247\n",
            "46 0.03962656269540379\n",
            "47 0.03964818421868836\n",
            "48 0.0396698436419906\n",
            "49 0.03969153758505763\n",
            "50 0.03971326295701582\n",
            "51 0.03973501692874006\n",
            "52 0.039756796908294006\n",
            "53 0.03977860051897864\n",
            "54 0.039800425579719974\n",
            "55 0.03982227008748514\n",
            "56 0.03984413220152059\n",
            "57 0.03986601022915251\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TH9-jEk4A_tq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.scatter(alphas_f,mses_f);\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o04VehXlBjUZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "the best result on the real test data was mse =  0.03900316219911488 with alpha = 8"
      ]
    },
    {
      "metadata": {
        "id": "Onsn4B2tJ20X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Resources and stretch goals"
      ]
    },
    {
      "metadata": {
        "id": "o_ZIP6O0J435",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Resources:\n",
        "- https://www.quora.com/What-is-regularization-in-machine-learning\n",
        "- https://blogs.sas.com/content/subconsciousmusings/2017/07/06/how-to-use-regularization-to-prevent-model-overfitting/\n",
        "- https://machinelearningmastery.com/introduction-to-regularization-to-reduce-overfitting-and-improve-generalization-error/\n",
        "- https://towardsdatascience.com/ridge-and-lasso-regression-a-complete-guide-with-python-scikit-learn-e20e34bcbf0b\n",
        "- https://stats.stackexchange.com/questions/111017/question-about-standardizing-in-ridge-regression#111022\n",
        "\n",
        "Stretch goals:\n",
        "- Revisit past data you've fit OLS models to, and see if there's an `alpha` such that ridge regression results in a model with lower MSE on a train/test split\n",
        "- Yes, Ridge can be applied to classification! Check out [sklearn.linear_model.RidgeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeClassifier.html#sklearn.linear_model.RidgeClassifier), and try it on a problem you previous approached with a different classifier (note - scikit LogisticRegression also automatically penalizes based on the $L^2$ norm, so the difference won't be as dramatic)\n",
        "- Implement your own function to calculate the full cost that ridge regression is optimizing (the sum of squared residuals + `alpha` times the sum of squared coefficients) - this alone won't fit a model, but you can use it to verify cost of trained models and that the coefficients from the equivalent OLS (without regularization) may have a higher cost"
      ]
    }
  ]
}