{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "module4-ridge-regression/Ridge-Regression-Assignment.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/quinn-dougherty/DS-Unit-2-Sprint-3-Advanced-Regression/blob/master/module4_ridge_regression/Ridge_Regression_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "YP3OBbg-l0S6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##  Resources & stretch goals:\n",
        "- https://www.quora.com/What-is-regularization-in-machine-learning\n",
        "- https://blogs.sas.com/content/subconsciousmusings/2017/07/06/how-to-use-regularization-to-prevent-model-overfitting/\n",
        "- https://machinelearningmastery.com/introduction-to-regularization-to-reduce-overfitting-and-improve-generalization-error/\n",
        "- https://towardsdatascience.com/ridge-and-lasso-regression-a-complete-guide-with-python-scikit-learn-e20e34bcbf0b\n",
        "- https://stats.stackexchange.com/questions/111017/question-about-standardizing-in-ridge-regression#111022\n",
        "\n",
        "Stretch goals:\n",
        "- Revisit past data you've fit OLS models to, and see if there's an `alpha` such that ridge regression results in a model with lower MSE on a train/test split\n",
        "- Yes, Ridge can be applied to classification! Check out [sklearn.linear_model.RidgeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeClassifier.html#sklearn.linear_model.RidgeClassifier), and try it on a problem you previous approached with a different classifier (note - scikit LogisticRegression also automatically penalizes based on the $L^2$ norm, so the difference won't be as dramatic)\n",
        "- Implement your own function to calculate the full cost that ridge regression is optimizing (the sum of squared residuals + `alpha` times the sum of squared coefficients) - this alone won't fit a model, but you can use it to verify cost of trained models and that the coefficients from the equivalent OLS (without regularization) may have a higher cost"
      ]
    },
    {
      "metadata": {
        "id": "a3iwP1T0lpIC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy.testing import assert_almost_equal\n",
        "\n",
        "from sklearn import linear_model\n",
        "from sklearn.preprocessing import StandardScaler, scale\n",
        "from sklearn.linear_model import LinearRegression, Ridge, LogisticRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7AOgpyLQlp0T",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Assignment\n",
        "\n",
        "Following is data describing characteristics of blog posts, with a target feature of how many comments will be posted in the following 24 hours.\n",
        "\n",
        "https://archive.ics.uci.edu/ml/datasets/BlogFeedback\n",
        "\n",
        "Investigate - you can try both linear and ridge. You can also sample to smaller data size and see if that makes ridge more important. Don't forget to scale!\n",
        "\n",
        "Focus on the training data, but if you want to load and compare to any of the test data files you can also do that.\n",
        "\n",
        "Note - Ridge may not be that fundamentally superior in this case. That's OK! It's still good to practice both, and see if you can find parameters or sample sizes where ridge does generalize and perform better.\n",
        "\n",
        "When you've fit models to your satisfaction, answer the following question:\n",
        "\n",
        "```\n",
        "Did you find cases where Ridge performed better? If so, describe (alpha parameter, sample size, any other relevant info/processing). If not, what do you think that tells you about the data?\n",
        "```\n",
        "\n",
        "You can create whatever plots, tables, or other results support your argument. In this case, your target audience is a fellow data scientist, *not* a layperson, so feel free to dig in!"
      ]
    },
    {
      "metadata": {
        "id": "ee77qbHzl6mL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy.testing import assert_almost_equal\n",
        "\n",
        "from sklearn import linear_model\n",
        "from sklearn.preprocessing import StandardScaler, scale\n",
        "from sklearn.linear_model import LinearRegression, Ridge, LogisticRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DiHsqPxhl6ti",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "outputId": "64d87b49-b59b-4956-f211-727045ca3408"
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "Data Set Information:\n",
        "\n",
        "This data originates from blog posts. The raw HTML-documents\n",
        "of the blog posts were crawled and processed.\n",
        "The prediction task associated with the data is the prediction\n",
        "of the number of comments in the upcoming 24 hours. In order\n",
        "to simulate this situation, we choose a basetime (in the past)\n",
        "and select the blog posts that were published at most\n",
        "72 hours before the selected base date/time. Then, we calculate\n",
        "all the features of the selected blog posts from the information\n",
        "that was available at the basetime, therefore each instance\n",
        "corresponds to a blog post. The target is the number of\n",
        "comments that the blog post received in the next 24 hours\n",
        "relative to the basetime.\n",
        "\n",
        "In the train data, the basetimes were in the years\n",
        "2010 and 2011. In the test data the basetimes were\n",
        "in February and March 2012. This simulates the real-world\n",
        "situtation in which training data from the past is available\n",
        "to predict events in the future.\n",
        "\n",
        "The train data was generated from different basetimes that may\n",
        "temporally overlap. Therefore, if you simply split the train\n",
        "into disjoint partitions, the underlying time intervals may\n",
        "overlap. Therefore, the you should use the provided, temporally\n",
        "disjoint train and test splits in order to ensure that the\n",
        "evaluation is fair.\n",
        "\n",
        "Attribute Information:\n",
        "\n",
        "1...50:\n",
        "Average, standard deviation, min, max and median of the\n",
        "Attributes 51...60 for the source of the current blog post\n",
        "With source we mean the blog on which the post appeared.\n",
        "For example, myblog.blog.org would be the source of\n",
        "the post myblog.blog.org/post_2010_09_10\n",
        "51: Total number of comments before basetime\n",
        "52: Number of comments in the last 24 hours before the\n",
        "basetime\n",
        "53: Let T1 denote the datetime 48 hours before basetime,\n",
        "Let T2 denote the datetime 24 hours before basetime.\n",
        "This attribute is the number of comments in the time period\n",
        "between T1 and T2\n",
        "54: Number of comments in the first 24 hours after the\n",
        "publication of the blog post, but before basetime\n",
        "55: The difference of Attribute 52 and Attribute 53\n",
        "56...60:\n",
        "The same features as the attributes 51...55, but\n",
        "features 56...60 refer to the number of links (trackbacks),\n",
        "while features 51...55 refer to the number of comments.\n",
        "61: The length of time between the publication of the blog post\n",
        "and basetime\n",
        "62: The length of the blog post\n",
        "63...262:\n",
        "The 200 bag of words features for 200 frequent words of the\n",
        "text of the blog post\n",
        "263...269: binary indicator features (0 or 1) for the weekday\n",
        "(Monday...Sunday) of the basetime\n",
        "270...276: binary indicator features (0 or 1) for the weekday\n",
        "(Monday...Sunday) of the date of publication of the blog\n",
        "post\n",
        "277: Number of parent pages: we consider a blog post P as a\n",
        "parent of blog post B, if B is a reply (trackback) to\n",
        "blog post P.\n",
        "278...280:\n",
        "Minimum, maximum, average number of comments that the\n",
        "parents received\n",
        "281: The target: the number of comments in the next 24 hours\n",
        "(relative to basetime)'''\n",
        "\n",
        "zipurl='https://archive.ics.uci.edu/ml/machine-learning-databases/00304/BlogFeedback.zip' \n",
        "\n",
        "#!wget unzip https://archive.ics.uci.edu/ml/machine-learning-databases/00304/BlogFeedback.zip\n",
        "  \n",
        "#!ls sample_data/ \n",
        "'''anscombe.json\t\t      mnist_test.csv\n",
        "california_housing_test.csv   mnist_train_small.csv\n",
        "california_housing_train.csv  README.md'''\n",
        "\n",
        "!cat sample_data/README.md"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "anscombe.json\t\t      mnist_test.csv\n",
            "california_housing_test.csv   mnist_train_small.csv\n",
            "california_housing_train.csv  README.md\n",
            "This directory includes a few sample datasets to get you started.\n",
            "\n",
            "* `california_housing_data*.csv` is California housing data from the 1990 US\n",
            "  Census; more information is available at:\n",
            "  https://developers.google.com/machine-learning/crash-course/california-housing-data-description\n",
            "\n",
            "* `mnist_*.csv` is a small sample of the [MNIST\n",
            "  database](https://en.wikipedia.org/wiki/MNIST_database), which is described\n",
            "  at: http://yann.lecun.com/exdb/mnist/\n",
            "\n",
            "* `anscombe.json` contains a copy of [Anscombe's\n",
            " quartet](https://en.wikipedia.org/wiki/Anscombe%27s_quartet);\n",
            "  it was originally described in\n",
            "\n",
            "      Anscombe, F. J. (1973). 'Graphs in Statistical Analysis'. American Statistician. 27 (1): 17-21. JSTOR 2682899.\n",
            "\n",
            "  and our copy was prepared by the [vega_datasets library](https://github.com/altair-viz/vega_datasets/blob/4f67bdaad10f45e3549984e17e1b3088c731503d/vega_datasets/_data/anscombe.json).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nwpTIrYSl6w2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_cali_url = 'sample_data/california_housing_test.csv'\n",
        "train_cali_url = 'sample_data/california_housing_train.csv'\n",
        "\n",
        "traindf = pd.read_csv(test_cali_url)\n",
        "\n",
        "testdf = pd.read_csv(train_cali_url)\n",
        "\n",
        "assert all([x==0 for x in traindf.isna().sum().values])\n",
        "assert all([x==0 for x in testdf.isna().sum().values])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M_6zllugl65Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aH9KKpPJl60f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8itHJxPvl67V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x-S80gEtl629",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OCKwBFoUl6zM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TT47E5gNl6rq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_y7ZtKfWl6qJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}