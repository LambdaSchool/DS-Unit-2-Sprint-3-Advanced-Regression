{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Regularization Assignment.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DimaKav/DS-Unit-2-Sprint-3-Advanced-Regression/blob/master/module4-ridge-regression/Regularization_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "k0AhsAmuJzT9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Assignment\n",
        "\n",
        "Following is data describing characteristics of blog posts, with a target feature of how many comments will be posted in the following 24 hours.\n",
        "\n",
        "https://archive.ics.uci.edu/ml/datasets/BlogFeedback\n",
        "\n",
        "Investigate - you can try both linear and ridge. You can also sample to smaller data size and see if that makes ridge more important. Don't forget to scale!\n",
        "\n",
        "Focus on the training data, but if you want to load and compare to any of the test data files you can also do that.\n",
        "\n",
        "Note - Ridge may not be that fundamentally superior in this case. That's OK! It's still good to practice both, and see if you can find parameters or sample sizes where ridge does generalize and perform better.\n",
        "\n",
        "When you've fit models to your satisfaction, answer the following question:\n",
        "\n",
        "```\n",
        "Did you find cases where Ridge performed better? If so, describe (alpha parameter, sample size, any other relevant info/processing). If not, what do you think that tells you about the data?\n",
        "```\n",
        "\n",
        "You can create whatever plots, tables, or other results support your argument. In this case, your target audience is a fellow data scientist, *not* a layperson, so feel free to dig in!"
      ]
    },
    {
      "metadata": {
        "id": "4q3PRDOHJHYq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Notes:\n",
        "* Overfit model performs poorly on the testing data but well on the training data, vice versa for underfit.\n",
        "* Not being able to generalize a models fit to out of sample data causes over-fitting or under-fitting.\n",
        "* Make sure to not only look at the test accuracy but also the training accuracy is well. Compare them, if one is substantially worse than the other, be able to tell whether it's over or underfit.\n",
        "* Ridge regression is not an inferential modelling technique, scaling the data makes the interpretation of coefficients different.\n",
        "* Precision = variance.\n",
        "* Bias = accuracy.\n",
        "* Read the intuition section in the lecture notes.\n",
        "* Really high R^2 values can also indicate overfitting.\n",
        "* The way to test for overfitting is get a train accuracy and then get a test accuracy. If training accuracy is better than test accuracy, then you have problems with overfitting.\n",
        "* We want to have at least 5x as many observations as features."
      ]
    },
    {
      "metadata": {
        "id": "HKKnNsttRpwI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1258
        },
        "outputId": "3f1d336e-f385-4494-c850-68f4025580bc"
      },
      "cell_type": "code",
      "source": [
        "# TODO - write some code!\n",
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00304/BlogFeedback.zip\n",
        "!unzip BlogFeedback.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-02-28 22:01:19--  https://archive.ics.uci.edu/ml/machine-learning-databases/00304/BlogFeedback.zip\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.249\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.249|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2583605 (2.5M) [application/zip]\n",
            "Saving to: ‘BlogFeedback.zip’\n",
            "\n",
            "BlogFeedback.zip    100%[===================>]   2.46M  3.71MB/s    in 0.7s    \n",
            "\n",
            "2019-02-28 22:01:20 (3.71 MB/s) - ‘BlogFeedback.zip’ saved [2583605/2583605]\n",
            "\n",
            "Archive:  BlogFeedback.zip\n",
            "  inflating: blogData_test-2012.02.01.00_00.csv  \n",
            "  inflating: blogData_test-2012.02.02.00_00.csv  \n",
            "  inflating: blogData_test-2012.02.03.00_00.csv  \n",
            "  inflating: blogData_test-2012.02.04.00_00.csv  \n",
            "  inflating: blogData_test-2012.02.05.00_00.csv  \n",
            "  inflating: blogData_test-2012.02.06.00_00.csv  \n",
            "  inflating: blogData_test-2012.02.07.00_00.csv  \n",
            "  inflating: blogData_test-2012.02.08.00_00.csv  \n",
            "  inflating: blogData_test-2012.02.09.00_00.csv  \n",
            "  inflating: blogData_test-2012.02.10.00_00.csv  \n",
            "  inflating: blogData_test-2012.02.11.00_00.csv  \n",
            "  inflating: blogData_test-2012.02.12.00_00.csv  \n",
            "  inflating: blogData_test-2012.02.13.00_00.csv  \n",
            "  inflating: blogData_test-2012.02.14.00_00.csv  \n",
            "  inflating: blogData_test-2012.02.15.00_00.csv  \n",
            "  inflating: blogData_test-2012.02.16.00_00.csv  \n",
            "  inflating: blogData_test-2012.02.17.00_00.csv  \n",
            "  inflating: blogData_test-2012.02.18.00_00.csv  \n",
            "  inflating: blogData_test-2012.02.19.00_00.csv  \n",
            "  inflating: blogData_test-2012.02.20.00_00.csv  \n",
            "  inflating: blogData_test-2012.02.21.00_00.csv  \n",
            "  inflating: blogData_test-2012.02.22.00_00.csv  \n",
            "  inflating: blogData_test-2012.02.23.00_00.csv  \n",
            "  inflating: blogData_test-2012.02.24.00_00.csv  \n",
            "  inflating: blogData_test-2012.02.25.00_00.csv  \n",
            "  inflating: blogData_test-2012.02.26.00_00.csv  \n",
            "  inflating: blogData_test-2012.02.27.00_00.csv  \n",
            "  inflating: blogData_test-2012.02.28.00_00.csv  \n",
            "  inflating: blogData_test-2012.02.29.00_00.csv  \n",
            "  inflating: blogData_test-2012.03.01.00_00.csv  \n",
            "  inflating: blogData_test-2012.03.02.00_00.csv  \n",
            "  inflating: blogData_test-2012.03.03.00_00.csv  \n",
            "  inflating: blogData_test-2012.03.04.00_00.csv  \n",
            "  inflating: blogData_test-2012.03.05.00_00.csv  \n",
            "  inflating: blogData_test-2012.03.06.00_00.csv  \n",
            "  inflating: blogData_test-2012.03.07.00_00.csv  \n",
            "  inflating: blogData_test-2012.03.08.00_00.csv  \n",
            "  inflating: blogData_test-2012.03.09.00_00.csv  \n",
            "  inflating: blogData_test-2012.03.10.00_00.csv  \n",
            "  inflating: blogData_test-2012.03.11.00_00.csv  \n",
            "  inflating: blogData_test-2012.03.12.00_00.csv  \n",
            "  inflating: blogData_test-2012.03.13.00_00.csv  \n",
            "  inflating: blogData_test-2012.03.14.00_00.csv  \n",
            "  inflating: blogData_test-2012.03.15.00_00.csv  \n",
            "  inflating: blogData_test-2012.03.16.00_00.csv  \n",
            "  inflating: blogData_test-2012.03.17.00_00.csv  \n",
            "  inflating: blogData_test-2012.03.18.00_00.csv  \n",
            "  inflating: blogData_test-2012.03.19.00_00.csv  \n",
            "  inflating: blogData_test-2012.03.20.00_00.csv  \n",
            "  inflating: blogData_test-2012.03.21.00_00.csv  \n",
            "  inflating: blogData_test-2012.03.22.00_00.csv  \n",
            "  inflating: blogData_test-2012.03.23.00_00.csv  \n",
            "  inflating: blogData_test-2012.03.24.00_00.csv  \n",
            "  inflating: blogData_test-2012.03.25.00_00.csv  \n",
            "  inflating: blogData_test-2012.03.26.01_00.csv  \n",
            "  inflating: blogData_test-2012.03.27.01_00.csv  \n",
            "  inflating: blogData_test-2012.03.28.01_00.csv  \n",
            "  inflating: blogData_test-2012.03.29.01_00.csv  \n",
            "  inflating: blogData_test-2012.03.30.01_00.csv  \n",
            "  inflating: blogData_test-2012.03.31.01_00.csv  \n",
            "  inflating: blogData_train.csv      \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YwoxXfWDkkNV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import KFold, cross_val_score\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pWX2O6a2oSNM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "outputId": "01038c9e-3b01-445b-aa16-6940164fba5c"
      },
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('blogData_train.csv', header=None)\n",
        "df.head()"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>271</th>\n",
              "      <th>272</th>\n",
              "      <th>273</th>\n",
              "      <th>274</th>\n",
              "      <th>275</th>\n",
              "      <th>276</th>\n",
              "      <th>277</th>\n",
              "      <th>278</th>\n",
              "      <th>279</th>\n",
              "      <th>280</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>40.30467</td>\n",
              "      <td>53.845657</td>\n",
              "      <td>0.0</td>\n",
              "      <td>401.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.52416</td>\n",
              "      <td>32.44188</td>\n",
              "      <td>0.0</td>\n",
              "      <td>377.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>40.30467</td>\n",
              "      <td>53.845657</td>\n",
              "      <td>0.0</td>\n",
              "      <td>401.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.52416</td>\n",
              "      <td>32.44188</td>\n",
              "      <td>0.0</td>\n",
              "      <td>377.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>40.30467</td>\n",
              "      <td>53.845657</td>\n",
              "      <td>0.0</td>\n",
              "      <td>401.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.52416</td>\n",
              "      <td>32.44188</td>\n",
              "      <td>0.0</td>\n",
              "      <td>377.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>40.30467</td>\n",
              "      <td>53.845657</td>\n",
              "      <td>0.0</td>\n",
              "      <td>401.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.52416</td>\n",
              "      <td>32.44188</td>\n",
              "      <td>0.0</td>\n",
              "      <td>377.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>40.30467</td>\n",
              "      <td>53.845657</td>\n",
              "      <td>0.0</td>\n",
              "      <td>401.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.52416</td>\n",
              "      <td>32.44188</td>\n",
              "      <td>0.0</td>\n",
              "      <td>377.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>27.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 281 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        0          1    2      3     4         5         6    7      8    9    \\\n",
              "0  40.30467  53.845657  0.0  401.0  15.0  15.52416  32.44188  0.0  377.0  3.0   \n",
              "1  40.30467  53.845657  0.0  401.0  15.0  15.52416  32.44188  0.0  377.0  3.0   \n",
              "2  40.30467  53.845657  0.0  401.0  15.0  15.52416  32.44188  0.0  377.0  3.0   \n",
              "3  40.30467  53.845657  0.0  401.0  15.0  15.52416  32.44188  0.0  377.0  3.0   \n",
              "4  40.30467  53.845657  0.0  401.0  15.0  15.52416  32.44188  0.0  377.0  3.0   \n",
              "\n",
              "   ...   271  272  273  274  275  276  277  278  279   280  \n",
              "0  ...   0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   1.0  \n",
              "1  ...   1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  \n",
              "2  ...   1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  \n",
              "3  ...   0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   1.0  \n",
              "4  ...   0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  27.0  \n",
              "\n",
              "[5 rows x 281 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "metadata": {
        "id": "lIul2dCS9P3A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "260d09f8-b71b-4376-eefc-1541c63f5d85"
      },
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(52397, 281)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 148
        }
      ]
    },
    {
      "metadata": {
        "id": "MzKy8EMIsF7C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fcd2b1e3-6858-45e8-b379-738b09945501"
      },
      "cell_type": "code",
      "source": [
        "(df.dtypes == 'float').sum()"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "281"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "metadata": {
        "id": "cYVdqKAasZ1Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "outputId": "89fe590f-88d1-43b2-e237-5ddca6e93548"
      },
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>271</th>\n",
              "      <th>272</th>\n",
              "      <th>273</th>\n",
              "      <th>274</th>\n",
              "      <th>275</th>\n",
              "      <th>276</th>\n",
              "      <th>277</th>\n",
              "      <th>278</th>\n",
              "      <th>279</th>\n",
              "      <th>280</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>52397.000000</td>\n",
              "      <td>52397.000000</td>\n",
              "      <td>52397.000000</td>\n",
              "      <td>52397.000000</td>\n",
              "      <td>52397.000000</td>\n",
              "      <td>52397.000000</td>\n",
              "      <td>52397.000000</td>\n",
              "      <td>52397.000000</td>\n",
              "      <td>52397.000000</td>\n",
              "      <td>52397.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>52397.000000</td>\n",
              "      <td>52397.000000</td>\n",
              "      <td>52397.000000</td>\n",
              "      <td>52397.000000</td>\n",
              "      <td>52397.000000</td>\n",
              "      <td>52397.000000</td>\n",
              "      <td>52397.0</td>\n",
              "      <td>52397.000000</td>\n",
              "      <td>52397.000000</td>\n",
              "      <td>52397.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>39.444167</td>\n",
              "      <td>46.806717</td>\n",
              "      <td>0.358914</td>\n",
              "      <td>339.853102</td>\n",
              "      <td>24.681661</td>\n",
              "      <td>15.214611</td>\n",
              "      <td>27.959159</td>\n",
              "      <td>0.002748</td>\n",
              "      <td>258.666030</td>\n",
              "      <td>5.829151</td>\n",
              "      <td>...</td>\n",
              "      <td>0.171327</td>\n",
              "      <td>0.162242</td>\n",
              "      <td>0.154455</td>\n",
              "      <td>0.096151</td>\n",
              "      <td>0.088917</td>\n",
              "      <td>0.119167</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.242094</td>\n",
              "      <td>0.769505</td>\n",
              "      <td>6.764719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>79.121821</td>\n",
              "      <td>62.359996</td>\n",
              "      <td>6.840717</td>\n",
              "      <td>441.430109</td>\n",
              "      <td>69.598976</td>\n",
              "      <td>32.251189</td>\n",
              "      <td>38.584013</td>\n",
              "      <td>0.131903</td>\n",
              "      <td>321.348052</td>\n",
              "      <td>23.768317</td>\n",
              "      <td>...</td>\n",
              "      <td>0.376798</td>\n",
              "      <td>0.368676</td>\n",
              "      <td>0.361388</td>\n",
              "      <td>0.294800</td>\n",
              "      <td>0.284627</td>\n",
              "      <td>1.438194</td>\n",
              "      <td>0.0</td>\n",
              "      <td>27.497979</td>\n",
              "      <td>20.338052</td>\n",
              "      <td>37.706565</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>2.285714</td>\n",
              "      <td>5.214318</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>29.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.891566</td>\n",
              "      <td>3.075076</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>22.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>10.630660</td>\n",
              "      <td>19.353120</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>162.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.150685</td>\n",
              "      <td>11.051215</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>121.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>40.304670</td>\n",
              "      <td>77.442830</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>478.000000</td>\n",
              "      <td>15.000000</td>\n",
              "      <td>15.998589</td>\n",
              "      <td>45.701206</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>387.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1122.666600</td>\n",
              "      <td>559.432600</td>\n",
              "      <td>726.000000</td>\n",
              "      <td>2044.000000</td>\n",
              "      <td>1314.000000</td>\n",
              "      <td>442.666660</td>\n",
              "      <td>359.530060</td>\n",
              "      <td>14.000000</td>\n",
              "      <td>1424.000000</td>\n",
              "      <td>588.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>136.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1778.000000</td>\n",
              "      <td>1778.000000</td>\n",
              "      <td>1424.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 281 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                0             1             2             3             4    \\\n",
              "count  52397.000000  52397.000000  52397.000000  52397.000000  52397.000000   \n",
              "mean      39.444167     46.806717      0.358914    339.853102     24.681661   \n",
              "std       79.121821     62.359996      6.840717    441.430109     69.598976   \n",
              "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
              "25%        2.285714      5.214318      0.000000     29.000000      0.000000   \n",
              "50%       10.630660     19.353120      0.000000    162.000000      4.000000   \n",
              "75%       40.304670     77.442830      0.000000    478.000000     15.000000   \n",
              "max     1122.666600    559.432600    726.000000   2044.000000   1314.000000   \n",
              "\n",
              "                5             6             7             8             9    \\\n",
              "count  52397.000000  52397.000000  52397.000000  52397.000000  52397.000000   \n",
              "mean      15.214611     27.959159      0.002748    258.666030      5.829151   \n",
              "std       32.251189     38.584013      0.131903    321.348052     23.768317   \n",
              "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
              "25%        0.891566      3.075076      0.000000     22.000000      0.000000   \n",
              "50%        4.150685     11.051215      0.000000    121.000000      1.000000   \n",
              "75%       15.998589     45.701206      0.000000    387.000000      2.000000   \n",
              "max      442.666660    359.530060     14.000000   1424.000000    588.000000   \n",
              "\n",
              "           ...                271           272           273           274  \\\n",
              "count      ...       52397.000000  52397.000000  52397.000000  52397.000000   \n",
              "mean       ...           0.171327      0.162242      0.154455      0.096151   \n",
              "std        ...           0.376798      0.368676      0.361388      0.294800   \n",
              "min        ...           0.000000      0.000000      0.000000      0.000000   \n",
              "25%        ...           0.000000      0.000000      0.000000      0.000000   \n",
              "50%        ...           0.000000      0.000000      0.000000      0.000000   \n",
              "75%        ...           0.000000      0.000000      0.000000      0.000000   \n",
              "max        ...           1.000000      1.000000      1.000000      1.000000   \n",
              "\n",
              "                275           276      277           278           279  \\\n",
              "count  52397.000000  52397.000000  52397.0  52397.000000  52397.000000   \n",
              "mean       0.088917      0.119167      0.0      1.242094      0.769505   \n",
              "std        0.284627      1.438194      0.0     27.497979     20.338052   \n",
              "min        0.000000      0.000000      0.0      0.000000      0.000000   \n",
              "25%        0.000000      0.000000      0.0      0.000000      0.000000   \n",
              "50%        0.000000      0.000000      0.0      0.000000      0.000000   \n",
              "75%        0.000000      0.000000      0.0      0.000000      0.000000   \n",
              "max        1.000000    136.000000      0.0   1778.000000   1778.000000   \n",
              "\n",
              "                280  \n",
              "count  52397.000000  \n",
              "mean       6.764719  \n",
              "std       37.706565  \n",
              "min        0.000000  \n",
              "25%        0.000000  \n",
              "50%        0.000000  \n",
              "75%        1.000000  \n",
              "max     1424.000000  \n",
              "\n",
              "[8 rows x 281 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "metadata": {
        "id": "7jpEz8cQWXTe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f478fb37-e5e4-4964-bcaa-3440431cc60c"
      },
      "cell_type": "code",
      "source": [
        "# Base model\n",
        "X = df.drop(280, axis=1)\n",
        "y = df[280]\n",
        "lin_reg = LinearRegression().fit(X, y)\n",
        "lin_reg.score(X, y)"
      ],
      "execution_count": 215,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.36476030152844696"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 215
        }
      ]
    },
    {
      "metadata": {
        "id": "HNCaX4kfxlJc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Lets standardize\n",
        "scaler = StandardScaler()\n",
        "df_scaled = pd.DataFrame(scaler.fit_transform(df))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wQaWgP1IgG_c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "2563cc29-b855-4d91-df47-c924668d7a62"
      },
      "cell_type": "code",
      "source": [
        "# Do we have overfitting when fitting on test data\n",
        "# and generalizing on train data?\n",
        "\n",
        "X = df_scaled.drop(280, axis=1)\n",
        "y = df_scaled[280]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "lin_reg = LinearRegression().fit(X_train, y_train)\n",
        "print('Training MSE:',mean_squared_error(y, lin_reg.predict(X)))\n",
        "print('Testing MSE:',mean_squared_error(y_test, lin_reg.predict(X_test)))\n",
        "print('R^2 overall:', lin_reg.score(X_train, y_train))"
      ],
      "execution_count": 217,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training MSE: 0.6375262907318987\n",
            "Testing MSE: 0.7115411357421808\n",
            "R^2 overall: 0.37568566318299035\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "k6yaTSxfhhU1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Looks like we do have some overfitting since testing accuracy is worse than training."
      ]
    },
    {
      "metadata": {
        "id": "zx2zF-vMizMR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "529330dd-4c42-4acd-9b4f-1d098ca4bafe"
      },
      "cell_type": "code",
      "source": [
        "# Let's regularize via ridge regression\n",
        "\n",
        "ridge_reg = Ridge(alpha=50,random_state=42).fit(X_train, y_train)\n",
        "print('Training MSE:',mean_squared_error(y, ridge_reg.predict(X)))\n",
        "print('Testing MSE:',mean_squared_error(y_test, ridge_reg.predict(X_test)))\n",
        "print('R^2 overall:', ridge_reg.score(X_train, y_train))"
      ],
      "execution_count": 227,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training MSE: 0.6374939707963968\n",
            "Testing MSE: 0.7113346161590954\n",
            "R^2 overall: 0.3756594253581357\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "raTbH4lmlEoo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Not much different from linear regression and it's not solving our overfitting isuse, let's tune some hyperparams, particularly regularization strength."
      ]
    },
    {
      "metadata": {
        "id": "J0qKw5fTlLJG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "alphas = []\n",
        "mses = []\n",
        "\n",
        "for alpha in range(1, 70, 1):\n",
        "  ridge_reg = Ridge(alpha=alpha).fit(X_train, y_train)\n",
        "  mse = mean_squared_error(y_test, ridge_reg.predict(X_test))\n",
        "  alphas.append(alpha)\n",
        "  mses.append(mse)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CtMltY96qnAi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "outputId": "ff88358d-298c-4bcc-d83a-c012c75d65d2"
      },
      "cell_type": "code",
      "source": [
        "# Let's find the optimal regularization strength\n",
        "plt.scatter(alphas, mses);"
      ],
      "execution_count": 257,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA9kAAAKTCAYAAAANAMtUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCE02AAAAAAAOQpMNAAAAAICD/B9TpZCc4GMSQAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "image/png": {
              "width": 492,
              "height": 329
            }
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "EYh8HouZIbNR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Looks like 50 will do great, but as we saw above, it doesn't do much to bring the training MSE closer to testing MSE."
      ]
    },
    {
      "metadata": {
        "id": "IDZKouddqmWL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "1aeb319b-8418-4f01-9afc-f3816a991ee9"
      },
      "cell_type": "code",
      "source": [
        "# Does sampling to smaller sample size make ridge regression more important?\n",
        "df_s = df.sample(10000,random_state=42)\n",
        "df_s = pd.DataFrame(scaler.fit_transform(df_s))\n",
        "X = df_s.drop(280,axis=1)\n",
        "y = df_s[280]\n",
        "\n",
        "ridge_reg = Ridge(alpha=50, random_state=42).fit(X_train, y_train)\n",
        "\n",
        "print('Training MSE:',mean_squared_error(y, ridge_reg.predict(X)))\n",
        "print('Testing MSE:',mean_squared_error(y_test, ridge_reg.predict(X_test)))\n",
        "print('R^2 overall:', ridge_reg.score(X_train, y_train))"
      ],
      "execution_count": 254,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training MSE: 0.7061889336302851\n",
            "Testing MSE: 0.7102796750395485\n",
            "R^2 overall: 0.3752972706249691\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6S7we49ugzOm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Ridge regression seems to work better on down-sampled data. The model is not nearly as overfit as it was before."
      ]
    },
    {
      "metadata": {
        "id": "Onsn4B2tJ20X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Resources and stretch goals"
      ]
    },
    {
      "metadata": {
        "id": "o_ZIP6O0J435",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Resources:\n",
        "- https://www.quora.com/What-is-regularization-in-machine-learning\n",
        "- https://blogs.sas.com/content/subconsciousmusings/2017/07/06/how-to-use-regularization-to-prevent-model-overfitting/\n",
        "- https://machinelearningmastery.com/introduction-to-regularization-to-reduce-overfitting-and-improve-generalization-error/\n",
        "- https://towardsdatascience.com/ridge-and-lasso-regression-a-complete-guide-with-python-scikit-learn-e20e34bcbf0b\n",
        "- https://stats.stackexchange.com/questions/111017/question-about-standardizing-in-ridge-regression#111022\n",
        "\n",
        "Stretch goals:\n",
        "- Revisit past data you've fit OLS models to, and see if there's an `alpha` such that ridge regression results in a model with lower MSE on a train/test split\n",
        "- Yes, Ridge can be applied to classification! Check out [sklearn.linear_model.RidgeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeClassifier.html#sklearn.linear_model.RidgeClassifier), and try it on a problem you previous approached with a different classifier (note - scikit LogisticRegression also automatically penalizes based on the $L^2$ norm, so the difference won't be as dramatic)\n",
        "- Implement your own function to calculate the full cost that ridge regression is optimizing (the sum of squared residuals + `alpha` times the sum of squared coefficients) - this alone won't fit a model, but you can use it to verify cost of trained models and that the coefficients from the equivalent OLS (without regularization) may have a higher cost"
      ]
    }
  ]
}