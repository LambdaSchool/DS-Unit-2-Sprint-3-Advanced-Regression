{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-eFju4_DDKeX"
   },
   "source": [
    "# Lambda School Data Science - Ridge Regression\n",
    "\n",
    "Regularize your way to a better tomorrow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5v5cBm19JxOj"
   },
   "source": [
    "# Lecture\n",
    "\n",
    "Data science depends on math, and math is generally focused on situations where:\n",
    "\n",
    "1. a solution exists,\n",
    "2. the solution is unique,\n",
    "3. the solution's behavior changes continuously with the initial conditions.\n",
    "\n",
    "These are known as [well-posed problems](https://en.wikipedia.org/wiki/Well-posed_problem), and are the sorts of assumptions so core in traditional techniques that it is easy to forget about them. But they do matter, as there can be exceptions:\n",
    "\n",
    "1. no solution - e.g. no $x$ such that $Ax = b$\n",
    "2. multiple solutions - e.g. several $x_1, x_2, ...$ such that $Ax = b$\n",
    "3. \"chaotic\" systems - situations where small changes in initial conditions interact and reverberate in essentially unpredictable ways - for instance, the difficulty in longterm predictions of weather (N.B. not the same thing as longterm predictions of *climate*) - you can think of this as models that fail to generalize well, because they overfit on the training data (the initial conditions)\n",
    "\n",
    "Problems suffering from the above are called ill-posed problems. Relating to linear algebra and systems of equations, the only truly well-posed problems are those with a single unique solution.\n",
    "\n",
    "![Intersecting lines](https://upload.wikimedia.org/wikipedia/commons/c/c0/Intersecting_Lines.svg)\n",
    "\n",
    "Think for a moment - what would the above plot look like if there was no solution? If there were multiple solutions? And how would that generalize to higher dimensions?\n",
    "\n",
    "A lot of what you covered with linear regression was about getting matrices into the right shape for them to be solvable in this sense. But some matrices just won't submit to this, and other problems may technically \"fit\" linear regression but still be violating the above assumptions in subtle ways.\n",
    "\n",
    "[Overfitting](https://en.wikipedia.org/wiki/Overfitting) is in some ways a special case of this - an overfit model uses more features/parameters than is \"justified\" by the data (essentially by the *dimensionality* of the data, as measured by $n$ the number of observations). As the number of features approaches the number of observations, linear regression still \"works\", but it starts giving fairly perverse results. In particular, it results in a model that fails to *generalize* - and so the core goal of prediction and explanatory power is undermined.\n",
    "\n",
    "How is this related to well and ill-posed problems? It's not clearly a no solution or multiple solution case, but it does fall in the third category - overfitting results in fitting to the \"noise\" in the data, which means the particulars of one random sample or another (different initial conditions )will result in dramatically different models.\n",
    "\n",
    "## Stop and think - what are ways to address these issues?\n",
    "\n",
    "Let's examine in the context of housing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "colab_type": "code",
    "id": "TDh_Oz9HDHeR",
    "outputId": "f3e4d42e-57c0-432b-c369-95522bc37dd3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.419782</td>\n",
       "      <td>0.284830</td>\n",
       "      <td>-1.287909</td>\n",
       "      <td>-0.272599</td>\n",
       "      <td>-0.144217</td>\n",
       "      <td>0.413672</td>\n",
       "      <td>-0.120013</td>\n",
       "      <td>0.140214</td>\n",
       "      <td>-0.982843</td>\n",
       "      <td>-0.666608</td>\n",
       "      <td>-1.459000</td>\n",
       "      <td>0.441052</td>\n",
       "      <td>-1.075562</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.417339</td>\n",
       "      <td>-0.487722</td>\n",
       "      <td>-0.593381</td>\n",
       "      <td>-0.272599</td>\n",
       "      <td>-0.740262</td>\n",
       "      <td>0.194274</td>\n",
       "      <td>0.367166</td>\n",
       "      <td>0.557160</td>\n",
       "      <td>-0.867883</td>\n",
       "      <td>-0.987329</td>\n",
       "      <td>-0.303094</td>\n",
       "      <td>0.441052</td>\n",
       "      <td>-0.492439</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.417342</td>\n",
       "      <td>-0.487722</td>\n",
       "      <td>-0.593381</td>\n",
       "      <td>-0.272599</td>\n",
       "      <td>-0.740262</td>\n",
       "      <td>1.282714</td>\n",
       "      <td>-0.265812</td>\n",
       "      <td>0.557160</td>\n",
       "      <td>-0.867883</td>\n",
       "      <td>-0.987329</td>\n",
       "      <td>-0.303094</td>\n",
       "      <td>0.396427</td>\n",
       "      <td>-1.208727</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.416750</td>\n",
       "      <td>-0.487722</td>\n",
       "      <td>-1.306878</td>\n",
       "      <td>-0.272599</td>\n",
       "      <td>-0.835284</td>\n",
       "      <td>1.016303</td>\n",
       "      <td>-0.809889</td>\n",
       "      <td>1.077737</td>\n",
       "      <td>-0.752922</td>\n",
       "      <td>-1.106115</td>\n",
       "      <td>0.113032</td>\n",
       "      <td>0.416163</td>\n",
       "      <td>-1.361517</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.412482</td>\n",
       "      <td>-0.487722</td>\n",
       "      <td>-1.306878</td>\n",
       "      <td>-0.272599</td>\n",
       "      <td>-0.835284</td>\n",
       "      <td>1.228577</td>\n",
       "      <td>-0.511180</td>\n",
       "      <td>1.077737</td>\n",
       "      <td>-0.752922</td>\n",
       "      <td>-1.106115</td>\n",
       "      <td>0.113032</td>\n",
       "      <td>0.441052</td>\n",
       "      <td>-1.026501</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       CRIM        ZN     INDUS      CHAS       NOX        RM       AGE  \\\n",
       "0 -0.419782  0.284830 -1.287909 -0.272599 -0.144217  0.413672 -0.120013   \n",
       "1 -0.417339 -0.487722 -0.593381 -0.272599 -0.740262  0.194274  0.367166   \n",
       "2 -0.417342 -0.487722 -0.593381 -0.272599 -0.740262  1.282714 -0.265812   \n",
       "3 -0.416750 -0.487722 -1.306878 -0.272599 -0.835284  1.016303 -0.809889   \n",
       "4 -0.412482 -0.487722 -1.306878 -0.272599 -0.835284  1.228577 -0.511180   \n",
       "\n",
       "        DIS       RAD       TAX   PTRATIO         B     LSTAT  Price  \n",
       "0  0.140214 -0.982843 -0.666608 -1.459000  0.441052 -1.075562   24.0  \n",
       "1  0.557160 -0.867883 -0.987329 -0.303094  0.441052 -0.492439   21.6  \n",
       "2  0.557160 -0.867883 -0.987329 -0.303094  0.396427 -1.208727   34.7  \n",
       "3  1.077737 -0.752922 -1.106115  0.113032  0.416163 -1.361517   33.4  \n",
       "4  1.077737 -0.752922 -1.106115  0.113032  0.441052 -1.026501   36.2  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "boston = load_boston()\n",
    "boston.data = scale(boston.data)  # Very helpful for regularization!\n",
    "df = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "df['Price'] = boston.target\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "3u24Yr-SkIhb",
    "outputId": "3cc8f97f-96d0-4b08-ced9-e29d1c740a22"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 14)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "0vlZShpFkll2",
    "outputId": "aeeeee4c-8dfc-4b63-e73a-98863358bdbb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.894831181729206"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try good old least squares!\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X = df.drop('Price', axis='columns')\n",
    "y = df.Price\n",
    "\n",
    "lin_reg = LinearRegression().fit(X, y)\n",
    "mean_squared_error(y, lin_reg.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "erOFuJKWlTad"
   },
   "source": [
    "That seems like a pretty good score, but...\n",
    "\n",
    "![Kitchen Sink](https://i.imgur.com/ZZxqhT1.jpg)\n",
    "\n",
    "Chances are this doesn't generalize very well. You can verify this by splitting the data to properly test model validity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "CG6DZ1UcqbEx",
    "outputId": "04af7cd1-5847-4531-b105-32a0cf449dd7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.347018673376052\n",
      "26.273991426429014\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=43)\n",
    "lin_reg_split = LinearRegression().fit(X_train, y_train)\n",
    "print(mean_squared_error(y, lin_reg_split.predict(X)))\n",
    "print(mean_squared_error(y_test, lin_reg_split.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ILHGe53Iqehg"
   },
   "source": [
    "Oops! 💥\n",
    "\n",
    "### What can we do?\n",
    "\n",
    "- Use fewer features - sure, but it can be a lot of work to figure out *which* features, and (in cases like this) there may not be any good reason to really favor some features over another.\n",
    "- Get more data! This is actually a pretty good approach in tech, since apps generate lots of data all the time (and we made this situation by artificially constraining our data). But for case studies, existing data, etc. it won't work.\n",
    "- **Regularize!**\n",
    "\n",
    "## Regularization just means \"add bias\"\n",
    "\n",
    "OK, there's a bit more to it than that. But that's the core intuition - the problem is the model working \"too well\", so fix it by making it harder for the model!\n",
    "\n",
    "It may sound strange - a technique that is purposefully \"worse\" - but in certain situations, it can really get results.\n",
    "\n",
    "What's bias? In the context of statistics and machine learning, bias is when a predictive model fails to identify relationships between features and the output. In a word, bias is *underfitting*.\n",
    "\n",
    "We want to add bias to the model because of the [bias-variance tradeoff](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff) - variance is the sensitivity of a model to the random noise in its training data (i.e. *overfitting*), and bias and variance are naturally (inversely) related. Increasing one will always decrease the other, with regards to the overall generalization error (predictive accuracy on unseen data).\n",
    "\n",
    "Visually, the result looks like this:\n",
    "\n",
    "![Regularization example plot](https://upload.wikimedia.org/wikipedia/commons/0/02/Regularization.svg)\n",
    "\n",
    "The blue line is overfit, using more dimensions than are needed to explain the data and so much of the movement is based on noise and won't generalize well. The green line still fits the data, but is less susceptible to the noise - depending on how exactly we parameterize \"noise\" we may throw out actual correlation, but if we balance it right we keep that signal and greatly improve generalizability.\n",
    "\n",
    "### Look carefully at the above plot and think of ways you can quantify the difference between the blue and green lines...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "7aQlX9e9lQLr",
    "outputId": "d5cef801-efec-4c36-fe27-c4b3f02a6750"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.895862166800143"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now with regularization via ridge regression\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge_reg = Ridge().fit(X, y)\n",
    "mean_squared_error(y, ridge_reg.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "qiMXYAWGomcB",
    "outputId": "5a583ecf-c93f-40f2-8502-41d9e561ba32"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.192201358877668"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The score is a bit worse than OLS - but that's expected (we're adding bias)\n",
    "# Let's try split\n",
    "\n",
    "ridge_reg_split = Ridge().fit(X_train, y_train)\n",
    "mean_squared_error(y_test, ridge_reg_split.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 4674
    },
    "colab_type": "code",
    "id": "PJhjFFeF2uoA",
    "outputId": "6289580e-aed7-4839-c3e6-2c643574e2ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Ridge in module sklearn.linear_model.ridge:\n",
      "\n",
      "class Ridge(_BaseRidge, sklearn.base.RegressorMixin)\n",
      " |  Linear least squares with l2 regularization.\n",
      " |  \n",
      " |  Minimizes the objective function::\n",
      " |  \n",
      " |  ||y - Xw||^2_2 + alpha * ||w||^2_2\n",
      " |  \n",
      " |  This model solves a regression model where the loss function is\n",
      " |  the linear least squares function and regularization is given by\n",
      " |  the l2-norm. Also known as Ridge Regression or Tikhonov regularization.\n",
      " |  This estimator has built-in support for multi-variate regression\n",
      " |  (i.e., when y is a 2d-array of shape [n_samples, n_targets]).\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <ridge_regression>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  alpha : {float, array-like}, shape (n_targets)\n",
      " |      Regularization strength; must be a positive float. Regularization\n",
      " |      improves the conditioning of the problem and reduces the variance of\n",
      " |      the estimates. Larger values specify stronger regularization.\n",
      " |      Alpha corresponds to ``C^-1`` in other linear models such as\n",
      " |      LogisticRegression or LinearSVC. If an array is passed, penalties are\n",
      " |      assumed to be specific to the targets. Hence they must correspond in\n",
      " |      number.\n",
      " |  \n",
      " |  fit_intercept : boolean\n",
      " |      Whether to calculate the intercept for this model. If set\n",
      " |      to false, no intercept will be used in calculations\n",
      " |      (e.g. data is expected to be already centered).\n",
      " |  \n",
      " |  normalize : boolean, optional, default False\n",
      " |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      " |      If True, the regressors X will be normalized before regression by\n",
      " |      subtracting the mean and dividing by the l2-norm.\n",
      " |      If you wish to standardize, please use\n",
      " |      :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
      " |      on an estimator with ``normalize=False``.\n",
      " |  \n",
      " |  copy_X : boolean, optional, default True\n",
      " |      If True, X will be copied; else, it may be overwritten.\n",
      " |  \n",
      " |  max_iter : int, optional\n",
      " |      Maximum number of iterations for conjugate gradient solver.\n",
      " |      For 'sparse_cg' and 'lsqr' solvers, the default value is determined\n",
      " |      by scipy.sparse.linalg. For 'sag' solver, the default value is 1000.\n",
      " |  \n",
      " |  tol : float\n",
      " |      Precision of the solution.\n",
      " |  \n",
      " |  solver : {'auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga'}\n",
      " |      Solver to use in the computational routines:\n",
      " |  \n",
      " |      - 'auto' chooses the solver automatically based on the type of data.\n",
      " |  \n",
      " |      - 'svd' uses a Singular Value Decomposition of X to compute the Ridge\n",
      " |        coefficients. More stable for singular matrices than\n",
      " |        'cholesky'.\n",
      " |  \n",
      " |      - 'cholesky' uses the standard scipy.linalg.solve function to\n",
      " |        obtain a closed-form solution.\n",
      " |  \n",
      " |      - 'sparse_cg' uses the conjugate gradient solver as found in\n",
      " |        scipy.sparse.linalg.cg. As an iterative algorithm, this solver is\n",
      " |        more appropriate than 'cholesky' for large-scale data\n",
      " |        (possibility to set `tol` and `max_iter`).\n",
      " |  \n",
      " |      - 'lsqr' uses the dedicated regularized least-squares routine\n",
      " |        scipy.sparse.linalg.lsqr. It is the fastest and uses an iterative\n",
      " |        procedure.\n",
      " |  \n",
      " |      - 'sag' uses a Stochastic Average Gradient descent, and 'saga' uses\n",
      " |        its improved, unbiased version named SAGA. Both methods also use an\n",
      " |        iterative procedure, and are often faster than other solvers when\n",
      " |        both n_samples and n_features are large. Note that 'sag' and\n",
      " |        'saga' fast convergence is only guaranteed on features with\n",
      " |        approximately the same scale. You can preprocess the data with a\n",
      " |        scaler from sklearn.preprocessing.\n",
      " |  \n",
      " |      All last five solvers support both dense and sparse data. However,\n",
      " |      only 'sag' and 'saga' supports sparse input when `fit_intercept` is\n",
      " |      True.\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         Stochastic Average Gradient descent solver.\n",
      " |      .. versionadded:: 0.19\n",
      " |         SAGA solver.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, optional, default None\n",
      " |      The seed of the pseudo random number generator to use when shuffling\n",
      " |      the data.  If int, random_state is the seed used by the random number\n",
      " |      generator; If RandomState instance, random_state is the random number\n",
      " |      generator; If None, the random number generator is the RandomState\n",
      " |      instance used by `np.random`. Used when ``solver`` == 'sag'.\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         *random_state* to support Stochastic Average Gradient.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  coef_ : array, shape (n_features,) or (n_targets, n_features)\n",
      " |      Weight vector(s).\n",
      " |  \n",
      " |  intercept_ : float | array, shape = (n_targets,)\n",
      " |      Independent term in decision function. Set to 0.0 if\n",
      " |      ``fit_intercept = False``.\n",
      " |  \n",
      " |  n_iter_ : array or None, shape (n_targets,)\n",
      " |      Actual number of iterations for each target. Available only for\n",
      " |      sag and lsqr solvers. Other solvers will return None.\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  RidgeClassifier : Ridge classifier\n",
      " |  RidgeCV : Ridge regression with built-in cross validation\n",
      " |  :class:`sklearn.kernel_ridge.KernelRidge` : Kernel ridge regression\n",
      " |      combines ridge regression with the kernel trick\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.linear_model import Ridge\n",
      " |  >>> import numpy as np\n",
      " |  >>> n_samples, n_features = 10, 5\n",
      " |  >>> np.random.seed(0)\n",
      " |  >>> y = np.random.randn(n_samples)\n",
      " |  >>> X = np.random.randn(n_samples, n_features)\n",
      " |  >>> clf = Ridge(alpha=1.0)\n",
      " |  >>> clf.fit(X, y) # doctest: +NORMALIZE_WHITESPACE\n",
      " |  Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      " |        normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Ridge\n",
      " |      _BaseRidge\n",
      " |      abc.NewBase\n",
      " |      sklearn.linear_model.base.LinearModel\n",
      " |      abc.NewBase\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.base.RegressorMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, alpha=1.0, fit_intercept=True, normalize=False, copy_X=True, max_iter=None, tol=0.001, solver='auto', random_state=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Fit Ridge regression model\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
      " |          Training data\n",
      " |      \n",
      " |      y : array-like, shape = [n_samples] or [n_samples, n_targets]\n",
      " |          Target values\n",
      " |      \n",
      " |      sample_weight : float or numpy array of shape [n_samples]\n",
      " |          Individual weights for each sample\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : returns an instance of self.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.linear_model.base.LinearModel:\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict using the linear model\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array_like or sparse matrix, shape (n_samples, n_features)\n",
      " |          Samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      C : array, shape (n_samples,)\n",
      " |          Returns predicted values.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.RegressorMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Returns the coefficient of determination R^2 of the prediction.\n",
      " |      \n",
      " |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      " |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      " |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      " |      The best possible score is 1.0 and it can be negative (because the\n",
      " |      model can be arbitrarily worse). A constant model that always\n",
      " |      predicts the expected value of y, disregarding the input features,\n",
      " |      would get a R^2 score of 0.0.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = (n_samples, n_features)\n",
      " |          Test samples. For some estimators this may be a\n",
      " |          precomputed kernel matrix instead, shape = (n_samples,\n",
      " |          n_samples_fitted], where n_samples_fitted is the number of\n",
      " |          samples used in the fitting for the estimator.\n",
      " |      \n",
      " |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      " |          True values for X.\n",
      " |      \n",
      " |      sample_weight : array-like, shape = [n_samples], optional\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          R^2 of self.predict(X) wrt. y.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# A little better (to same test split w/OLS) - can we improve it further?\n",
    "# We just went with defaults, but as always there's plenty of parameters\n",
    "help(Ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F4eY9TKw4S4F"
   },
   "source": [
    "How to tune alpha? For now, let's loop and try values.\n",
    "\n",
    "(For longterm/stretch/next week, check out [cross-validation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html#sklearn.linear_model.RidgeCV).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3490
    },
    "colab_type": "code",
    "id": "DISx148Z4Sqi",
    "outputId": "6df5438d-e168-4714-82c4-ae4688bfdd23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 26.273991426429053\n",
      "1 26.192201358877668\n",
      "2 26.118328007697226\n",
      "3 26.051117952293595\n",
      "4 25.989569283205444\n",
      "5 25.93287356811407\n",
      "6 25.880372753122625\n",
      "7 25.831526788692837\n",
      "8 25.785889053385123\n",
      "9 25.743087513207442\n",
      "10 25.702810145277628\n",
      "11 25.66479356379559\n",
      "12 25.628814073392263\n",
      "13 25.59468057863174\n",
      "14 25.56222892458644\n",
      "15 25.53131734932431\n",
      "16 25.50182280665663\n",
      "17 25.473637974726547\n",
      "18 25.44666880864133\n",
      "19 25.420832527348388\n",
      "20 25.396055949160893\n",
      "21 25.372274108781504\n",
      "22 25.349429102822004\n",
      "23 25.327469121742823\n",
      "24 25.30634763462536\n",
      "25 25.286022699825878\n",
      "26 25.266456379775086\n",
      "27 25.24761424230921\n",
      "28 25.229464934192976\n",
      "29 25.211979815108453\n",
      "30 25.19513264248022\n",
      "31 25.178899299197408\n",
      "32 25.163257557659424\n",
      "33 25.14818687468406\n",
      "34 25.13366821272317\n",
      "35 25.11968388357409\n",
      "36 25.106217411385522\n",
      "37 25.093253412260974\n",
      "38 25.080777488180427\n",
      "39 25.068776133307583\n",
      "40 25.057236651039766\n",
      "41 25.046147080399017\n",
      "42 25.035496130566347\n",
      "43 25.02527312253186\n",
      "44 25.015467936977434\n",
      "45 25.006070967630855\n",
      "46 24.997073079433843\n",
      "47 24.98846557095439\n",
      "48 24.980240140548986\n",
      "49 24.972388855844812\n",
      "50 24.96490412616673\n",
      "51 24.95777867758141\n",
      "52 24.951005530271846\n",
      "53 24.944577977990345\n",
      "54 24.9384895693689\n",
      "55 24.9327340908919\n",
      "56 24.92730555135946\n",
      "57 24.92219816768905\n",
      "58 24.917406351921144\n",
      "59 24.912924699309215\n",
      "60 24.9087479773882\n",
      "61 24.904871115926827\n",
      "62 24.901289197679855\n",
      "63 24.897997449864803\n",
      "64 24.8949912362963\n",
      "65 24.892266050117623\n",
      "66 24.889817507075655\n",
      "67 24.88764133929069\n",
      "68 24.885733389477625\n",
      "69 24.88408960557926\n",
      "70 24.882706035776263\n",
      "71 24.88157882384208\n",
      "72 24.880704204813682\n",
      "73 24.880078500952195\n",
      "74 24.879698117969724\n",
      "75 24.879559541500758\n",
      "76 24.87965933379892\n",
      "77 24.879994130641087\n",
      "78 24.8805606384229\n",
      "79 24.881355631430836\n",
      "80 24.882375949277577\n",
      "81 24.88361849448833\n",
      "82 24.88508023022692\n",
      "83 24.886758178151386\n",
      "84 24.888649416389928\n",
      "85 24.89075107762813\n",
      "86 24.89306034730016\n",
      "87 24.895574461876226\n",
      "88 24.898290707239912\n",
      "89 24.90120641714914\n",
      "90 24.90431897177517\n",
      "91 24.907625796314402\n",
      "92 24.911124359668285\n",
      "93 24.914812173186736\n",
      "94 24.918686789471128\n",
      "95 24.92274580123304\n",
      "96 24.92698684020521\n",
      "97 24.93140757610152\n",
      "98 24.93600571562298\n",
      "99 24.94077900150688\n",
      "100 24.945725211616683\n",
      "101 24.950842158070053\n",
      "102 24.95612768640294\n",
      "103 24.96157967476758\n",
      "104 24.96719603316249\n",
      "105 24.972974702692664\n",
      "106 24.97891365485829\n",
      "107 24.985010890870456\n",
      "108 24.99126444099231\n",
      "109 24.99767236390434\n",
      "110 25.004232746092597\n",
      "111 25.010943701258537\n",
      "112 25.017803369749362\n",
      "113 25.02480991800798\n",
      "114 25.0319615380414\n",
      "115 25.03925644690685\n",
      "116 25.04669288621445\n",
      "117 25.05426912164612\n",
      "118 25.061983442489353\n",
      "119 25.069834161185728\n",
      "120 25.077819612893087\n",
      "121 25.085938155060855\n",
      "122 25.09418816701804\n",
      "123 25.102568049573183\n",
      "124 25.111076224625787\n",
      "125 25.119711134788766\n",
      "126 25.128471243021377\n",
      "127 25.137355032272303\n",
      "128 25.14636100513223\n",
      "129 25.155487683495902\n",
      "130 25.164733608232837\n",
      "131 25.174097338866744\n",
      "132 25.183577453263027\n",
      "133 25.193172547324206\n",
      "134 25.2028812346929\n",
      "135 25.212702146462046\n",
      "136 25.222633930892243\n",
      "137 25.232675253135735\n",
      "138 25.24282479496694\n",
      "139 25.25308125451928\n",
      "140 25.26344334602802\n",
      "141 25.273909799578966\n",
      "142 25.284479360862818\n",
      "143 25.29515079093497\n",
      "144 25.305922865980495\n",
      "145 25.31679437708437\n",
      "146 25.32776413000649\n",
      "147 25.338830944961526\n",
      "148 25.349993656403374\n",
      "149 25.361251112814\n",
      "150 25.37260217649681\n",
      "151 25.384045723373994\n",
      "152 25.39558064278813\n",
      "153 25.407205837307533\n",
      "154 25.418920222535693\n",
      "155 25.430722726924202\n",
      "156 25.442612291589445\n",
      "157 25.45458787013271\n",
      "158 25.466648428463827\n",
      "159 25.478792944627976\n",
      "160 25.491020408635883\n",
      "161 25.50332982229701\n",
      "162 25.515720199055906\n",
      "163 25.528190563831558\n",
      "164 25.540739952859465\n",
      "165 25.5533674135368\n",
      "166 25.56607200427009\n",
      "167 25.578852794325684\n",
      "168 25.591708863682904\n",
      "169 25.604639302889613\n",
      "170 25.617643212920317\n",
      "171 25.63071970503678\n",
      "172 25.643867900650903\n",
      "173 25.657086931189966\n",
      "174 25.670375937964163\n",
      "175 25.68373407203625\n",
      "176 25.697160494093474\n",
      "177 25.71065437432154\n",
      "178 25.724214892280617\n",
      "179 25.737841236783435\n",
      "180 25.751532605775324\n",
      "181 25.765288206216116\n",
      "182 25.779107253964067\n",
      "183 25.792988973661497\n",
      "184 25.8069325986223\n",
      "185 25.820937370721257\n",
      "186 25.83500254028498\n",
      "187 25.849127365984653\n",
      "188 25.863311114730404\n",
      "189 25.87755306156723\n",
      "190 25.891852489572667\n",
      "191 25.906208689755893\n",
      "192 25.920620960958406\n",
      "193 25.93508860975623\n",
      "194 25.949610950363557\n",
      "195 25.964187304537848\n",
      "196 25.97881700148633\n",
      "197 25.99349937777395\n",
      "198 26.008233777232597\n",
      "199 26.023019550871716\n"
     ]
    }
   ],
   "source": [
    "alphas = []\n",
    "mses = []\n",
    "\n",
    "for alpha in range(0, 200, 1):\n",
    "    ridge_reg_split = Ridge(alpha=alpha).fit(X_train, y_train)\n",
    "    mse = mean_squared_error(y_test, ridge_reg_split.predict(X_test))\n",
    "    print(alpha, mse)\n",
    "    alphas.append(alpha)\n",
    "    mses.append(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 347
    },
    "colab_type": "code",
    "id": "iRB3KHyWiO4y",
    "outputId": "a98e6ff2-c184-4fe5-eb76-a64b2705c4b3"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib.pyplot import scatter\n",
    "scatter(alphas, mses);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WzgTBd-FcctM"
   },
   "source": [
    "## What's the intuition? What are we doing?\n",
    "\n",
    "The `alpha` parameter corresponds to the weight being given to the extra penalty being calculated by [Tikhonov regularization](https://en.wikipedia.org/wiki/Tikhonov_regularization) (this parameter is sometimes referred to as $\\lambda$ in the context of ridge regression).\n",
    "\n",
    "Normal linear regression (OLS) minimizes the **sum of square error of the residuals**.\n",
    "\n",
    "Ridge regression minimizes the **sum of square error of the residuals** *AND* **the squared slope of the fit model, times the alpha parameter**.\n",
    "\n",
    "This is why the MSE for the first model in the for loop (`alpha=0`) is the same as the MSE for linear regression - it's the same model!\n",
    "\n",
    "As `alpha` is increased, we give more and more penalty to a steep slope. In two or three dimensions this is fairly easy to visualize - beyond, think of it as penalizing coefficient size. Each coefficient represents the slope of an individual dimension (feature) of the model, so ridge regression is just squaring and summing those.\n",
    "\n",
    "So while `alpha=0` reduces to OLS, as `alpha` approaches infinity eventually the penalty gets so extreme that the model will always output every coefficient as 0 (any non-zero coefficient resulting in a penalty that outweighs whatever improvement in the residuals), and just fit a flat model with intercept at the mean of the dependent variable.\n",
    "\n",
    "Of course, what we want is somewhere in-between these extremes. Intuitively, what we want to do is apply an appropriate \"cost\" or penalty to the model for fitting parameters, much like adjusted $R^2$ takes into account the cost of adding complexity to a model. What exactly is an appropriate penalty will vary, so you'll have to put on your model comparison hat and give it a go!\n",
    "\n",
    "PS - scaling the data helps, as that way this cost is consistent and can be added uniformly across features, and it is simpler to search for the `alpha` parameter.\n",
    "\n",
    "### Bonus - magic! ✨\n",
    "\n",
    "Ridge regression doesn't just reduce overfitting and help with the third aspect of well-posed problems (poor generalizability). It can also fix the first two (no unique solution)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "rdogs9EMX6Vd",
    "outputId": "eaf2492e-2a61-4e96-c2eb-a1baf6d2f4c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 14)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tiny = df.sample(10, random_state=27)\n",
    "print(df_tiny.shape)\n",
    "X = df_tiny.drop('Price', axis='columns')\n",
    "y = df_tiny.Price\n",
    "\n",
    "lin_reg = LinearRegression().fit(X, y)\n",
    "lin_reg.score(X, y)  # Perfect multi-collinearity!\n",
    "# NOTE - True OLS would 💥 here\n",
    "# scikit protects us from actual error, but still gives a poor model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "zesVR59NhA7A",
    "outputId": "83b429ca-d564-4d0b-fe0c-0b8943b6275c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9760119331942763"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_reg = Ridge().fit(X, y)\n",
    "ridge_reg.score(X, y)  # More plausible (not \"perfect\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "WP6zwLtshaVR",
    "outputId": "50f9033f-fbbc-4dcb-c96c-17e44bb3df81"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103.04429449784261"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using our earlier test split\n",
    "mean_squared_error(y_test, lin_reg.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "QeL_O8vNhSqj",
    "outputId": "a3aefe55-881f-4667-869e-c04d8c17a95e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41.79869373639458"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ridge generalizes *way* better (and we've not even tuned alpha)\n",
    "mean_squared_error(y_test, ridge_reg.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x2N5WDV6nd3S"
   },
   "source": [
    "## And a bit more math\n",
    "\n",
    "The regularization used by Ridge Regression is also known as **$L^2$ regularization**, due to the squaring of the slopes being summed. This corresponds to [$L^2$ space](https://en.wikipedia.org/wiki/Square-integrable_function), a metric space of square-integrable functions that generally measure what we intuitively think of as \"distance\" (at least, on a plane) - what is referred to as Euclidean distance.\n",
    "\n",
    "The other famous norm is $L^1$, also known as [taxicab geometry](https://en.wikipedia.org/wiki/Taxicab_geometry), because it follows the \"grid\" to measure distance like a car driving around city blocks (rather than going directly like $L^2$). When referred to as a distance this is called \"Manhattan distance\", and can be used for regularization (see [LASSO](https://en.wikipedia.org/wiki/Lasso_(statistics%29), which [uses the $L^1$ norm](https://www.quora.com/What-is-the-difference-between-L1-and-L2-regularization-How-does-it-solve-the-problem-of-overfitting-Which-regularizer-to-use-and-when)).\n",
    "\n",
    "All this comes down to - regularization means increasing model bias by \"watering down\" coefficients with a penalty typically based on some sort of distance metric, and thus reducing variance (overfitting the model to the noise in the data). It gives us another lever to try and another tool for our toolchest!\n",
    "\n",
    "## Putting it all together - one last example\n",
    "\n",
    "The official scikit-learn documentation has many excellent examples - [this one](https://scikit-learn.org/stable/auto_examples/linear_model/plot_ols_ridge_variance.html#sphx-glr-auto-examples-linear-model-plot-ols-ridge-variance-py) illustrates how ridge regression effectively reduces the variance, again by increasing the bias, penalizing coefficients to reduce the effectiveness of features (but also the impact of noise).\n",
    "\n",
    "```\n",
    "Due to the few points in each dimension and the straight line that linear regression uses to follow these points as well as it can, noise on the observations will cause great variance as shown in the first plot. Every line’s slope can vary quite a bit for each prediction due to the noise induced in the observations.\n",
    "\n",
    "Ridge regression is basically minimizing a penalised version of the least-squared function. The penalising shrinks the value of the regression coefficients. Despite the few data points in each dimension, the slope of the prediction is much more stable and the variance in the line itself is greatly reduced, in comparison to that of the standard linear regression\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "id": "LaOYdswIB6Bo",
    "outputId": "7081e218-bc17-478a-f6dd-37735fce78c3"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Code source: Gaël Varoquaux\n",
    "# Modified for documentation by Jaques Grobler\n",
    "# License: BSD 3 clause\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import linear_model\n",
    "\n",
    "X_train = np.c_[.5, 1].T\n",
    "y_train = [.5, 1]\n",
    "X_test = np.c_[0, 2].T\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "classifiers = dict(ols=linear_model.LinearRegression(),\n",
    "                   ridge=linear_model.Ridge(alpha=.1))\n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "    fig, ax = plt.subplots(figsize=(4, 3))\n",
    "\n",
    "    for _ in range(6):\n",
    "        this_X = .1 * np.random.normal(size=(2, 1)) + X_train\n",
    "        clf.fit(this_X, y_train)\n",
    "\n",
    "        ax.plot(X_test, clf.predict(X_test), color='gray')\n",
    "        ax.scatter(this_X, y_train, s=3, c='gray', marker='o', zorder=10)\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    ax.plot(X_test, clf.predict(X_test), linewidth=2, color='blue')\n",
    "    ax.scatter(X_train, y_train, s=30, c='red', marker='+', zorder=10)\n",
    "\n",
    "    ax.set_title(name)\n",
    "    ax.set_xlim(0, 2)\n",
    "    ax.set_ylim((0, 1.6))\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('y')\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k0AhsAmuJzT9"
   },
   "source": [
    "# Assignment\n",
    "\n",
    "Following is data describing characteristics of blog posts, with a target feature of how many comments will be posted in the following 24 hours.\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/BlogFeedback\n",
    "\n",
    "Investigate - you can try both linear and ridge. You can also sample to smaller data size and see if that makes ridge more important. Don't forget to scale!\n",
    "\n",
    "Focus on the training data, but if you want to load and compare to any of the test data files you can also do that.\n",
    "\n",
    "Note - Ridge may not be that fundamentally superior in this case. That's OK! It's still good to practice both, and see if you can find parameters or sample sizes where ridge does generalize and perform better.\n",
    "\n",
    "When you've fit models to your satisfaction, answer the following question:\n",
    "\n",
    "```\n",
    "Did you find cases where Ridge performed better? If so, describe (alpha parameter, sample size, any other relevant info/processing). If not, what do you think that tells you about the data?\n",
    "```\n",
    "\n",
    "You can create whatever plots, tables, or other results support your argument. In this case, your target audience is a fellow data scientist, *not* a layperson, so feel free to dig in!\n",
    "\n",
    "## Imports, Data Import, and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import glob\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "import plotly\n",
    "import plotly.plotly as py\n",
    "import cufflinks as cf\n",
    "import plotly.graph_objs as go\n",
    "plotly.tools.set_credentials_file(username='zangell', api_key='bs2CJxqOA2hlrJXKyeM9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HKKnNsttRpwI"
   },
   "outputs": [],
   "source": [
    "# !curl https://archive.ics.uci.edu/ml/machine-learning-databases/00304/BlogFeedback.zip --output BlogFeedback.zip\n",
    "# !7za x BlogFeedback.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>271</th>\n",
       "      <th>272</th>\n",
       "      <th>273</th>\n",
       "      <th>274</th>\n",
       "      <th>275</th>\n",
       "      <th>276</th>\n",
       "      <th>277</th>\n",
       "      <th>278</th>\n",
       "      <th>279</th>\n",
       "      <th>280</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40.30467</td>\n",
       "      <td>53.845657</td>\n",
       "      <td>0.0</td>\n",
       "      <td>401.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.52416</td>\n",
       "      <td>32.44188</td>\n",
       "      <td>0.0</td>\n",
       "      <td>377.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40.30467</td>\n",
       "      <td>53.845657</td>\n",
       "      <td>0.0</td>\n",
       "      <td>401.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.52416</td>\n",
       "      <td>32.44188</td>\n",
       "      <td>0.0</td>\n",
       "      <td>377.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40.30467</td>\n",
       "      <td>53.845657</td>\n",
       "      <td>0.0</td>\n",
       "      <td>401.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.52416</td>\n",
       "      <td>32.44188</td>\n",
       "      <td>0.0</td>\n",
       "      <td>377.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40.30467</td>\n",
       "      <td>53.845657</td>\n",
       "      <td>0.0</td>\n",
       "      <td>401.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.52416</td>\n",
       "      <td>32.44188</td>\n",
       "      <td>0.0</td>\n",
       "      <td>377.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40.30467</td>\n",
       "      <td>53.845657</td>\n",
       "      <td>0.0</td>\n",
       "      <td>401.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.52416</td>\n",
       "      <td>32.44188</td>\n",
       "      <td>0.0</td>\n",
       "      <td>377.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 281 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0          1    2      3     4         5         6    7      8    9    \\\n",
       "0  40.30467  53.845657  0.0  401.0  15.0  15.52416  32.44188  0.0  377.0  3.0   \n",
       "1  40.30467  53.845657  0.0  401.0  15.0  15.52416  32.44188  0.0  377.0  3.0   \n",
       "2  40.30467  53.845657  0.0  401.0  15.0  15.52416  32.44188  0.0  377.0  3.0   \n",
       "3  40.30467  53.845657  0.0  401.0  15.0  15.52416  32.44188  0.0  377.0  3.0   \n",
       "4  40.30467  53.845657  0.0  401.0  15.0  15.52416  32.44188  0.0  377.0  3.0   \n",
       "\n",
       "   ...   271  272  273  274  275  276  277  278  279   280  \n",
       "0  ...   0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   1.0  \n",
       "1  ...   1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  \n",
       "2  ...   1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  \n",
       "3  ...   0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   1.0  \n",
       "4  ...   0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  27.0  \n",
       "\n",
       "[5 rows x 281 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load training data\n",
    "blog_train = pd.read_csv('blogData_train.csv', header=None)\n",
    "blog_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>271</th>\n",
       "      <th>272</th>\n",
       "      <th>273</th>\n",
       "      <th>274</th>\n",
       "      <th>275</th>\n",
       "      <th>276</th>\n",
       "      <th>277</th>\n",
       "      <th>278</th>\n",
       "      <th>279</th>\n",
       "      <th>280</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>52397.000000</td>\n",
       "      <td>52397.000000</td>\n",
       "      <td>52397.000000</td>\n",
       "      <td>52397.000000</td>\n",
       "      <td>52397.000000</td>\n",
       "      <td>52397.000000</td>\n",
       "      <td>52397.000000</td>\n",
       "      <td>52397.000000</td>\n",
       "      <td>52397.000000</td>\n",
       "      <td>52397.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>52397.000000</td>\n",
       "      <td>52397.000000</td>\n",
       "      <td>52397.000000</td>\n",
       "      <td>52397.000000</td>\n",
       "      <td>52397.000000</td>\n",
       "      <td>52397.000000</td>\n",
       "      <td>52397.0</td>\n",
       "      <td>52397.000000</td>\n",
       "      <td>52397.000000</td>\n",
       "      <td>52397.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>39.444167</td>\n",
       "      <td>46.806717</td>\n",
       "      <td>0.358914</td>\n",
       "      <td>339.853102</td>\n",
       "      <td>24.681661</td>\n",
       "      <td>15.214611</td>\n",
       "      <td>27.959159</td>\n",
       "      <td>0.002748</td>\n",
       "      <td>258.666030</td>\n",
       "      <td>5.829151</td>\n",
       "      <td>...</td>\n",
       "      <td>0.171327</td>\n",
       "      <td>0.162242</td>\n",
       "      <td>0.154455</td>\n",
       "      <td>0.096151</td>\n",
       "      <td>0.088917</td>\n",
       "      <td>0.119167</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.242094</td>\n",
       "      <td>0.769505</td>\n",
       "      <td>6.764719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>79.121821</td>\n",
       "      <td>62.359996</td>\n",
       "      <td>6.840717</td>\n",
       "      <td>441.430109</td>\n",
       "      <td>69.598976</td>\n",
       "      <td>32.251189</td>\n",
       "      <td>38.584013</td>\n",
       "      <td>0.131903</td>\n",
       "      <td>321.348052</td>\n",
       "      <td>23.768317</td>\n",
       "      <td>...</td>\n",
       "      <td>0.376798</td>\n",
       "      <td>0.368676</td>\n",
       "      <td>0.361388</td>\n",
       "      <td>0.294800</td>\n",
       "      <td>0.284627</td>\n",
       "      <td>1.438194</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.497979</td>\n",
       "      <td>20.338052</td>\n",
       "      <td>37.706565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.285714</td>\n",
       "      <td>5.214318</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.891566</td>\n",
       "      <td>3.075076</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>10.630660</td>\n",
       "      <td>19.353120</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>162.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.150685</td>\n",
       "      <td>11.051215</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>121.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>40.304670</td>\n",
       "      <td>77.442830</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>478.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>15.998589</td>\n",
       "      <td>45.701206</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>387.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1122.666600</td>\n",
       "      <td>559.432600</td>\n",
       "      <td>726.000000</td>\n",
       "      <td>2044.000000</td>\n",
       "      <td>1314.000000</td>\n",
       "      <td>442.666660</td>\n",
       "      <td>359.530060</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>1424.000000</td>\n",
       "      <td>588.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>136.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1778.000000</td>\n",
       "      <td>1778.000000</td>\n",
       "      <td>1424.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 281 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0             1             2             3             4    \\\n",
       "count  52397.000000  52397.000000  52397.000000  52397.000000  52397.000000   \n",
       "mean      39.444167     46.806717      0.358914    339.853102     24.681661   \n",
       "std       79.121821     62.359996      6.840717    441.430109     69.598976   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        2.285714      5.214318      0.000000     29.000000      0.000000   \n",
       "50%       10.630660     19.353120      0.000000    162.000000      4.000000   \n",
       "75%       40.304670     77.442830      0.000000    478.000000     15.000000   \n",
       "max     1122.666600    559.432600    726.000000   2044.000000   1314.000000   \n",
       "\n",
       "                5             6             7             8             9    \\\n",
       "count  52397.000000  52397.000000  52397.000000  52397.000000  52397.000000   \n",
       "mean      15.214611     27.959159      0.002748    258.666030      5.829151   \n",
       "std       32.251189     38.584013      0.131903    321.348052     23.768317   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.891566      3.075076      0.000000     22.000000      0.000000   \n",
       "50%        4.150685     11.051215      0.000000    121.000000      1.000000   \n",
       "75%       15.998589     45.701206      0.000000    387.000000      2.000000   \n",
       "max      442.666660    359.530060     14.000000   1424.000000    588.000000   \n",
       "\n",
       "           ...                271           272           273           274  \\\n",
       "count      ...       52397.000000  52397.000000  52397.000000  52397.000000   \n",
       "mean       ...           0.171327      0.162242      0.154455      0.096151   \n",
       "std        ...           0.376798      0.368676      0.361388      0.294800   \n",
       "min        ...           0.000000      0.000000      0.000000      0.000000   \n",
       "25%        ...           0.000000      0.000000      0.000000      0.000000   \n",
       "50%        ...           0.000000      0.000000      0.000000      0.000000   \n",
       "75%        ...           0.000000      0.000000      0.000000      0.000000   \n",
       "max        ...           1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "                275           276      277           278           279  \\\n",
       "count  52397.000000  52397.000000  52397.0  52397.000000  52397.000000   \n",
       "mean       0.088917      0.119167      0.0      1.242094      0.769505   \n",
       "std        0.284627      1.438194      0.0     27.497979     20.338052   \n",
       "min        0.000000      0.000000      0.0      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.0      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.0      0.000000      0.000000   \n",
       "75%        0.000000      0.000000      0.0      0.000000      0.000000   \n",
       "max        1.000000    136.000000      0.0   1778.000000   1778.000000   \n",
       "\n",
       "                280  \n",
       "count  52397.000000  \n",
       "mean       6.764719  \n",
       "std       37.706565  \n",
       "min        0.000000  \n",
       "25%        0.000000  \n",
       "50%        0.000000  \n",
       "75%        1.000000  \n",
       "max     1424.000000  \n",
       "\n",
       "[8 rows x 281 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blog_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>271</th>\n",
       "      <th>272</th>\n",
       "      <th>273</th>\n",
       "      <th>274</th>\n",
       "      <th>275</th>\n",
       "      <th>276</th>\n",
       "      <th>277</th>\n",
       "      <th>278</th>\n",
       "      <th>279</th>\n",
       "      <th>280</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.056075</td>\n",
       "      <td>0.330159</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.018692</td>\n",
       "      <td>0.192442</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>47.776787</td>\n",
       "      <td>93.737470</td>\n",
       "      <td>1.0</td>\n",
       "      <td>598.0</td>\n",
       "      <td>7.5</td>\n",
       "      <td>17.857143</td>\n",
       "      <td>56.888218</td>\n",
       "      <td>0.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 281 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0          1    2      3    4          5          6    7      8    \\\n",
       "0   0.000000   0.000000  0.0    0.0  0.0   0.000000   0.000000  0.0    0.0   \n",
       "1   0.000000   0.000000  0.0    0.0  0.0   0.000000   0.000000  0.0    0.0   \n",
       "2   0.056075   0.330159  0.0    2.0  0.0   0.018692   0.192442  0.0    2.0   \n",
       "3   0.000000   0.000000  0.0    0.0  0.0   0.000000   0.000000  0.0    0.0   \n",
       "4  47.776787  93.737470  1.0  598.0  7.5  17.857143  56.888218  0.0  594.0   \n",
       "\n",
       "   9   ...   271  272  273  274  275  276  277  278  279  280  \n",
       "0  0.0 ...   0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1  0.0 ...   0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2  0.0 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3  0.0 ...   0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4  1.0 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 281 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load all test data from current directory\n",
    "allFiles = glob.glob(\"*test*.csv\")\n",
    "list_ = []\n",
    "\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=None)\n",
    "    list_.append(df)\n",
    "\n",
    "blog_test = pd.concat(list_, axis = 0, ignore_index = True)\n",
    "blog_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Setup\n",
    "\n",
    "Formalizing our X and y for train/test, and scaling the data as preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 280\n",
    "features = blog_train.columns.drop(target)\n",
    "\n",
    "X_train, y_train = blog_train[features], blog_train[target]\n",
    "X_test, y_test = blog_test[features], blog_test[target]\n",
    "\n",
    "# scale data\n",
    "std_scale = StandardScaler()\n",
    "X_train = std_scale.fit_transform(X_train)\n",
    "X_test = std_scale.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Fitting\n",
    "\n",
    "### Linear Regression\n",
    "\n",
    "First, let's fit a simple Linear Regression and record the MSE on the train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE: 903.1798715130888\n",
      "Test MSE: 647.2055110289385\n"
     ]
    }
   ],
   "source": [
    "linear_reg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "print ('Train MSE:', mse(linear_reg.predict(X_train), y_train))\n",
    "print ('Test MSE:', mse(linear_reg.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression\n",
    "\n",
    "It seems our linear regression is actually underfit, MSE is significantly lower on the testing data. Oh well, let's see how ridge regression helps/hurts.\n",
    "\n",
    "We'll start using the default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE: 903.2827764120972\n",
      "Test MSE: 646.884057145517\n"
     ]
    }
   ],
   "source": [
    "ridge_reg = Ridge().fit(X_train, y_train)\n",
    "\n",
    "print ('Train MSE:', mse(ridge_reg.predict(X_train), y_train))\n",
    "print ('Test MSE:', mse(ridge_reg.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurprisingly, for the default parameters we get a pretty similar value. Let's explore the effects of tunning alpha on the test MSE.\n",
    "\n",
    "### Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.arange(1, 20000, 50)\n",
    "train_mse_list = []\n",
    "test_mse_list = []\n",
    "\n",
    "# loop through different alpha values\n",
    "for alpha in alphas:\n",
    "    ridge_reg = Ridge(alpha=alpha).fit(X_train, y_train)\n",
    "    train_mse_list.append(mse(ridge_reg.predict(X_train), y_train))\n",
    "    test_mse_list.append(mse(ridge_reg.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zach/anaconda3/lib/python3.6/site-packages/IPython/core/display.py:689: UserWarning:\n",
      "\n",
      "Consider using IPython.display.IFrame instead\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~zangell/78.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_alphas = pd.DataFrame({'alpha' : alphas, \n",
    "                          'test_mse' : test_mse_list,\n",
    "                         'train_mse' : train_mse_list})\n",
    "\n",
    "data = [\n",
    "    go.Scatter(\n",
    "        x=df_alphas['alpha'],\n",
    "        y=df_alphas['test_mse'],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            sizemin=10,\n",
    "#             size=df['read_ratio'],\n",
    "#             colorscale='Rainbow',\n",
    "#             showscale=True,\n",
    "#             color=df_alphas['mse'],\n",
    "            line=dict(color='black', width=0.5)))\n",
    "]\n",
    "\n",
    "figure = go.Figure(\n",
    "    data=data,\n",
    "    layout=go.Layout(\n",
    "        xaxis=dict(title='Regularization Strength (Alpha)'),\n",
    "        yaxis=dict(title='Test Mean Squared Error'),\n",
    "        title='Hyperparameter Tuning - Ridge Regression'\n",
    "    ))\n",
    "py.iplot(figure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "For the blog post dataset, Ridge regression offered a moderate improvement in test model accuracy over simple Linear Regression. Because Ridge regression is able to improve our test accuracy, we would assume a Linear Regression overfits the training data.\n",
    "\n",
    "In general, our prediction was more accurate on the test data than on the training data. It is possible that the training data is noisier than the test data, which were taken a year-plus apart.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Onsn4B2tJ20X"
   },
   "source": [
    "# Resources and stretch goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o_ZIP6O0J435"
   },
   "source": [
    "Resources:\n",
    "- https://www.quora.com/What-is-regularization-in-machine-learning\n",
    "- https://blogs.sas.com/content/subconsciousmusings/2017/07/06/how-to-use-regularization-to-prevent-model-overfitting/\n",
    "- https://machinelearningmastery.com/introduction-to-regularization-to-reduce-overfitting-and-improve-generalization-error/\n",
    "- https://towardsdatascience.com/ridge-and-lasso-regression-a-complete-guide-with-python-scikit-learn-e20e34bcbf0b\n",
    "- https://stats.stackexchange.com/questions/111017/question-about-standardizing-in-ridge-regression#111022\n",
    "\n",
    "Stretch goals:\n",
    "- Revisit past data you've fit OLS models to, and see if there's an `alpha` such that ridge regression results in a model with lower MSE on a train/test split\n",
    "- Yes, Ridge can be applied to classification! Check out [sklearn.linear_model.RidgeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeClassifier.html#sklearn.linear_model.RidgeClassifier), and try it on a problem you previous approached with a different classifier (note - scikit LogisticRegression also automatically penalizes based on the $L^2$ norm, so the difference won't be as dramatic)\n",
    "- Implement your own function to calculate the full cost that ridge regression is optimizing (the sum of squared residuals + `alpha` times the sum of squared coefficients) - this alone won't fit a model, but you can use it to verify cost of trained models and that the coefficients from the equivalent OLS (without regularization) may have a higher cost"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "LS_DS_234_Ridge_Regression.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
