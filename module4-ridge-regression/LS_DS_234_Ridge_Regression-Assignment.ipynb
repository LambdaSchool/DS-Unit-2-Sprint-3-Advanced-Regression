{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-eFju4_DDKeX"
   },
   "source": [
    "# Lambda School Data Science - Ridge Regression\n",
    "\n",
    "Regularize your way to a better tomorrow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k0AhsAmuJzT9"
   },
   "source": [
    "# Assignment\n",
    "\n",
    "Following is data describing characteristics of blog posts, with a target feature of how many comments will be posted in the following 24 hours.\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/BlogFeedback\n",
    "\n",
    "Investigate - you can try both linear and ridge. You can also sample to smaller data size and see if that makes ridge more important. Don't forget to scale!\n",
    "\n",
    "Focus on the training data, but if you want to load and compare to any of the test data files you can also do that.\n",
    "\n",
    "Note - Ridge may not be that fundamentally superior in this case. That's OK! It's still good to practice both, and see if you can find parameters or sample sizes where ridge does generalize and perform better.\n",
    "\n",
    "When you've fit models to your satisfaction, answer the following question:\n",
    "\n",
    "```\n",
    "Did you find cases where Ridge performed better? If so, describe (alpha parameter, sample size, any other relevant info/processing). If not, what do you think that tells you about the data?\n",
    "```\n",
    "\n",
    "You can create whatever plots, tables, or other results support your argument. In this case, your target audience is a fellow data scientist, *not* a layperson, so feel free to dig in!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HKKnNsttRpwI"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.linear_model import Ridge\n",
    "import glob\n",
    "\n",
    "pd.set_option('display.max_columns', None)  # Unlimited columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the columns in all the datasets are numerical and clean of NANs\n",
    "train = pd.read_csv('blogData_train.csv', header=None)\n",
    "\n",
    "# I imported all the test files as separate DFs\n",
    "test_files = glob.glob('tests/*.csv')\n",
    "test_list = [pd.read_csv(file, header=None) for file in test_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing the data (X variables only, not y)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(train.drop(columns=280))\n",
    "y_train = train[280]\n",
    "\n",
    "# I put all X test matrices into a single vector, and do the same\n",
    "# with the y test vectors.\n",
    "X_test_list = []\n",
    "y_test_list = []\n",
    "\n",
    "for df in test_list:\n",
    "    scaler = StandardScaler()\n",
    "    X = df.drop(columns=280)\n",
    "    y = df[280]\n",
    "    X_test_list.append(scaler.fit_transform(X))\n",
    "    y_test_list.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Alternative scaler\n",
    "# X_train = scale(train.drop(columns=280))\n",
    "# y_train = scale(train[280])\n",
    "\n",
    "# # I put all X test matrices into a single vector, and do the same\n",
    "# # with the y test vectors.\n",
    "# X_test_list = []\n",
    "# y_test_list = []\n",
    "\n",
    "# for df in test_list:\n",
    "#     X = df.drop(columns=280)\n",
    "#     y = df[280]\n",
    "#     X_test_list.append(scale(X))\n",
    "#     y_test_list.append(scale(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # No standardization at all\n",
    "# X_train = train.drop(columns=280)\n",
    "# y_train = train[280]\n",
    "\n",
    "# # I put all X test matrices into a single vector, and do the same\n",
    "# # with the y test vectors.\n",
    "# X_test_list = []\n",
    "# y_test_list = []\n",
    "\n",
    "# for df in test_list:\n",
    "#     X = df.drop(columns=280)\n",
    "#     y = df[280]\n",
    "#     X_test_list.append(X)\n",
    "#     y_test_list.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression\n",
    "model_OLS = LinearRegression()\n",
    "model_OLS.fit(X_train, y_train)\n",
    "intercept = model_OLS.intercept_  # single number\n",
    "coefficients = model_OLS.coef_    # array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict based on the test data (create array)\n",
    "y_pred_list = [model_OLS.predict(X_test) for X_test in X_test_list]\n",
    "\n",
    "# Calculate scores from y_test and predictions\n",
    "RMSE_list = []\n",
    "R2_list = []\n",
    "for y_test, y_pred in zip(y_test_list, y_pred_list):\n",
    "    RMSE_list.append((np.sqrt(mean_squared_error(y_test, y_pred))))\n",
    "    R2_list.append(r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept   : 6.76\n"
     ]
    }
   ],
   "source": [
    "# Print report\n",
    "print(f\"Intercept   : {intercept:.2f}\")\n",
    "# print(\"\\nCoefficients:\\n\")\n",
    "# for var, coef in zip(train.columns[:-1], coefficients):\n",
    "#     print(f'{var:>12}: {coef:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE        : 1947050480846.11\n",
      "RMSE        : 665781774660.31\n",
      "RMSE        : 1504564487288.19\n",
      "RMSE        : 3299421211187.39\n",
      "RMSE        : 2409996552255.31\n",
      "RMSE        : 5314388895477.54\n",
      "RMSE        : 1436709317519.60\n",
      "RMSE        : 2824645049971.34\n",
      "RMSE        : 2272053511508.48\n",
      "RMSE        : 1153703676367.20\n",
      "RMSE        : 1822663582787.08\n",
      "RMSE        : 1360137603062.04\n",
      "RMSE        : 2608989977502.50\n",
      "RMSE        : 1167355669801.19\n",
      "RMSE        : 3191240589412.27\n",
      "RMSE        : 5335381828529.95\n",
      "RMSE        : 3655532615419.20\n",
      "RMSE        : 5138256738035.18\n",
      "RMSE        : 3286815217333.24\n",
      "RMSE        : 1812728155079.50\n",
      "RMSE        : 3527519471255.85\n",
      "RMSE        : 1371279646455.09\n",
      "RMSE        : 942076672953.02\n",
      "RMSE        : 1243828122299.68\n",
      "RMSE        : 5881795981770.34\n",
      "RMSE        : 2857071413068.64\n",
      "RMSE        : 6010937513532.05\n",
      "RMSE        : 3158344745978.20\n",
      "RMSE        : 983200085652.07\n",
      "RMSE        : 646993829117.29\n",
      "RMSE        : 1984884779246.97\n",
      "RMSE        : 1659187244098.96\n",
      "RMSE        : 3957246391623.49\n",
      "RMSE        : 1549378310375.17\n",
      "RMSE        : 4364238861831.18\n",
      "RMSE        : 1485514627800.36\n",
      "RMSE        : 1125513741784.50\n",
      "RMSE        : 1644994399804.60\n",
      "RMSE        : 1621555000762.62\n",
      "RMSE        : 2273938591517.32\n",
      "RMSE        : 1190711626993.35\n",
      "RMSE        : 1081681949295.95\n",
      "RMSE        : 1402775347823.29\n",
      "RMSE        : 3167652735795.27\n",
      "RMSE        : 1409272394405.71\n",
      "RMSE        : 1017576180835.62\n",
      "RMSE        : 2615200581765.93\n",
      "RMSE        : 2108156133184.83\n",
      "RMSE        : 1472281080149.15\n",
      "RMSE        : 1093180878396.61\n",
      "RMSE        : 1090896044981.15\n",
      "RMSE        : 1604521982952.95\n",
      "RMSE        : 3342435662220.17\n",
      "RMSE        : 3430465684078.93\n",
      "RMSE        : 2364339812589.39\n",
      "RMSE        : 4063906286208.34\n",
      "RMSE        : 3486998246547.47\n",
      "RMSE        : 1150278752996.16\n",
      "RMSE        : 950775929104.49\n",
      "RMSE        : 3013408709818.84\n",
      "\n",
      "R^2         : -18176893031153007591424.00\n",
      "R^2         : -155070602859580915712.00\n",
      "R^2         : -1416722494731635654656.00\n",
      "R^2         : -16123446150650268745728.00\n",
      "R^2         : -6287793243914970857472.00\n",
      "R^2         : -27108920511365556207616.00\n",
      "R^2         : -4040593655220479721472.00\n",
      "R^2         : -3431394892955814723584.00\n",
      "R^2         : -3331269165876600897536.00\n",
      "R^2         : -4000830483374991212544.00\n",
      "R^2         : -8839693863700444217344.00\n",
      "R^2         : -6711643810794462773248.00\n",
      "R^2         : -40998541590581309603840.00\n",
      "R^2         : -3729288381954472280064.00\n",
      "R^2         : -73788914076371285704704.00\n",
      "R^2         : -15257811990263191568384.00\n",
      "R^2         : -47378967284104036352000.00\n",
      "R^2         : -13792092267153668964352.00\n",
      "R^2         : -10608531197026145140736.00\n",
      "R^2         : -1483770994183262961664.00\n",
      "R^2         : -5354476506649953566720.00\n",
      "R^2         : -826100728470912237568.00\n",
      "R^2         : -6640744425436709126144.00\n",
      "R^2         : -9733456558118244712448.00\n",
      "R^2         : -50950883214687706546176.00\n",
      "R^2         : -12980852992139748966400.00\n",
      "R^2         : -5969037106367446325919744.00\n",
      "R^2         : -40977836927358536253440.00\n",
      "R^2         : -21621692346839211180032.00\n",
      "R^2         : -1681411921443295330304.00\n",
      "R^2         : -16051203005682771034112.00\n",
      "R^2         : -3900085515054609858560.00\n",
      "R^2         : -7765660293538584199168.00\n",
      "R^2         : -1393405107914223648768.00\n",
      "R^2         : -72764302912535115333632.00\n",
      "R^2         : -8102177482141042999296.00\n",
      "R^2         : -73889405541571301998592.00\n",
      "R^2         : -3596403442731514855424.00\n",
      "R^2         : -11170198492568564531200.00\n",
      "R^2         : -5088223696482533900288.00\n",
      "R^2         : -3471599527078010028032.00\n",
      "R^2         : -972726859823966978048.00\n",
      "R^2         : -1414074868683261345792.00\n",
      "R^2         : -8169302289691133870080.00\n",
      "R^2         : -2109463031545930186752.00\n",
      "R^2         : -601126001059612590080.00\n",
      "R^2         : -12877533663761702846464.00\n",
      "R^2         : -4575504683504942710784.00\n",
      "R^2         : -1421187537878142681088.00\n",
      "R^2         : -9139421371287335862272.00\n",
      "R^2         : -930491320186284277760.00\n",
      "R^2         : -878143991874749857792.00\n",
      "R^2         : -22646618482047671336960.00\n",
      "R^2         : -154457536497064360280064.00\n",
      "R^2         : -6939901559788035637248.00\n",
      "R^2         : -20083924771878085853184.00\n",
      "R^2         : -19750633145024752123904.00\n",
      "R^2         : -1090394041795530850304.00\n",
      "R^2         : -867316102051859267584.00\n",
      "R^2         : -7897583647830716710912.00\n"
     ]
    }
   ],
   "source": [
    "for RMSE in RMSE_list:\n",
    "    print(f'RMSE        : {RMSE:.2f}')\n",
    "print()\n",
    "for R2 in R2_list:\n",
    "    print(f'R^2         : {R2:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Regression\n",
    "Since OLS isn't yielding sensible results, let's try to fit to a different model altogether."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ridge = Ridge().fit(X_train, y_train)\n",
    "intercept = model_ridge.intercept_  # single number\n",
    "coefficients = model_ridge.coef_    # array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict based on the test data (create array)\n",
    "y_pred_list = [model_ridge.predict(X_test) for X_test in X_test_list]\n",
    "\n",
    "# Calculate scores from y_test and predictions\n",
    "RMSE_list = []\n",
    "R2_list = []\n",
    "for y_test, y_pred in zip(y_test_list, y_pred_list):\n",
    "    RMSE_list.append((np.sqrt(mean_squared_error(y_test, y_pred))))\n",
    "    R2_list.append(r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept   : 6.76\n"
     ]
    }
   ],
   "source": [
    "# Print report\n",
    "print(f\"Intercept   : {intercept:.2f}\")\n",
    "# print(\"\\nCoefficients:\\n\")\n",
    "# for var, coef in zip(train.columns[:-1], coefficients):\n",
    "#     print(f'{var:>12}: {coef:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE        : 21.70\n",
      "RMSE        : 48.92\n",
      "RMSE        : 20.69\n",
      "RMSE        : 24.69\n",
      "RMSE        : 28.89\n",
      "RMSE        : 19.75\n",
      "RMSE        : 18.75\n",
      "RMSE        : 37.89\n",
      "RMSE        : 39.99\n",
      "RMSE        : 18.62\n",
      "RMSE        : 17.26\n",
      "RMSE        : 23.93\n",
      "RMSE        : 18.80\n",
      "RMSE        : 15.59\n",
      "RMSE        : 18.67\n",
      "RMSE        : 22.43\n",
      "RMSE        : 17.74\n",
      "RMSE        : 35.10\n",
      "RMSE        : 26.68\n",
      "RMSE        : 32.12\n",
      "RMSE        : 26.82\n",
      "RMSE        : 39.60\n",
      "RMSE        : 20.13\n",
      "RMSE        : 15.52\n",
      "RMSE        : 13.67\n",
      "RMSE        : 22.60\n",
      "RMSE        : 18.27\n",
      "RMSE        : 15.77\n",
      "RMSE        : 18.52\n",
      "RMSE        : 20.18\n",
      "RMSE        : 20.57\n",
      "RMSE        : 14.87\n",
      "RMSE        : 39.23\n",
      "RMSE        : 25.80\n",
      "RMSE        : 20.10\n",
      "RMSE        : 17.27\n",
      "RMSE        : 19.50\n",
      "RMSE        : 28.09\n",
      "RMSE        : 16.29\n",
      "RMSE        : 11.67\n",
      "RMSE        : 16.16\n",
      "RMSE        : 17.96\n",
      "RMSE        : 28.96\n",
      "RMSE        : 25.65\n",
      "RMSE        : 26.34\n",
      "RMSE        : 26.97\n",
      "RMSE        : 24.65\n",
      "RMSE        : 14.53\n",
      "RMSE        : 31.48\n",
      "RMSE        : 14.21\n",
      "RMSE        : 21.56\n",
      "RMSE        : 50.20\n",
      "RMSE        : 17.06\n",
      "RMSE        : 18.80\n",
      "RMSE        : 15.15\n",
      "RMSE        : 28.25\n",
      "RMSE        : 28.73\n",
      "RMSE        : 26.31\n",
      "RMSE        : 24.80\n",
      "RMSE        : 31.73\n",
      "\n",
      "R^2         : -1.26\n",
      "R^2         : 0.16\n",
      "R^2         : 0.73\n",
      "R^2         : 0.10\n",
      "R^2         : 0.10\n",
      "R^2         : 0.63\n",
      "R^2         : 0.31\n",
      "R^2         : 0.38\n",
      "R^2         : -0.03\n",
      "R^2         : -0.04\n",
      "R^2         : 0.21\n",
      "R^2         : -1.08\n",
      "R^2         : -1.13\n",
      "R^2         : 0.33\n",
      "R^2         : -1.53\n",
      "R^2         : 0.73\n",
      "R^2         : -0.12\n",
      "R^2         : 0.36\n",
      "R^2         : 0.30\n",
      "R^2         : 0.53\n",
      "R^2         : 0.69\n",
      "R^2         : 0.31\n",
      "R^2         : -2.03\n",
      "R^2         : -0.52\n",
      "R^2         : 0.72\n",
      "R^2         : 0.19\n",
      "R^2         : -54.17\n",
      "R^2         : -0.02\n",
      "R^2         : -6.67\n",
      "R^2         : -0.64\n",
      "R^2         : -0.72\n",
      "R^2         : 0.69\n",
      "R^2         : 0.24\n",
      "R^2         : 0.61\n",
      "R^2         : -0.54\n",
      "R^2         : -0.09\n",
      "R^2         : -21.19\n",
      "R^2         : -0.05\n",
      "R^2         : -0.13\n",
      "R^2         : 0.87\n",
      "R^2         : 0.36\n",
      "R^2         : 0.73\n",
      "R^2         : 0.40\n",
      "R^2         : 0.46\n",
      "R^2         : 0.26\n",
      "R^2         : 0.58\n",
      "R^2         : -0.14\n",
      "R^2         : 0.78\n",
      "R^2         : 0.35\n",
      "R^2         : -0.54\n",
      "R^2         : 0.64\n",
      "R^2         : 0.14\n",
      "R^2         : 0.41\n",
      "R^2         : -3.64\n",
      "R^2         : 0.72\n",
      "R^2         : 0.03\n",
      "R^2         : -0.34\n",
      "R^2         : 0.43\n",
      "R^2         : 0.41\n",
      "R^2         : 0.12\n"
     ]
    }
   ],
   "source": [
    "for RMSE in RMSE_list:\n",
    "    print(f'RMSE        : {RMSE:.2f}')\n",
    "print()\n",
    "for R2 in R2_list:\n",
    "    print(f'R^2         : {R2:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What if we combine all the test data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE_OLS        : 157900068850.36\n",
      "RMSE_ridge        : 25.08\n",
      "\n",
      "R^2_OLS        : -26802055480007806976.00\n",
      "R^2_ridge         : 0.32\n"
     ]
    }
   ],
   "source": [
    "test_combined = pd.concat(test_list, axis = 0, ignore_index = True)\n",
    "scaler = StandardScaler()\n",
    "X_test_comb = scaler.fit_transform(test_combined.drop(columns=280))\n",
    "y_test_comb = test_combined[280]\n",
    "\n",
    "y_pred_comb_OLS = model_OLS.predict(X_test_comb)\n",
    "y_pred_comb_ridge = model_ridge.predict(X_test_comb)\n",
    "\n",
    "RMSE_OLS = (np.sqrt(mean_squared_error(y_test_comb, y_pred_comb_OLS)))\n",
    "RMSE_ridge = (np.sqrt(mean_squared_error(y_test_comb, y_pred_comb_ridge)))\n",
    "\n",
    "R2_OLS = r2_score(y_test_comb, y_pred_comb_OLS)\n",
    "R2_ridge = r2_score(y_test_comb, y_pred_comb_ridge)\n",
    "\n",
    "print(f'RMSE_OLS        : {RMSE_OLS:.2f}')\n",
    "print(f'RMSE_ridge        : {RMSE_ridge:.2f}')\n",
    "print()\n",
    "print(f'R^2_OLS        : {R2_OLS:.2f}')\n",
    "print(f'R^2_ridge         : {R2_ridge:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge Regression continues to work way better than OLS, which still refuses to converge into something sensible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Varying alpha\n",
    "Let's try changing up the alpha parameter.  I'll only use the combined test data, because that's simpler to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               alpha > RMSE\n",
      "        0.0000000000 > 2707889278128.54\n",
      "        0.0000000010 > 2799.89\n",
      "        0.0000004642 > 24.58\n",
      "        0.0002154435 > 24.88\n",
      "        0.1000000000 > 24.91\n",
      "       46.4158883361 > 25.37\n",
      "    21544.3469003188 > 25.52\n",
      " 10000000.0000000000 > 29.67\n",
      "4641588833.6127538681 > 30.53\n",
      "2154434690031.8779296875 > 30.53\n",
      "1000000000000000.0000000000 > 30.53\n"
     ]
    }
   ],
   "source": [
    "# X_train and y_train are all our training data\n",
    "# X_test_comb and y_test_comb are all the test data \n",
    "\n",
    "alphas = []\n",
    "RMSEs = []\n",
    "\n",
    "print('               alpha > RMSE')\n",
    "for alpha in ([0] + np.logspace(-9,15, num=10).tolist()):\n",
    "    ridge_split = Ridge(alpha=alpha).fit(X_train, y_train)\n",
    "    RMSE = np.sqrt(mean_squared_error(y_test_comb,\n",
    "                        ridge_split.predict(X_test_comb)))\n",
    "    print(f'{alpha:20.10f} > {RMSE:.2f}')\n",
    "    alphas.append(alpha)\n",
    "    RMSEs.append(RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Same numbers, plotted.\n",
    "from matplotlib.pyplot import scatter\n",
    "scatter(alphas, RMSEs);\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlim(1e-15,1e20)\n",
    "plt.ylim(1e1,1e5)\n",
    "plt.xlabel('Alpha');\n",
    "plt.ylabel('RMSE');\n",
    "plt.title('Any Ridge Regression is better than none', fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What if we use a small sample of the total data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52397, 281)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_small = train.sample(1000, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train_small = scaler.fit_transform(train_small.drop(columns=280))\n",
    "y_train_small = train_small[280]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'll train on the small dataset, but test on the full test data\n",
    "X = X_train_small\n",
    "y = y_train_small\n",
    "X_test = X_test_comb\n",
    "y_test = y_test_comb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE        : 41660500755294.83\n",
      "R^2         : 0.12\n"
     ]
    }
   ],
   "source": [
    "# Linear regression\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "RMSE = (np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "R2_ridge = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'RMSE        : {RMSE:.2f}')\n",
    "print(f'R^2         : {R2:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE        : 33.87\n",
      "R^2         : 0.12\n"
     ]
    }
   ],
   "source": [
    "# Ridge regression\n",
    "model = Ridge()\n",
    "model.fit(X, y)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "RMSE = (np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "R2_ridge = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'RMSE        : {RMSE:.2f}')\n",
    "print(f'R^2         : {R2:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               alpha > RMSE\n",
      "        0.0000000000 > 4621392003322477.00\n",
      "        0.0000000010 > 75.34\n",
      "        0.0000004642 > 35.66\n",
      "        0.0002154435 > 35.66\n",
      "        0.1000000000 > 34.20\n",
      "       46.4158883361 > 31.90\n",
      "    21544.3469003188 > 28.37\n",
      " 10000000.0000000000 > 30.50\n",
      "4641588833.6127538681 > 30.52\n",
      "2154434690031.8779296875 > 30.52\n",
      "1000000000000000.0000000000 > 30.52\n"
     ]
    }
   ],
   "source": [
    "# Same regression, varying alpha\n",
    "\n",
    "alphas = []\n",
    "RMSEs = []\n",
    "\n",
    "print('               alpha > RMSE')\n",
    "for alpha in ([0] + np.logspace(-9,15, num=10).tolist()):\n",
    "    ridge_split = Ridge(alpha=alpha).fit(X, y)\n",
    "    RMSE = np.sqrt(mean_squared_error(y_test,\n",
    "                        ridge_split.predict(X_test)))\n",
    "    print(f'{alpha:20.10f} > {RMSE:.2f}')\n",
    "    alphas.append(alpha)\n",
    "    RMSEs.append(RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Same numbers, plotted.\n",
    "from matplotlib.pyplot import scatter\n",
    "scatter(alphas, RMSEs);\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlim(1e-15,1e20)\n",
    "plt.ylim(1e1,1e5)\n",
    "plt.xlabel('Alpha');\n",
    "plt.ylabel('RMSE');\n",
    "plt.title('Any Ridge Regression is better than none', fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "Ridge regression consistently performed better than OLS for this dataset, and the differences are telling. OLS almost always produced astronomical (~10^19) RMSE values, whereas RR trained on the same data would produce small values (~10^2).  This behavior was consistent throughout several combinations of training and testing datasets. \n",
    "\n",
    "Interestingly, OLS produced sensible RMSE values for some cases where I used a very small sample of training points (N=100), but the values would be either crazy or small if I used different random seeds. The leading explanation is that the dataset contains a few outliers that either do or don't get included in the random draws with N=100, and which cause OLS to crash when present.  \n",
    "\n",
    "RR was consistently resistant to these outliers, so long as the alpha was nonzero. An alpha=0 makes RR turn into OLS, of course, but it's interesting that alpha ~ 1e-8 was enough to more or less fix the problem of crazy RMSE. The ideal value of alpha (to minimize RMSE) was somewhere around 1e-6.\n",
    "\n",
    "Other people got different behaviors, most notably different graphs of alpha versus RMSE. Mine was basically a stark step function.  These changes apparently stem from different choices of training/testing data.  This seems to be a peculiarity of this dataset; the authors themselves warn that we should use the test data provided because of issues with overlapping timelines.  I'm not sure how this generalizes to the world at large, other than reinforcing the lesson that our choice of training/testing data matters a lot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Onsn4B2tJ20X"
   },
   "source": [
    "# Resources and stretch goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o_ZIP6O0J435"
   },
   "source": [
    "Resources:\n",
    "- https://www.quora.com/What-is-regularization-in-machine-learning\n",
    "- https://blogs.sas.com/content/subconsciousmusings/2017/07/06/how-to-use-regularization-to-prevent-model-overfitting/\n",
    "- https://machinelearningmastery.com/introduction-to-regularization-to-reduce-overfitting-and-improve-generalization-error/\n",
    "- https://towardsdatascience.com/ridge-and-lasso-regression-a-complete-guide-with-python-scikit-learn-e20e34bcbf0b\n",
    "- https://stats.stackexchange.com/questions/111017/question-about-standardizing-in-ridge-regression#111022\n",
    "\n",
    "Stretch goals:\n",
    "- Revisit past data you've fit OLS models to, and see if there's an `alpha` such that ridge regression results in a model with lower MSE on a train/test split\n",
    "- Yes, Ridge can be applied to classification! Check out [sklearn.linear_model.RidgeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeClassifier.html#sklearn.linear_model.RidgeClassifier), and try it on a problem you previous approached with a different classifier (note - scikit LogisticRegression also automatically penalizes based on the $L^2$ norm, so the difference won't be as dramatic)\n",
    "- Implement your own function to calculate the full cost that ridge regression is optimizing (the sum of squared residuals + `alpha` times the sum of squared coefficients) - this alone won't fit a model, but you can use it to verify cost of trained models and that the coefficients from the equivalent OLS (without regularization) may have a higher cost"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "LS_DS_234_Ridge_Regression.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
