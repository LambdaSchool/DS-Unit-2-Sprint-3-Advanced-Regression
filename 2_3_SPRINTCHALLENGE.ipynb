{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2_3_SPRINTCHALLENGE.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/quinn-dougherty/DS-Unit-2-Sprint-3-Advanced-Regression/blob/master/2_3_SPRINTCHALLENGE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "ayDccRP01GJD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Data Science Unit 2 Sprint Challenge 3\n",
        "\n",
        "## Logistic Regression and Beyond\n",
        "\n",
        "In this sprint challenge you will fit a logistic regression modeling the probability of an adult having an income above 50K. The dataset is available at UCI:\n",
        "\n",
        "https://archive.ics.uci.edu/ml/datasets/adult\n",
        "\n",
        "Your goal is to:\n",
        "\n",
        "1. Load, validate, and clean/prepare the data.\n",
        "2. Fit a logistic regression model\n",
        "3. Answer questions based on the results (as well as a few extra questions about the other modules)\n",
        "\n",
        "Don't let the perfect be the enemy of the good! Manage your time, and make sure to get to all parts. If you get stuck wrestling with the data, simplify it (if necessary, drop features or rows) so you're able to move on. If you have time at the end, you can go back and try to fix/improve.\n",
        "\n",
        "### Hints\n",
        "\n",
        "It has a variety of features - some are continuous, but many are categorical. You may find [pandas.get_dummies](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html) (a method to one-hot encode) helpful!\n",
        "\n",
        "The features have dramatically different ranges. You may find [sklearn.preprocessing.minmax_scale](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.minmax_scale.html#sklearn.preprocessing.minmax_scale) helpful!"
      ]
    },
    {
      "metadata": {
        "id": "U22R1Ud51hxb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Part 1 - Load, validate, and prepare data\n",
        "\n",
        "The data is available at: https://archive.ics.uci.edu/ml/datasets/adult\n",
        "\n",
        "Load it, name the columns, and make sure that you've loaded the data successfully. Note that missing values for categorical variables can essentially be considered another category (\"unknown\"), and may not need to be dropped.\n",
        "\n",
        "You should also prepare the data for logistic regression - one-hot encode categorical features as appropriate."
      ]
    },
    {
      "metadata": {
        "id": "SeOByIkht-NS",
        "colab_type": "code",
        "outputId": "03ec5fcb-a151-4b5b-fc1b-a943586ff43d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        }
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        ">50K, <=50K.\n",
        "\n",
        "age: continuous.\n",
        "workclass: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.\n",
        "fnlwgt: continuous.\n",
        "education: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.\n",
        "education-num: continuous.\n",
        "marital-status: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.\n",
        "occupation: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.\n",
        "relationship: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.\n",
        "race: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black.\n",
        "sex: Female, Male.\n",
        "capital-gain: continuous.\n",
        "capital-loss: continuous.\n",
        "hours-per-week: continuous.\n",
        "native-country: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands.\n",
        "'''\n",
        "FEATURE_names = ['age', 'workclass', 'fnlwgt','education', 'education_num', \n",
        "                  'marital_status', 'occupation', 'relationship', 'race', 'sex', \n",
        "                  'capital_gain', 'capital_loss', 'hours_per_week', \n",
        "                  'native_country', 'TARGET']\n",
        "\n",
        "dependent = FEATURE_names[-1]\n",
        "\n",
        "train_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data'\n",
        "test_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test'\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, scale\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from numpy.testing import assert_almost_equal\n",
        "from heapq import nlargest, nsmallest\n",
        "from functools import reduce\n",
        "\n",
        "df_ = pd.read_csv(train_url, header=None, names=FEATURE_names, skipinitialspace=True)\n",
        "\n",
        "assert all([x==0 for x in df_.isna().sum().values])\n",
        "\n",
        "side_hustle = {'?': 'Side_hustle'}\n",
        "df_.workclass = df_.workclass.replace(side_hustle)\n",
        "\n",
        "# edu_replace = {'Preschool': 0, '1st-4th': 1, '5th-6th': 2, '7th-8th': 3, \n",
        "#                '9th': 4, '10th': 5, '11th': 6, '12th': 7, 'HS-grad': 8, \n",
        "#                'Some-college': 9, 'Assoc-voc': 10, 'Assoc-acdm': 10, \n",
        "#                'Prof-school': 11, 'Bachelors': 13, 'Masters': 14, \n",
        "#                'Doctorate': 15} # THIS WAS A WASTE OF TIME because education_num does this for me.\n",
        "# assert len(edu_replace.keys())==len(df_.education.value_counts().index)\n",
        "\n",
        "df_.occupation = df_.occupation.replace(side_hustle)\n",
        "\n",
        "df_.native_country = df_.native_country.replace({'?': 'Unknown'})#.apply(lambda s: s + '_ORIGIN')\n",
        "\n",
        "# for capital_gain and capital_loss, i mostly care about the difference between 0 and 0+. \n",
        "# i'll pronounce this. \n",
        "df_['log_capital_gain'] = np.log(df_.capital_gain + 1)\n",
        "df_['log_capital_loss'] = np.log(df_.capital_loss + 1)\n",
        "\n",
        "df_[dependent] = df_[dependent].replace({'<=50K': 0, '>50K': 1})\n",
        "\n",
        "to_onehot = ['workclass', 'marital_status', 'occupation', 'relationship', \n",
        "             'race', 'sex', 'native_country']\n",
        "# drop after onehotting. \n",
        "\n",
        "to_drop = ['education', 'capital_gain', 'capital_loss'] + to_onehot\n",
        "\n",
        "df_.head()"
      ],
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>workclass</th>\n",
              "      <th>fnlwgt</th>\n",
              "      <th>education</th>\n",
              "      <th>education_num</th>\n",
              "      <th>marital_status</th>\n",
              "      <th>occupation</th>\n",
              "      <th>relationship</th>\n",
              "      <th>race</th>\n",
              "      <th>sex</th>\n",
              "      <th>capital_gain</th>\n",
              "      <th>capital_loss</th>\n",
              "      <th>hours_per_week</th>\n",
              "      <th>native_country</th>\n",
              "      <th>TARGET</th>\n",
              "      <th>log_capital_gain</th>\n",
              "      <th>log_capital_loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>39</td>\n",
              "      <td>State-gov</td>\n",
              "      <td>77516</td>\n",
              "      <td>Bachelors</td>\n",
              "      <td>13</td>\n",
              "      <td>Never-married</td>\n",
              "      <td>Adm-clerical</td>\n",
              "      <td>Not-in-family</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>2174</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>United-States</td>\n",
              "      <td>0</td>\n",
              "      <td>7.684784</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>50</td>\n",
              "      <td>Self-emp-not-inc</td>\n",
              "      <td>83311</td>\n",
              "      <td>Bachelors</td>\n",
              "      <td>13</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Exec-managerial</td>\n",
              "      <td>Husband</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>13</td>\n",
              "      <td>United-States</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>38</td>\n",
              "      <td>Private</td>\n",
              "      <td>215646</td>\n",
              "      <td>HS-grad</td>\n",
              "      <td>9</td>\n",
              "      <td>Divorced</td>\n",
              "      <td>Handlers-cleaners</td>\n",
              "      <td>Not-in-family</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>United-States</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>53</td>\n",
              "      <td>Private</td>\n",
              "      <td>234721</td>\n",
              "      <td>11th</td>\n",
              "      <td>7</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Handlers-cleaners</td>\n",
              "      <td>Husband</td>\n",
              "      <td>Black</td>\n",
              "      <td>Male</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>United-States</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>28</td>\n",
              "      <td>Private</td>\n",
              "      <td>338409</td>\n",
              "      <td>Bachelors</td>\n",
              "      <td>13</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Prof-specialty</td>\n",
              "      <td>Wife</td>\n",
              "      <td>Black</td>\n",
              "      <td>Female</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>Cuba</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   age         workclass  fnlwgt  education  education_num  \\\n",
              "0   39         State-gov   77516  Bachelors             13   \n",
              "1   50  Self-emp-not-inc   83311  Bachelors             13   \n",
              "2   38           Private  215646    HS-grad              9   \n",
              "3   53           Private  234721       11th              7   \n",
              "4   28           Private  338409  Bachelors             13   \n",
              "\n",
              "       marital_status         occupation   relationship   race     sex  \\\n",
              "0       Never-married       Adm-clerical  Not-in-family  White    Male   \n",
              "1  Married-civ-spouse    Exec-managerial        Husband  White    Male   \n",
              "2            Divorced  Handlers-cleaners  Not-in-family  White    Male   \n",
              "3  Married-civ-spouse  Handlers-cleaners        Husband  Black    Male   \n",
              "4  Married-civ-spouse     Prof-specialty           Wife  Black  Female   \n",
              "\n",
              "   capital_gain  capital_loss  hours_per_week native_country  TARGET  \\\n",
              "0          2174             0              40  United-States       0   \n",
              "1             0             0              13  United-States       0   \n",
              "2             0             0              40  United-States       0   \n",
              "3             0             0              40  United-States       0   \n",
              "4             0             0              40           Cuba       0   \n",
              "\n",
              "   log_capital_gain  log_capital_loss  \n",
              "0          7.684784               0.0  \n",
              "1          0.000000               0.0  \n",
              "2          0.000000               0.0  \n",
              "3          0.000000               0.0  \n",
              "4          0.000000               0.0  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 205
        }
      ]
    },
    {
      "metadata": {
        "id": "H3-CvMfrzWtB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_test_ = pd.read_csv(test_url, header=None, names=FEATURE_names, skiprows=1, skipinitialspace=True)\n",
        "\n",
        "#df_test_.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Gu9CWabP0M75",
        "colab_type": "code",
        "outputId": "dded1fa8-1cb2-4ebf-d73d-196fe243460f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 325
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "to_standardize = df_.drop(to_drop+['log_capital_'+x for x in ['gain', 'loss']]+[dependent], axis=1).columns\n",
        "\n",
        "df = pd.concat([df_.drop(to_drop, axis=1), pd.get_dummies(df_[to_onehot])], axis=1, join='outer')\n",
        "\n",
        "df[to_standardize] = StandardScaler().fit_transform(df[to_standardize])\n",
        "# we don't want to standardize log capital gain/loss! \n",
        "\n",
        "f = lambda s: s.replace('-', '_')\n",
        "df.columns = [f(x) for x in df.columns]\n",
        "\n",
        "assert df.shape[0]==df_.shape[0]\n",
        "#assert df_.shape[1]+pd.get_dummies(df_[to_onehot]).shape[1]==df.shape[1] # There's an extra `+3` here because of the dropped columns\n",
        "#df.shape[1], df_.shape[1]+pd.get_dummies(df_[to_onehot]).shape[1]\n",
        "\n",
        "df.head()"
      ],
      "execution_count": 207,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
            "  return self.partial_fit(X, y)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
            "  return self.fit(X, **fit_params).transform(X)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>fnlwgt</th>\n",
              "      <th>education_num</th>\n",
              "      <th>hours_per_week</th>\n",
              "      <th>TARGET</th>\n",
              "      <th>log_capital_gain</th>\n",
              "      <th>log_capital_loss</th>\n",
              "      <th>workclass_Federal_gov</th>\n",
              "      <th>workclass_Local_gov</th>\n",
              "      <th>workclass_Never_worked</th>\n",
              "      <th>...</th>\n",
              "      <th>native_country_Puerto_Rico</th>\n",
              "      <th>native_country_Scotland</th>\n",
              "      <th>native_country_South</th>\n",
              "      <th>native_country_Taiwan</th>\n",
              "      <th>native_country_Thailand</th>\n",
              "      <th>native_country_Trinadad&amp;Tobago</th>\n",
              "      <th>native_country_United_States</th>\n",
              "      <th>native_country_Unknown</th>\n",
              "      <th>native_country_Vietnam</th>\n",
              "      <th>native_country_Yugoslavia</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.030671</td>\n",
              "      <td>-1.063611</td>\n",
              "      <td>1.134739</td>\n",
              "      <td>-0.035429</td>\n",
              "      <td>0</td>\n",
              "      <td>7.684784</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.837109</td>\n",
              "      <td>-1.008707</td>\n",
              "      <td>1.134739</td>\n",
              "      <td>-2.222153</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.042642</td>\n",
              "      <td>0.245079</td>\n",
              "      <td>-0.420060</td>\n",
              "      <td>-0.035429</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.057047</td>\n",
              "      <td>0.425801</td>\n",
              "      <td>-1.197459</td>\n",
              "      <td>-0.035429</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.775768</td>\n",
              "      <td>1.408176</td>\n",
              "      <td>1.134739</td>\n",
              "      <td>-0.035429</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 93 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        age    fnlwgt  education_num  hours_per_week  TARGET  \\\n",
              "0  0.030671 -1.063611       1.134739       -0.035429       0   \n",
              "1  0.837109 -1.008707       1.134739       -2.222153       0   \n",
              "2 -0.042642  0.245079      -0.420060       -0.035429       0   \n",
              "3  1.057047  0.425801      -1.197459       -0.035429       0   \n",
              "4 -0.775768  1.408176       1.134739       -0.035429       0   \n",
              "\n",
              "   log_capital_gain  log_capital_loss  workclass_Federal_gov  \\\n",
              "0          7.684784               0.0                      0   \n",
              "1          0.000000               0.0                      0   \n",
              "2          0.000000               0.0                      0   \n",
              "3          0.000000               0.0                      0   \n",
              "4          0.000000               0.0                      0   \n",
              "\n",
              "   workclass_Local_gov  workclass_Never_worked            ...              \\\n",
              "0                    0                       0            ...               \n",
              "1                    0                       0            ...               \n",
              "2                    0                       0            ...               \n",
              "3                    0                       0            ...               \n",
              "4                    0                       0            ...               \n",
              "\n",
              "   native_country_Puerto_Rico  native_country_Scotland  native_country_South  \\\n",
              "0                           0                        0                     0   \n",
              "1                           0                        0                     0   \n",
              "2                           0                        0                     0   \n",
              "3                           0                        0                     0   \n",
              "4                           0                        0                     0   \n",
              "\n",
              "   native_country_Taiwan  native_country_Thailand  \\\n",
              "0                      0                        0   \n",
              "1                      0                        0   \n",
              "2                      0                        0   \n",
              "3                      0                        0   \n",
              "4                      0                        0   \n",
              "\n",
              "   native_country_Trinadad&Tobago  native_country_United_States  \\\n",
              "0                               0                             1   \n",
              "1                               0                             1   \n",
              "2                               0                             1   \n",
              "3                               0                             1   \n",
              "4                               0                             0   \n",
              "\n",
              "   native_country_Unknown  native_country_Vietnam  native_country_Yugoslavia  \n",
              "0                       0                       0                          0  \n",
              "1                       0                       0                          0  \n",
              "2                       0                       0                          0  \n",
              "3                       0                       0                          0  \n",
              "4                       0                       0                          0  \n",
              "\n",
              "[5 rows x 93 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 207
        }
      ]
    },
    {
      "metadata": {
        "id": "5iMVLgVnECeQ",
        "colab_type": "code",
        "outputId": "0091a968-1322-47e9-8c35-8340b4a63190",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 223,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Private', 'Self-emp-not-inc', 'Local-gov', 'Side_hustle', 'State-gov',\n",
              "       'Self-emp-inc', 'Federal-gov', 'Without-pay', 'Never-worked'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 223
        }
      ]
    },
    {
      "metadata": {
        "id": "RT1LFnFO1lo6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Part 2 - Fit and present a Logistic Regression\n",
        "\n",
        "Your data should now be in a state to fit a logistic regression. Use scikit-learn, define your `X` (independent variable) and `y`, and fit a model.\n",
        "\n",
        "Then, present results - display coefficients in as interpretible a way as you can (hint - scaling the numeric features will help, as it will at least make coefficients more comparable to each other). If you find it helpful for interpretation, you can also generate predictions for cases (like our 5 year old rich kid on the Titanic) or make visualizations - but the goal is your exploration to be able to answer the question, not any particular plot (i.e. don't worry about polishing it).\n",
        "\n",
        "It is *optional* to use `train_test_split` or validate your model more generally - that is not the core focus for this week. So, it is suggested you focus on fitting a model first, and if you have time at the end you can do further validation."
      ]
    },
    {
      "metadata": {
        "id": "s7fTRDXguD7N",
        "colab_type": "code",
        "outputId": "0910dfa4-eb60-4e33-bb6d-54e4cd521aed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "cell_type": "code",
      "source": [
        "X = df.drop(dependent, axis=1)\n",
        "y = df[dependent]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "\n",
        "m = LogisticRegression(solver='lbfgs', max_iter=1000).fit(X_train,y_train)\n",
        "\n",
        "def REPORT_(logi_regr_model, n=5): \n",
        "  \n",
        "  avg_correct = logi_regr_model.score(X_test, y_test)\n",
        "  \n",
        "  report_dict = dict(zip(X.columns, \n",
        "                         logi_regr_model.coef_[0]))\n",
        "\n",
        "  n_highest = dict(zip(nlargest(n, report_dict), \n",
        "                          nlargest(n, report_dict.values())))\n",
        "\n",
        "  n_smallest = dict(zip(nsmallest(n, report_dict), \n",
        "                        nsmallest(n, report_dict.values())))\n",
        "  \n",
        "  mean_coef = np.divide(sum(report_dict.values()), len(report_dict.values()))\n",
        "  \n",
        "  s0 = f'The rate of prediction correctness on our testing data is {avg_correct:.3}\\n\\n'\n",
        "  \n",
        "  s1 = f'The {n} strongest predictors of annual income >50k are {list(n_highest.keys())}. \\n'\n",
        "  s11 = 'Each one of these is from the workclass feature, onehot\\'d\\n\\n' \n",
        "  s2 = f'The {n} strongest predictors of annual income <=50k are {list(n_smallest.keys())}. \\n'\n",
        "  \n",
        "  s21 = 'education_num is education level, ordinally represented. ' +\\\n",
        "        'Age says \"every year you get older, you\\'re less likely to earn above 50k\"'\n",
        "  \n",
        "  s3 = '\\n\\nthe coefficients in general are not very drastic. ' + \\\n",
        "       'This could be because of standardization I performed, continuous ' + \\\n",
        "       'features were centered at origin and scaled to unit variance, besides '+ \\\n",
        "       'capital gains/losses of which I took the logarithm. ' + \\\n",
        "       'ultimately, the avg coefficient was {:.3}. '.format(mean_coef)\n",
        "  \n",
        "  return reduce(lambda s, t: s+t, [s0, s1, s11, s2, s21, s3])\n",
        "\n",
        "print(REPORT_(m, 3))"
      ],
      "execution_count": 234,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The rate of prediction correctness on our testing data is 0.844\n",
            "\n",
            "The 3 strongest predictors of annual income >50k are ['workclass_Without_pay', 'workclass_State_gov', 'workclass_Side_hustle']. \n",
            "Each one of these is from the workclass feature, onehot'd\n",
            "\n",
            "The 3 strongest predictors of annual income <=50k are ['age', 'education_num', 'fnlwgt']. \n",
            "education_num is education level, ordinally represented. Age says \"every year you get older, you're less likely to earn above 50k\"\n",
            "\n",
            "the coefficients in general are not very drastic. This could be because of standardization I performed, continuous features were centered at origin and scaled to unit variance, besides capital gains/losses of which I took the logarithm. ultimately, the avg coefficient was 0.0203. \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yRK3BmHIAmEU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "931ce891-9ce7-4bf2-9f18-ac54edbb1a21"
      },
      "cell_type": "code",
      "source": [
        "df_.workclass.value_counts().index"
      ],
      "execution_count": 225,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Private', 'Self-emp-not-inc', 'Local-gov', 'Side_hustle', 'State-gov',\n",
              "       'Self-emp-inc', 'Federal-gov', 'Without-pay', 'Never-worked'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 225
        }
      ]
    },
    {
      "metadata": {
        "id": "hyX715LtO9_P",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Three features positively correlated with income above 50k: \n",
        "When I gave `workclass` feature to `pd.get_dummies`, it made 9 features: `'Private', 'Self-emp-not-inc', 'Local-gov', 'Side_hustle', 'State-gov', 'Self-emp-inc', 'Federal-gov', 'Without-pay', 'Never-worked'`\n",
        "\n",
        "the 3 best performing features are all _one-hot encodings_ of the `workclass` feature\n",
        "- Without pay\n",
        "- State gov\n",
        "- \"Side hustle\", which in the data was originally \"?\". I made a gut move to consider \"?\" as stuff like uber, not exactly employment but not exactly unemployment either. This could have been totally wrong, and maybe \"?\" would have been better. All the docs tell us about this is that they `Convert Unknown to \"?\"`, meaning they **genuinely don't know**. \n",
        "\n",
        "##My choice to say `side hustle` instead of `?` is **not the point**,  the point is rather that _one of the strongest predictors is something the dataset knows nothing about_."
      ]
    },
    {
      "metadata": {
        "id": "m3EPUWjcSJJD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Three features _negatively_ correlated with income above 50k. In other words, three features _positively_ correlated with income below 50k. \n",
        "\n",
        "- Age, which says \"every year you get older, you\\'re less likely to earn above 50k\"\n",
        "- `fnlwgt`: This is a creation of our fine curators, Ron Kohavi and Barry Becker of Silcon Graphics. In the [docs](https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.names), they write \"_The weights on the files are controlled to independent estimates of the civilian noninstitutional population of the US.  These are prepared monthly for us by Population Division here at the Census Bureau. ...  The term estimate refers to population totals derived from CPS by creating \"weighted tallies\" of any specified socio-economic characteristics of the population. ... People with similar demographic characteristics should have similar weights. _\"\n",
        "- education level, represented ordinally. "
      ]
    },
    {
      "metadata": {
        "id": "CdzsNQGsUMPy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Overall, I feel like we've learned something meaningful about income in the united states. \n",
        "- Coefficient magnitudes are moderate, meaning nothing has a _suspiciously extreme_ impact on the prediction.\n",
        "- Some coefficients are negligible, and could probably be factored out in a more parsimonious model. \n",
        "- If I had done an ordinal interpretation of the `workclass` feature rather than onehot, the impact would have been _huge_. \n"
      ]
    },
    {
      "metadata": {
        "id": "BkIa-Sa21qdC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Part 4: Match the following situation descriptions with the model most appropriate to addressing them\n",
        "\n",
        "In addition to logistic regression, a number of other approaches were covered this week. Pair them with the situations they are most appropriate for, and briefly explain why.\n",
        "\n",
        "Situations:\n",
        "1. You are given data on academic performance of primary school students, and asked to fit a model to help predict \"at-risk\" students who are likely to receive the bottom tier of grades.\n",
        "2. You are studying tech companies and their patterns in releasing new products, and would like to be able to model and predict when a new product is likely to be launched.\n",
        "3. You are working on modeling expected plant size and yield with a laboratory that is able to capture fantastically detailed physical data about plants, but only of a few dozen plants at a time.\n",
        "\n",
        "Approaches:\n",
        "1. Ridge Regression\n",
        "2. Quantile Regression\n",
        "3. Survival Analysis"
      ]
    },
    {
      "metadata": {
        "id": "Yjj0sseiuHib",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1. We need some way to represent `at_risk` in `{0,1}`. Ultimately, we want to do _survival analysis_\n",
        "\n",
        "\n",
        "## 2. We'd use the _censorship_ idea in preprocessing because we're interested in _timeline_. Ultimately, we want _quantile regression_ because median time-to-launch is more useful than mean time-to-launch. \n",
        "\n",
        "\n",
        "## 3. Plant size and yield are both _continuous, quantitative_ dependent variables. We want _ridge regression_ here so that our coefficients are _regularized_ between weird sample sizes and a sub-ideal number of relevant covariates. "
      ]
    }
  ]
}